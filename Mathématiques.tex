% !TEX encoding = UTF-8 Unicode
\documentclass[11pt,a4paper,fleqn,pdftex]{report}
\include{Preambule}
\begin{document}
\newcounter{density}
\setcounter{density}{20}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Page de Garde
\begin{titlepage}
\newgeometry{left=3cm,bottom=5cm,right=3cm,top=5cm}	
	\begin{center}
		\textsc{}\\[3.5cm]
		\textsc{\Large Cours de Prépa}\\[0.5cm]
		% Title
		\HRule \\[0.4cm]
		{ \huge \bfseries Mathématiques \\[0.4cm] }
		
		\HRule \\[1.5cm]
		
		% Author
		\begin{minipage}{0.4\textwidth}
		\begin{flushleft} \large
		\emphh{Écrit par}\\
		Alexandre \textsc{Jouandin}
		\end{flushleft}
		\end{minipage}
		\begin{minipage}{0.4\textwidth}
		\begin{flushright} \large
		\emphh{Année} \\
		2013---2015
		\end{flushright}
		\end{minipage}
		\begin{tikzpicture}[scale=0.7]
    \def\couleur{MidnightBlue}
    \path[coordinate] (0,0)  coordinate(A)
                ++( 60:12cm) coordinate(B)
                ++(-60:12cm) coordinate(C);
    \draw[fill=\couleur!\thedensity] (A) -- (B) -- (C) -- cycle;
    \foreach \x in {1,...,17}{%
        \pgfmathsetcounter{density}{\thedensity+10}
        \setcounter{density}{\thedensity}
        \path[coordinate] coordinate(X) at (A){};
        \path[coordinate] (A) -- (B) coordinate[pos=.15](A)
                            -- (C) coordinate[pos=.15](B)
                            -- (X) coordinate[pos=.15](C);
        \draw[fill=\couleur!\thedensity] (A)--(B)--(C)--cycle;
    }
\end{tikzpicture}
		\vfill
		
	\end{center}
		% Bottom of the page
		{\large \today}
	
\end{titlepage}
\addtocounter{page}{1}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Début du livre
\part{Première année}
\chapter{Géométrie} % (fold)
\label{cha:geometrie}
\section{Équations générales} % (fold)
\label{sec:equations_generales}
\begin{tabular}{r | l}
     Type       & Équation \\
     Droite     & $ax + by +c = 0$ \\
     Plan       & $ax + by +cz +d = 0$ \\
     Cercle     & $(x-x_0)^2 + (y-y_0)^2 = R^2$ \\
     Sphère     & $(x-x_0)^2 + (y-y_0)^2  + (z-z_0)^2 = R^2$
\end{tabular}
% section equations_generales (end)
% chapter geometrie (end)
\chapter{Calculs algébriques} % (fold)
\label{cha:calculs_algebriques}
\section{Somme des termes d'une suite arithmétique} % (fold)
\label{sec:sommes_arithmetiques}
\begin{dfn}
     Soit $I$ un ensemble fini, et $(x_i)_{i\in I}$ une famille de nombres complexes. \\[\baselineskip]
     La somme des $x_i$ est notée $\sum_{i\in I} x_i$\\
     Le produit des $x_i$ est noté $\prod_{i\in I} x_i$
\end{dfn}
\begin{theorem}[Somme des entiers de $1$ à $n$]
     Pour tout $n$ de $1$ à $n$ : 
     \begin{equation}
     \sum_{k=1}^n k = \dfrac{n(n-1)}{2}
     \end{equation}
\end{theorem}
\begin{proof}
\begin{equation}
     \begin{array}{r c *{4}{c@{\; + \;}} c}
          S & = & 1 & 2 & \cdots & n-1 & n \\
          +\quad S & = & n & n-1 & \cdots & 2 & 1 \\
          \hline
          2S & = & n+1 & n+1 & \cdots & n+1 & n+1
     \end{array}
\end{equation}
d'où $2S = n \times (n+1)$, et $S=\dfrac{n(n+1)}{2}$
\end{proof}
\begin{theorem}[Somme des premières puissances]
    Pour tout $n$ entier naturel non nul, on a : 
    \begin{subequations}
         \begin{align}
              \sum_{k=1}^n k^2 &= \dfrac{n(n+1)(2n+1)}{6} \\
              \sum_{k=1}^n k^3 &= \dfrac{n^2(n+1)^2}{4} \\
               &= \left( \sum_{k=1}^n k \right)^2 \notag
         \end{align}
    \end{subequations}
    Les démonstrations de ces formules se font par récurrence (en connaissant le résultat), ou en primitivant.
\end{theorem}
% section sommes_arithmetiques (end)
\section{Coefficients binomiaux} % (fold)
\label{sec:coefficients_binomiaux}
\begin{dfn}
     Pour $E$ un ensemble fini de $n$ éléments, on note $\binom{n}{p}$ le nombre de sous-parties de $E$ à $p$ éléments.
     \begin{equation}
     \binom{n}{p} = \dfrac{n!}{p!(n-p)!}
     \end{equation}
\end{dfn}
% section coefficients_binomiaux (end)
% chapter calculs_algebriques (end)

\chapter{Suites}
\begin{dfn}[Borne supérieure]
     On appelle \emph{borne supérieure} d'une partie $F$ d'un ensemble ordonné fini $E$ \emphh{le plus petit des majorants} de $F$.\newline
     En d'autres termes,
     \begin{equation}
      a = \sup F \Leftrightarrow \forall y \in F, \left[ \vphantom{\sum{}} a \le y \Leftrightarrow \left( \forall x \in F, x \le y \right) \right]
     \end{equation} 
\end{dfn}
\begin{itheorem}[Théorème de la suite monotone]
    \begin{tabbing}
     Soit $\left( u_n\right)_{n\in \mathbb{N}}$ une suite croissante de $\Reel{}^\mathbb{N}$. \\
     So\= \kill
     \> Si $(u_n)$ est majorée, alors elle converge \emphhs{vers sa borne supérieure}.\\
     \> Sinon, si $(u_n)$ n'est pas majorée, alors elle admet $+\infty$ pour limite.
     \end{tabbing}
\end{itheorem}
\section{Comparaison de suites}
\begin{dfn}[Suites équivalentes]\label{equivalence}
Deux suites $u_n$ et $v_n$ sont dites \emph{équivalentes}\index{equivalence@Équivalence} en l'infini s'il existe une suite $w_n$ tendant vers 1 en l'infini telle que $u_n = w_n \times v_n$.\\
Autrement dit : 
\begin{empheq}[box = \ibox]{equation}
	u_n \sim v_n \Leftrightarrow \exists w_n \xrightarrow[+\infty]{} 1 \text{ tq } u_n = w_n v_n
\end{empheq}
\end{dfn}
\begin{dfn}[$O(\cdots)$ et $o(\cdots)$]
Si $x_n$ est une suite de $(E,N)^\mathbb{N}$ et $(\alpha_n)$ une suite de $\Reel^\mathbb{N}$ : 
    \begin{equation}
    \begin{array}{r@{\text{, si }}l@{\quad n \ge n_0 \implies}l}
       x_n = O(\alpha_n) & \forall M \in R^+, \exists n_0, \forall n \in \mathbb{N},& N(x_n) \le M|\alpha_n| \\
       x_n = o(\alpha_n) & \forall \varepsilon > 0, \exists n_0, \forall n \in \mathbb{N}, & N(x_n) \le \varepsilon | \alpha_n|
      \end{array}  
    \end{equation}
\end{dfn}
\begin{dfn}[$O(\cdots)$ et $o(\cdots)$ dans \Reel{}]
\label{dfn:domination_negligeabilite}
    Si $(\alpha_n)_n$ est une suite à valeurs dans $\Reel^{\textcolor{couleurImp}{*}}$, 
    \begin{equation}
    \begin{array}{r@{\text{, \ssi{} }}c}
       x_n \underset{n\to +\infty}{=} O(\alpha_n) & \dfrac{x_n}{\alpha_n} \text{ est bornée.} \\[15pt]
       x_n \underset{n\to +\infty}{=} o(\alpha_n) & \dfrac{x_n}{\alpha_n} \xrightarrow[n\to +\infty]{} 0
      \end{array} 
    \end{equation}
\end{dfn}
\Attention{Une suite ne peut pas, à notre niveau, être $\sim 0$, en $o(0)$ ou $O(0)$, car la définition dirait que la suite est nulle à partir d'un certain rang.}
\section{Suites de Cauchy} % (fold)
\label{sec:suites_de_cauchy}
\begin{dfn}[Suite de \textsc{Cauchy}]
    Une suite $(x_n)_n$ dans $(E,N)$ est dite de Cauchy si 
    \begin{equation}
    \forall \varepsilon > 0, \exists n_0, \forall (n,p) \in \mathbb{N}^2, n\ge n_0 \implies \boxed{N\big( x_{n+p} - x_n \big)< \varepsilon}
    \end{equation}
\end{dfn}
% section suites_de_cauchy (end)
%
\chapter{Nombres complexes} % (fold)
\label{cha:nombres_complexes}
\section{Plan complexe} % (fold)
\label{sec:plan_complexe}
\begin{dfn}[Corps complexe $(\Cmplx{},+,\times{})$]
     Un \emph{nombre complexe}\index{Complexe} est un élément $(a,b) \in \Reel{}^2$. L'ensemble des nombres complexes est noté \Cmplx{}, c'est un corps muni des lois suivantes : 
     \begin{description}
         \item[Addition] $(a,b) + (c,d) = (a+c,b+d)$\hfill \\de neutre $(0,0)$
         \item[Multiplication] $(a,b) \times (c,d) = (ac - bd, ad + bc)$\hfill \\de neutre $(1,0)$
     \end{description}
\end{dfn}
\begin{theorem}
     $(\Cmplx{},+,\times{})$ est un corps \emphl{(\textit{cf.} tableau \ref{tab:Groupe-Anneau-Corps} page \pageref{tab:Groupe-Anneau-Corps} pour la définition d'un corps)}
\end{theorem}
\begin{dfn}[Module]
     Soit $z=x+iy$ un nombre complexe. On appelle \emph{module}\index{Module} la valeur $|z| = \sqrt{x^2 + y^2}$
\end{dfn}
% section plan_complexe (end)
\section{Nombres complexes de module 1} % (fold)
\label{sec:nombres_complexes_de_module_1}
\begin{dfn}
     On note $\mathcal{U}$ l'ensemble des nombres complexes de module 1.\\
     Le disque unité est l'ensemble de ses points.
\end{dfn}
\needspace{4cm}
\begin{prop}
    \begin{itemize}
        \item $\mathcal{U}$ est stable par le produit $\times$
        \item $z \in \mathcal{U} \Longleftrightarrow \overline{z} = \frac{1}{z}$
    \end{itemize}
\end{prop}
% section nombres_complexes_de_module_1 (end)
% chapter nombres_complexes (end)
\part{Structures algébriques usuelles} % (fold)
\label{prt:structures_algebriques_usuelles}
% Tableau des définitions
\begin{table}[ht]
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabu}{c r |[1.3pt] *{3}{c|} c |[1.3pt]}
 & & Groupe & Groupe Abélien & Anneau & Corps \\
\tabucline[1pt]{-}
\multirow{4}{*}{\rotatebox[origin=c]{90}{Loi ($+$/$*$)}}
     & Neutre $e$ (ou $0$) & \cellcolor{couleurClaire}\cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark \\
     \cline{2-6} 
     & Assossiative & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark \\
     \cline{2-6} 
     & Symétrique (admet $a^{-1}$) & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark \\
     \cline{2-6} 
     & Commutative &   & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark \\
\tabucline[1pt]{-}
\multirow{5}{*}{\rotatebox[origin=c]{90}{Loi $\times$}}
     & Neutre $1$ & & & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark \\
     \cline{2-6} 
     & Associative & & & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark \\
     \cline{2-6} 
     & Distributive de la loi + & & & \cellcolor{couleurClaire}\checkmark & \cellcolor{couleurClaire}\checkmark \\
     \cline{2-6} 
     & Commutative & & & & \cellcolor{couleurClaire}\checkmark \\
     \cline{2-6} 
     & Inversible & & & & \cellcolor{couleurClaire}\checkmark \\
\tabucline[1pt]{-}
\end{tabu}
\end{center}
\caption{Tableau récapitulatif des définitions}\label{tab:Groupe-Anneau-Corps}
\end{table}
%
\chapter{Groupes et sous-groupes}
\section{Groupes et sous-groupes} % (fold)
\label{sec:definition_d_un_groupe}
\begin{dfn}[Groupe]
     On appelle \emph{groupe}\index{Groupe} le couple $(G,*)$ où $G$ est un ensemble muni d'$*$, une \gls{LCI} associative, symétrique, et admettant un neutre.
\end{dfn}
% section definition_d_un_groupe (end)
\subsection{Produit fini de groupes} % (fold)
\label{sub:produit_fini_de_groupes}
\begin{dfn}[Groupe Produit]
     Soient $(G,*)$ et $(G',\circ)$ deux groupes. \newline
     Le groupe $\left( G\times G', \square \right)$ tel que
     \[
         (x,x')\square (y,y') = (x*y,x'\circ y')
     \]
     est un groupe appelé \emph{groupe produit}\index{Groupe!produit} de $G$ et $G'$
\end{dfn}
\begin{exemple}[Groupe de \textsc{Klein}]\label{ex:groupe_de_klein}
     Le groupe $(\mathbb{Z}/2\mathbb{Z} \times \mathbb{Z}/2\mathbb{Z})$ est appelé \emphh{groupe de \textsc{Klein}}\index{Groupe!Klein@\emphi{de }\textsc{Klein}}. C'est un groupe produit, il n'est pas isomorphe à $\mathbb{Z}/4\mathbb{Z}$, et \emphhs{il a la spécificité de ne pas être cyclique}. 
\end{exemple}
% subsection produit_fini_de_groupes (end)
\subsection{Sous-groupe} % (fold)
\label{sec:sous_groupe}
\begin{dfn}[Sous-groupe]
     Soit $(G,*)$ un groupe, et soit $H$ une partie de $G$\\
     On dit que $H$ est un \emph{sous-groupe}\index{Sous-groupe} de $G$ si, muni de la \gls{LCI} $*$, $H$ est un groupe stable par $*$.
\end{dfn}
\needspace{5cm}
\begin{itheorem}[Caractérisation d'un sous-groupe]
     Avec les notations précédentes, $H$ est un sous-groupe de $G$ si : 
     \begin{itemize}
         \item $H$ n'est pas vide
         \item $\forall (x,y) \in H^2,\, x*y^{-1} \in H$.
     \end{itemize}
     En général, pour vérifier que $H$ est non vide, on vérifie que le neutre $e$ de $G$ est aussi dans $G$.
\end{itheorem}
\begin{theorem}[Intersection de sous-groupes]
     Soit $\left( H_i \right)_{i\in \mathbb{N}}$ une famille de sous-groupes.\\
     Alors $H=\bigcap_{i\in \mathbb{N}} H_i$ est un sous-groupe.
\end{theorem}
\begin{proof}
     Avec les notations précédentes, montrons que $H$ est un sous-groupe : 
     \[
     \begin{array}{r c r}
         \forall i\in \mathbb{N},\, e\in H_i &\implies & e\in H \\
         \forall (x,y)\in H^2,\, \forall i\in \mathbb{N},\, (x,y)\in H_i^2,\text{ donc }x*y^{-1}\in H_i & \implies & x*y^{-1}\in H
     \end{array}
     \]
     Ainsi, $H$ respecte les propriétés de caractérisation d'un sous-groupe, donc $H$ est un sous-groupe.\qed
\end{proof}
\begin{dfn}[Sous-groupe engendré]
     Soit $G$ un groupe, et soit $A$ une partie de $G$. \\
     L'intersection des sous-groupes de $G$ contenant $A$ est le plus petit sous-groupe contenant $A$. On le note $\langle A \rangle$, et on dit que c'est le \emph{sous-groupe engendré}\index{Sous-groupe!engendre@engendré} par $A$.
\end{dfn}
\begin{itheorem}[Sous-groupes de $\left( \mathbb{Z},+ \right) $]\label{thm:sous_groupe_de_Z}
     Pour tout sous-groupe $H$ de $\left( \mathbb{Z},+ \right),\,\exists n\in \mathbb{N}$ tel que $H=n\mathbb{Z}$
\end{itheorem}
\begin{proof}
     Soit $H$ un sous-groupe. Si $H=\lbrace 0 \rbrace$, alors $H=0\mathbb{Z}$.\\
     Supposons maintenant que $H$ contient au moins un entier. \newline
     Soit \uline{$n$ le plus petit entier} de $H$. Il convient de dire que $n\mathbb{Z}\in H$. \newline
     Soit $m$ un entier quelconque de $H$. Effectuons sa division euclidienne par $n$ : 
     \begin{gather*}
         m = nq+r \quad\quad\quad 0\le r < n \\
         nq\in H,\, m\in H \implies \boxed{r\in H}
     \end{gather*}
     Or $n$ étant le plus petit entier dans $H$, $r$ dans $H$ étant inférieur à $n$, $r=0$, donc $m=nq$\qed
\end{proof}
% subsection sous_groupe (end)
\section{Morphismes de groupes} % (fold)
\label{sec:morphismes_de_groupes}
\subsection{Définition} % (fold)
\label{sub:morphismes_de_groupe_definition}
\begin{dfn}[Morphisme de groupes]
     On appelle \emph{morphisme}\index{Morphisme!groupe@\emphi{de }groupe} d'un groupe $(G,*)$ à un groupe $(H,\times )$ l'application $f$ telle que
     \begin{equation}
     \forall (x,y) \in G^2,\, f \left( x*y \right) = f(x)\times  f(y)
     \end{equation}
\end{dfn}
% subsection morphismes_de_groupe_definition (end)
\subsection{Propriétés d'un morphisme de groupes} % (fold)
\label{sub:proprietes_d_un_morphisme}
\begin{theorem}[Image et image réciproque d'un sous-groupe]
     L'image d'un sous-groupe par un morphisme est un sous groupe. \newline
     L'image réciproque d'une sous-groupe par un morphisme est un sous-groupe\\
     \begin{center}
     \begin{tikzpicture}
     \begin{scope}[rotate around={-40:(-4,0)}] % rotate around={degree:coordinate}
          \draw[color=couleurNoirClair, fill=couleurGrisFonce, line width=1pt] (-4,0) ellipse (2 and 1);
          \draw[dashed, color=couleurNoirClair, fill=couleurGrisClair, rotate around={15:(-3.8,0)}, line width=1pt] (-3.8,0) ellipse(1 and 0.7) (-3.6,0) node{$H$}; 
          \draw (-5.3,0) node{$G$} (-4,0) node{$\bullet$} (-4,0) node[above]{$e$};
     \end{scope}
          \begin{scope}[rotate around={15:(4,0)}] % rotate around={degree:coordinate}
          \draw[color=couleurNoirClair, fill=couleurGrisFonce, line width=1pt] (4,0) ellipse (2 and 1);
          \draw[dashed, color=couleurNoirClair, fill=couleurGrisClair, rotate around={15:(3.3,0)}, line width=1pt] (3.3,0) ellipse(1 and 0.7) (2.6,0) node{$H'$}; 
          \draw (5.3,0) node{$G'$} (4,0) node{$\bullet$} (4,0) node[above left]{$e'$};
     \end{scope}
          \draw[<->, >=latex, line width=2pt, color=couleurFonce] (-3,1) to [bend left=40] node[midway, below]{$f$} (2.1,0.8);
     \end{tikzpicture}
     \captionof{figure}{Image et image réciproque d'un sous-groupe par un morphisme $f$}
     \end{center}
\end{theorem}
\begin{itheorem}[Condition d'injectivité d'un morphisme]
     Soit $f$ un morphisme de groupes de $\left( G,* \right) $ dans $\left( H, \times \right) $. Alors : 
     \begin{tabbing}
          $f$ est \= \emphh{surjective} \= $\Leftrightarrow \mathrm{Im} f = H$ \kill
          $f$ est \> \emphh{injective} \> $\Leftrightarrow \mathrm{Ker} f = \lbrace e \rbrace$ \\
          $f$ est \> \emphh{surjective} \> $\Leftrightarrow \mathrm{Im} f = H$
     \end{tabbing}
\end{itheorem}
\begin{prop}
Soit $f : G \to G'$ un morphisme de groupes de neutres respectifs $e$ et $e'$. Alors : 
     \begin{itemize}
         \item $f(e) = e'$
         \item $\forall x\in G,\, f(x^{-1}) = f(x)^{-1}$
     \end{itemize}
\end{prop}
% subsection proprietes_d_un_morphisme (end)
\subsection{Isomorphismes} % (fold)
\label{sub:isomorphismes}
\begin{dfn}[Isomorphisme]
     Un morphisme de groupe \emphh{bijectif} est appelé \emph{isomorphisme}\index{Isomorphisme}
\end{dfn}
\begin{theorem}[Réciproque d'un isomorphisme]
     La bijection réciproque d'un isomorphisme est un isomorphisme.
\end{theorem}
% subsection isomorphismes (end)
% section morphismes_de_groupes (end)
\section{Groupes monogènes et cycliques} % (fold)
\label{sec:groupes_monogenes_et_cycliques}
\begin{dfn}[Groupe $\big( \mathbb{Z}/n\mathbb{Z},+ \big)$]
Un élément de $\mathbb{Z}/ n\mathbb{Z}$ est la classe des éléments ayant tous le même reste par la division euclidienne par $n$.\newline
$\big( \mathbb{Z}/n\mathbb{Z},+ \big)$ est un groupe.
\end{dfn}
E.g.\textit{: Dans $\mathbb{Z}/3\mathbb{Z}$, l'élément $\bar{1}$ est la classe des éléments de $\mathbb{Z}$ ayants tous le même reste $\bar{1}$ dans leur division par $3$.}
\begin{theorem}[Générateurs de $\mathbb{Z}/n\mathbb{Z}$]
     Soit $n\in \mathbb{N}^*$. Les éléments générateurs du groupe $\big( \mathbb{Z}/n\mathbb{Z},+ \big)$ sont les classes $\dot{k}$ où $k \wedge n = 1$.
\end{theorem}
\begin{dfn}[Groupe monogène]
     $G$ est un \emph{groupe monogène}\index{Monogene@Monogène}\index{Groupe!monogene@monogène} s'il est engendré par un seul élément $a$, c'est-à-dire si $G = \langle a \rangle$.
\end{dfn}
\begin{dfn}[Groupe cyclique]
     Un groupe monogène fini est appelé \emph{groupe cyclique}.\\
     Plus visuellement, un groupe monogène est cyclique $G = \langle a \rangle$ s'il peut s'écrire sous la forme : 
     \[G = \left\lbrace e,a,a^2,\ldots,a^n \right\rbrace \]
\end{dfn}
\begin{itheorem}
     Tout groupe monogène infini est isomorphe à $(\mathbb{Z},+)$.\\
     Tout groupe monogène fini de cardinal $n$ (groupe cyclique d'ordre $n$) est isomorphe à $\big(\mathbb{Z}/n\mathbb{Z},+\big)$.
\end{itheorem}
% section groupes_monogenes_et_cycliques (end)
\section{Ordre d'un élément dans un groupe} % (fold)
\label{sec:ordre_d_un_element_dans_un_groupe}
\begin{dfn}[Ordre d'un groupe]
     L'\emph{ordre}\index{Ordre!groupe@\emphi{d'un} groupe} d'un groupe fini est son cardinal $n$.
\end{dfn}
\begin{dfn}[Ordre d'un élément dans un groupe]
     Soit $a\in G$. Si $\langle a \langle$ est fini, l'\emph{ordre de $a$}\index{Ordre!element@\emphi{d'un} élément} est le cardinal de ce sous-groupe.
\end{dfn}
\begin{itheorem}[Théorème de \textsc{Lagrange}]
     Dans un groupe fini, l'ordre d'un sous-groupe est un \emphhs{diviseur} de l'ordre du groupe.
\end{itheorem}
\begin{dfn}[Éléments nilpotents]
Un élément est \emph{nilpotent}\index{Nilpotent} si, composé par lui même, il peut être nul : 
\begin{equation}
\left\lbrace \begin{array}{l} a\text{ nilpotent}\\ a\neq 0 \end{array}\right. \Leftrightarrow \exists p \in \mathbb{N}^{\alpha} \text{ tel que } a^p =0 
\end{equation}
\end{dfn}
% section ordre_d_un_element_dans_un_groupe (end)
\section{Classe d'équivalence}
\begin{dfn}[Relation d'équivalence]
Une relation d'équivalence $\mathcal{R}$ est une relation binaire caractérisée de la manière suivante : 
\begin{equation}
\left|
\begin{array}{r l c}
\forall x \in E,& \boldsymbol{x\mathcal{R}x} & \text{\textbf{(Réfléxivité)}}\\
\forall (x,y) \in E^2, & x\mathcal{R}y\implies \boldsymbol{y\mathcal{R}x} &\text{\textcolor{couleurImp}{\textbf{(Symétrie)}}}\\
\forall (x,y,z) \in E^3, & \big( x\mathcal{R}y \text{ et } y\mathcal{R}z \big) \implies \boldsymbol{x\mathcal{R}z} & \text{\textbf{(Transitivité)}}
\end{array}
\right.
\end{equation}
\end{dfn}

\begin{dfn}[Relation d'ordre]
Une relation d'ordre $\mathcal{R}$ est également une relation binaire. Elle se caractérise de la manière suivante : 
\begin{equation}
\left|
\begin{array}{r l c}
\forall x \in E,& \boldsymbol{x\mathcal{R}x} & \text{\textbf{(Réfléxivité)}}\\
\forall (x,y) \in E^2, & \big( x\mathcal{R}y \text{ et } y\mathcal{R}x \big) \implies \boldsymbol{x=y} & \text{\textcolor{couleurImp}{\textbf{(Anti-symétrie)}}}\\
\forall (x,y,z) \in E^3, & \big( x\mathcal{R}y \text{ et } y\mathcal{R}z \big) \implies \boldsymbol{x\mathcal{R}z} & \text{\textbf{(Transitivité)}}
\end{array}
\right.
\end{equation}
\end{dfn}

\Attention{Bien savoir ce que signifient Symétrie et Anti-symétrie}



\begin{theorem}[Indicatrice d'\textsc{Euler}\index{Euler@\textsc{Euler}}]
C'est la fonction $\varphi$ telle que 
\[
    \varphi (n) = \mathrm{Card}\big( \left\lbrace \left( \mathbb{Z} / n \mathbb{Z} \right) ^{*} \right\rbrace \big)
\]
\end{theorem}

%TODO Théorème Chinois
% part structures_algebriques_usuelles (end)
\part{Algèbre}
\chapter{Fonctions convexes} % (fold)
\label{cha:fonctions_convexes}
\section{Parties convexes d'un espace vectoriel réel} % (fold)
\label{sec:parties_convexes_d_un_ev}
\subsection{Barycentre} % (fold)
\label{sub:barycentre}
\begin{dfn}[Barycentre]
     Soit $\big( x_i,\alpha_i\big)_{i\in \mathbb{N}}$ une famille \uline{finie} de points pondérés telle que la somme des coefficients $\sum_i \alpha_i \neq 0$. On appelle \emph{barycentre}\index{Barycentre} des points $x_i$ le point $G$ tel que : 
     \begin{equation}
     G = \dfrac{\displaystyle\sum_i \alpha_i~x_i}{\sum \alpha_i}
     \end{equation}
\end{dfn}
% TODO Rajouter les propriétés des barycentres
\todo{Rajouter les propriétés des barycentres}
% subsection barycentre (end)
\subsection{Partie convexe} % (fold)
\label{sub:partie_convexe}
La partie suivante est copiée de la sous-section~\ref{sub:convexe_connexite} page~\pageref{sub:convexe_connexite} : 
\begin{dfn}[Convexe]
    \begin{minipage}{0.6\textwidth}
        Un ensemble $E$ est \emphh{convexe}\index{Convexe} si : 
    \begin{equation}
    \forall (x,y) \in E^2, \forall t \in [0,1], \boxed{tx + (1-t)y \in E}
    \end{equation}
    Intuitivement, un ensemble est convexe si on peut relier deux points avec une ligne contenue dans cet ensemble.
    \end{minipage}\hspace{5mm}
    \begin{minipage}{0.3\textwidth}
    \begin{tikzpicture}
        \draw[thick, color=couleurImp] (0.2,0.2) -- (2.7, 0.5);
        \filldraw[fill = couleurClaire, decorate, rounded corners=5mm]%
        (0,0) -- (-0.3,2) -- (3, 2.5) -- (3,0) -- (1.5,1) -- (0.2,-0.5) -- (0,0);
        \begin{scope}
        \clip[decorate, rounded corners=5mm] (0,0) -- (-0.3,2) -- (3, 2.5) -- (3,0) -- (1.5,1) -- (0.2,-0.5) -- (0,0);
        \draw[thick] (0.2,0.2) node{$\bullet$} -- (2.7, 0.5) node{$\bullet$};
        \end{scope}
    \end{tikzpicture}
    \captionof{figure}{Un ensemble \uline{non} convexe}
    \end{minipage}
\end{dfn}
\begin{theorem}[Intersection de convexes]
     L'intersection d'une famille quelconque de convexes est convexe.\\[\baselineskip]
     \emphh{Remarque :} Ce théorème est même valable pour une famille infinie.
\end{theorem}
\begin{theorem}[Convexe dans \Reel]
    $I$ de $\Reel{}$ est convexe \emphh{si et seulement si} $I$ est un intervalle de $\Reel{}$
\end{theorem}
\begin{itheorem}[Caractérisation à l'aide de barycentres]
     Une partie $X$ de $E$ est convexe \ssi{} le barycentre $G$ de toute famille pondérée finie $\big( x_i,\alpha_i \big)_i$ telle que $\alpha_i \ge 0\; \forall i$ appartient à $X$ : 
     \begin{equation}
     G \in X
     \end{equation}
     (Bien sûr, on a toujours $\sum \alpha_i \neq 0$.)
\end{itheorem}
% subsection partie_convexe (end)
% section parties_convexes_d_un_ev (end)
\section{Fonctions convexes d'une variable réelle} % (fold)
\label{sec:fonctions_convexes_d_une_variable_reelle}
La définition suivante est copiée de la définition~\ref{dfn:convexe_connexite} page~\pageref{dfn:convexe_connexite}.
\begin{dfn}[Fonction convexe]
   \begin{minipage}{0.75\textwidth}
    Une fonction $f : I \mapsto \Reel{}$ est dite \emph{convexe}\index{Convexe} si :
   \begin{equation}
   \forall (a,b) \in I^2, \forall \lambda  \in ]0,1[, \boxed{f(\lambda a + (1-\lambda )b) \le \lambda f(a) + (1-\lambda )f(b)}
   \end{equation}
    \end{minipage}\hspace{2mm}
    \begin{minipage}{0.2\textwidth}
    \begin{tikzpicture}
        \draw [->, >=stealth] (0,0) -- (3.2,0);
        \draw [->, >=stealth] (0,0) -- (0,3.2);
        \draw [thick] (0,0).. controls (0.8,0.2) and (2,1) .. (3,3);
        \draw [color=couleurNoirClair, densely dashed] (0,0) -- (3,3);
    \end{tikzpicture}
    \captionof{figure}{Une fonction convexe}
    \end{minipage}
\end{dfn}
\begin{theorem}[Position relative du graphe]
     Une fonction est convexe \ssi{} tout arc de son graphe est situé en-dessous de la corde correspondante.
\end{theorem}
\begin{dfn}[Épigraphe]
     Soit $f$ une fonction de graphe $\Gamma_f = \{ (x,y)\in I\times \Reel \: |\: y = f(x)\}$.\\
     On appelle \emph{épigraphe}\index{Epigraphe@Épigraphe} de $f$ l'ensemble $\Gamma_f^+$ des points situés au dessus du graphe $\Gamma_f$. C'est-à-dire : 
     \begin{equation}
     \Gamma_f^+ = \{ (x,y)\in I\times\Reel \: |\: y\ge f(x) \}
     \end{equation}
\end{dfn}
\begin{theorem}[Convexité de l'épigraphe]
     Une fonction est convexe \ssi{} son épigraphe est convexe.
\end{theorem}
\begin{itheorem}[Inégalité de \textsc{Jensen}]
     Soit $f$ une fonction \emphh{convexe} sur un intervalle $I$. \\
     Soit $n\ge 2$. Soit $(x_1,\cdots ,x_n) \in I^n$ une famille de points de $I$. \\
     Pour toute famille de \emphhs{réels positifs} $(\lambda_1,\ldots ,\lambda_n)\in {\left( \Reel^+ \right) }^n$ telle que \[
         \sum_{i=1}^n \lambda_i = 1
     \]
     l'\emph{inégalité de \textsc{Jensen}}\index{Inegalite@Inégalité!Jensen@\emphi{de }\textsc{Jensen}} donne :
     \begin{equation}
     f \left( \sum_{i=1}^n \lambda_i x_i \right) \le \sum_{i=1}^n \lambda_i f(x_i)
     \end{equation}
\end{itheorem}
\todo{Démo épigraphe}
\begin{theorem}
     Soit $f$ une fonction de $\Reel{}$ dans $\Reel{}$ définie sur un intervalle $I$ non vide et non réduit à un singleton.\\
     Pour tout $x\in I$, soit $\Phi_x$ la fonction :
     \begin{equation}
      \Phi_x : t\mapsto \dfrac{f(t) - f(x)}{t-x}
     \end{equation} 
     $f$ est convexe sur $I$ \ssi{}, pour tout $x\in I$, $\Phi_x$ est croissante sur $I\backslash \{x\}$.
\end{theorem}
\begin{itheorem}[Inégalité des pentes]
\label{thm:inegalite_des_pentes}
     Soit $f$ une fonction de $\Reel{}$ dans $\Reel{}$ définie sur un \emphhs{ouvert} $I\in \Reel{}$ tel que $I$ n'est pas réduit à un singleton. \\
     Si $f$ est convexe, alors $f$ est dérivable en tout point de $I$ à gauche et à droite, et pour tout $(x,y) \in I^2$ tel que $x<y$ :
     \begin{equation}
     f'_g(x) \le f'_d(x) \le \dfrac{f(y) - f(x)}{y - x} \le f'_g(y) \le f'_d(y)
     \end{equation}
     \emphh{Remarque :} On déduit de la dérivabilité que $f$ est continue sur $I$.
\end{itheorem}
\begin{dfn}[Fonction concave]
    \begin{minipage}{0.4\textwidth}
    Une fonction $f : I \mapsto \Reel{}$ est dite \emph{concave}\index{Concave} si son opposée $-f$ est une fonction convexe. 
    \end{minipage}\hspace{0.2\textwidth}
    \begin{minipage}{0.4\textwidth}
    \begin{center}
    \begin{tikzpicture}[scale=1.5]
        \draw [->, >=stealth] (0,0) -- (3.2,0);
        \draw [->, >=stealth] (0,0) -- (0,2);
        \draw [thick] (0,0).. controls (0.4,0.8) and (2,2) .. (3,2);
        \draw [color=couleurNoirClair, densely dashed] (0,0) -- (3,2);
    \end{tikzpicture}
    \captionof{figure}{Une fonction concave}
    \end{center}
    \end{minipage}
\end{dfn}
% section fonctions_convexes_d_une_variable_reelle (end)
\section{Fonctions convexes dérivables, deux fois dérivables} % (fold)
\label{sec:fonctions_convexes_derivables_deux_fois_derivables}
\subsection{Dérivabilité et convexité} % (fold)
\label{sub:derivabilite_et_convexite}
\begin{theorem}[Caractérisation par la dérivabilité]
     Soit $f$ une fonction dérivable sur $I$. \\
     $f$ est convexe sur $I$ \ssi{} sa dérivée $f'$ est croissante sur $I$.
\end{theorem}
\begin{proof}
     Si $f$ est convexe, d'après le théorème~\ref{thm:inegalite_des_pentes} de l'inégalité des pentes, pour $x < y$, on a $f'(x) \le f(y)$. Donc $f'$ est croissante. \\[0.7\baselineskip]
     % Cette partie est copiée intégralement de Vuibert
     Réciproquement, supposons $f'$ croissante, et prenons deux points $x$ et $y$ de $I$ tels que $x < y$. Soient $a$ et $b$ tels que $y = ax + b$ est l'équation de la droite joignant $x$ et $y$. Soit $g(t) = f(t) - (at + b)$, de dérivée $g'(t) = f'(t) -a$ croissante. D'après le théorème~\ref{thm:accroissements_finis} des accroissements finis (page~\pageref{thm:accroissements_finis}), $\exists c\in ]x,y[$ tel que $f'(c) = \dfrac{f(y) - f(x)}{y-x} = a$, c'est-à-dire tel que $g'(c) = 0$. $g'$ étant croissante, elle est négative sur $[x,c]$, et positive sur $[c,y]$. On en déduit le tableau de variation : 
     \begin{center}
     \begin{tabular}{r | c c c c c}
          $t$ & $x$ & & $c$ & & $y$ \\
          \hline
          $g'$& & $-$ & $0$ & $+$ & \\
          \hline
          \multirow{2}{*}{$g$}& $0$ &&&& $0$ \\
                                 &   & $\searrow$ & & $\nearrow$ & 
     \end{tabular}
     \end{center}
     Donc $g$ est toujours négative, donc $f(t) \le (at + b)$ implique que la courbe est toujours en dessous de sa corde, donc $f$ est convexe sur $I$.\\[\baselineskip]
     Ainsi, $f$ convexe $\Longleftrightarrow$ $f'$ est croissante.\qed
\end{proof}
\begin{theorem}[Caractérisation par la dérivée double]
     Soit $f$ une fonction deux fois dérivable sur $I$. \\
     $f$ est convexe sur $I$ \ssi{} sa dérivée double $f''$ est positive sur $I$ : 
     \begin{equation}
     f\text{ convexe sur }I\quad \Longleftrightarrow \quad \forall x\in I,\, f''(x)>0
     \end{equation}
\end{theorem}
% subsection derivabilite_et_convexite (end)
\subsection{Position de la tangente} % (fold)
\label{sub:position_de_la_tangente}
\begin{theorem}[Tangentes]
     Le graphe d'une fonction convexe \emphh{dérivable} est au-dessus de chacune de ses tangentes
\end{theorem}
% subsection position_de_la_tangente (end)
\subsection{Exemples d'inégalités de convexité} % (fold)
\label{sub:exemples_d_inegalites_de_convexite}

% subsection exemples_d_inegalites_de_convexite (end)
% section fonctions_convexes_derivables_deux_fois_derivables (end)
% chapter fonctions_convexes (end)
\chapter{Réduction des Endomorphismes}
\begin{methode}
     \titre{Valeurs propres}
     Pour montrer que $\lambda$ est une valeur propre d'un endomorphisme $u$ d'un espace vectoriel $E$, on peut : 
     \begin{itemize}
         \item Revenir à la définition, et trouver un vecteur propre $x$ tel que $u(x) = \lambda x$
         \item Montrer que l'application $f - \lambda \mathrm{Id}_E$ est non-injective, \uline{c'est-à-dire} que \[\det (f - \lambda \mathrm{Id}_E) = 0\]
         \item Montrer que $\lambda$ est une racine du polynôme caractéristique $\chi_u$ de $u$
         \item On sait que la somme des valeurs propres est égale à la trace
     \end{itemize}
     \titre{Polynôme caractéristique}
     Si on cherche le polynôme caractéristique d'un endomorphisme $u$, ces étapes peuvent permettre d'avancer sa détermination :
     \begin{itemize}
         \item Prendre le polynôme dont les racines sont les valeurs propres de $u$. C'est à dire le polynôme $\prod (X - \lambda_i)$
         \item Reconnaitre les coefficients de degré $n-1$ et $0$ (\textit{cf}. théorème~\ref{thm:coeff_polynome_caracteristique} page~\pageref{thm:coeff_polynome_caracteristique}).
         \item Si la matrice est triangulaire, faire le produit des éléments diagonaux.
     \end{itemize}
     \titre{Polynôme minimal}
     Si on cherche le polynôme minimal d'un endomorphisme $u$ dans l'espace $E$ de \emphhs{dimension finie}, on peut avoir recours aux affirmations suivantes :
     \begin{itemize}
         \item Le polynôme minimal divise le polynome caratéristique. 
         \item Si $\lambda_1, \cdots, \lambda_p$ sont des valeurs propres distinctes de $u$ d'ordre de multiplicité $m_i$, alors le polynôme minimal est la valeur minimale des $m'_i$ tels que
         \[
             \Pi_u (u) = \bigg( X - \lambda_i \bigg)^{m'_i} (u) = 0
         \]
         \item Si on a un polynôme annulateur $P$, on peut le factoriser pour obtenir les racines. Puisque le polynôme minimal $\Pi_u$ divise $P$, il reste à essayer de combiner ces racines pour obtenir le polynôme de plus petit degré qui annule $u$. 
     \end{itemize}
     \titre{Théorème de décomposition des noyaux}
     En général, dès qu'on voit une somme directe, on utilise le théorème de décomposition des noyaux. \\
     Si on a $P$, un polynôme annulateur de $u$ tel que $P(u) = 0$, alors on a $\mathrm{Ker}P(u) = E$, et si $P(u)$ est le produit de plusieurs polynômes, par exemple $A$ et $B$, on peut écrire 
     \[
         E = \mathrm{Ker} A(u) \oplus \mathrm{Ker} B(u)
     \]
     \titre{Diagonalisation}
Pour vérifier qu'une diagonalisation est possible, se rapporter au théorème~\ref{thm:caracterisation_diagonalisation} page~\pageref{thm:caracterisation_diagonalisation} sur la caractérisation de la diagonalisation. Une fois vérifiée, la diagonalisation peut s'effectuer à l'aide des astuces suivantes : 
\begin{itemize}
    \item Chercher un vecteur propre évident, par exemple $\begin{pmatrix} 1\\ \vdots \\ 1 \end{pmatrix}$ si les lignes sont toutes de même somme ;
    \item Utiliser des opérations sur des lignes ou des colonnes et développer pour obtenir un déterminant plus simple à calculer ;
    \item Faire un pivot de \textsc{Gauss} pour obtenir une matrice triangulaire (ou éventuellement triangulaire par blocs) ;
    \item Calculer le polynôme caractéristique avec la méthode de \textsc{Sarrus}.
\end{itemize}
\end{methode}
\section{Genéralités}
%
\subsection{Matrices carrées semblables} % (fold)
\label{sub:matrices_carrees_semblables}
\begin{dfn}[Matrices semblables]
     Deux matrices sont dites \emph{semblables}\index{Matrice!semblable} si elles représentent le même endomorphisme d'un espace vectoriel de dimension finie.
\end{dfn}
\begin{theorem}
     Deux matrices sont \emph{semblables} \ssi il existe une matrice $P$ inversible telle que 
     \begin{equation}
     A_1 = P A_2 P^{-1}
     \end{equation}
\end{theorem}
\begin{itheorem}[Trace et déterminant]
     Deux matrices semblables ont la même trace et le même déterminant. Ce sont des \emph{invariants de similitude}.% interprétation géométrique du programme
\end{itheorem}
C'est ce dernier théorème qui permet de confirmer l'unicité de la trace et du déterminant d'un endomorphisme.
%TODO: Propriétés de la trace et du determinant
% subsection matrices_carrees_semblables (end)
\subsection{Sous-espace stable par un endomorphisme} % (fold)
\label{sub:sous_espace_stable_par_un_endomorphisme}
\begin{dfn}[Sous-espace stable]
     Soit $E$ un $\mathbb{K}$-espace vectoriel, $F$ un sous-espace vectoriel de $E$, et $u$ un endomorphisme de $E$.\newline
     On dit que $F$ est stable par $u$ si :
     \begin{equation}
     \forall x \in F, \, u(x) \in F
     \end{equation}
     On écrit alors que $u(F) \subset F$.\\[0.6\baselineskip]

     La restriction de $u$ à $F$ au départ et à l'arrivée est l'\emph{endomorphisme induit}\index{Endomorphisme!induit}.
\end{dfn}
\Attention{La simple restriction de $u$ à $F$ ($u_{|F}$) est une application de $F$ dans $E$ et ce n'est pas un endomorphisme, alors que l'endomorphisme induit va de $F$ à $F$.}\\[\baselineskip]
\needspace{7cm}
\subsubsection{Traduction matricielle}
On va maintenant voir, conformément au programme, la traduction en termes de matrices. On se place donc dans $E$ qui est cette fois de \emph{dimension finie} $n$.\\
Si on reprend le sous-espace $F$, on peut trouver une base $(e_1, \cdots , e_p)$. Cette base peut être complétée en une base de $n$ vecteurs de $E$ : $\mathcal{B}=(e_1, \cdots , e_p, \cdots, e_n)$. La matrice de $u$ dans $\mathcal{B}$ s'écrit : 
\[
  \begin{blockarray}{ccccc|cccc}
    & e_1 & \BAmulticolumn{2}{c}{\hdots} & e_p & \BAmulticolumn{3}{c}{\hdots} & e_n \\
    \begin{block}{c(cccc|cccc@{\hspace*{5pt}})}
    e_1 & \BAmulticolumn{4}{c|}{\multirow{4}{*}{\scalebox{2.5}{$B$}}}&\BAmulticolumn{4}{c}{\multirow{4}{*}{\scalebox{2.5}{$C$}}}\\
    \multirow{2}{*}{\vdots} & &&&&&&&\\
    & &&&&&&&\\
    e_p & &&&&&&&\\[2mm]
    \cline{1-9}% don't use \hline
    \vdots & \BAmulticolumn{4}{c|}{\multirow{2}{*}{\scalebox{1.5}{$0$}}}&\BAmulticolumn{4}{c}{\multirow{2}{*}{\scalebox{1.5}{$D$}}}\\
    e_n & &&&&&&&\\
    \end{block}
  \end{blockarray}
  \qquad\text{où $B$ est la matrice de l'endomorphisme induit.}
\]
% subsection sous_espace_stable_par_un_endomorphisme (end)
\section{Éléments propres d'un endomorphisme} % (fold)
\label{sec:elements_propres_d_un_endomorphisme}
\begin{theorem}[Droite stable]
     Une \emph{droite} est \emph{stable}\index{Droite!stable} par un endomorphisme $u$ \emphhs{ssi} elle est engendrée par un vecteur propre de $u$.
\end{theorem}
\subsection{Éléments propres}
\begin{dfn}[Valeur propre, vecteur propre]
     Soit $u$ un endomorphisme de $E$ un $\mathbb{K}$-espace vectoriel.\\ 
     Un scalaire $\lambda$ est appelé \emph{valeur propre}\index{Valeur!propre} de $u$ s'il existe un vecteur $x$ \emphhs{non nul} de $E$ tel que 
     \begin{equation}
     u(x) = \lambda x
     \end{equation}
     Si un tel vecteur $x$ existe, on l'appelera \emph{vecteur propre}\index{Vecteur!propre}.
\end{dfn}
\begin{dfn}[Sous-espace propre]
\label{dfn:sous-espace_propre}
     Avec les notations précédentes, on appelera \emph{\acrfull{sep}}\index{Sous-espace!propre} associé à une valeur propre $\lambda$ le sous-espace vectoriel $E_\lambda$ : 
     \begin{equation}
     E_\lambda = \mathrm{Ker} \left( u - \lambda \mathrm{Id} \right) 
     \end{equation}

     C'est donc le sous-espace de $E$ contenant $0$ et tous les vecteurs propres de $u$.
\end{dfn}
\subsection{Éléments propres en dimension finie} % (fold)
\label{sub:elements_propres_dim_finie}
\Attention{On se place dans un espace $E$ de \emphhs{dimension finie}. Les théorèmes et définitions qui suivent ne sont valables (au programme) que dans ces conditions. }
\begin{dfn}[Spectre]
     Le spectre d'un endomorphisme $u$ de $E$, noté $\mathrm{sp}(u)$, est l'ensemble de ses valeurs propres. 
\end{dfn}
\needspace{5cm}
\begin{theorem}[Famille finie de \gls{sep}]
     La somme d'une famille \emphhs{finie} de \gls{sep} $E_{\lambda_i}$ de valeurs propres $\lambda_i$ deux à deux distinctes est directe : 
     \begin{equation}
     \sum_i E_{\lambda_i} = \bigoplus_i E_{\lambda_i}
     \end{equation}
\end{theorem}
Le programme officiel précise le corrolaire qui va avec : 
\begin{theorem}[Famille de vecteurs propres]
     Toute famille de vecteurs propres dont les valeurs propres associées sont deux à deux distinctes est libre. 
\end{theorem}
\begin{theorem}
  Pour $u$ un endomorphisme de $E$ de dimension finie $n$, le \textbf{spectre} de $u$ est fini, et de cardinal \emphhs{au plus} $n$.
\end{theorem}
\begin{theorem}
     Deux matrices semblables ont même spectre.
\end{theorem}
\begin{itheorem}[Endomorphismes commutant]
    Soient $u$ et $v$ sont deux endomorphismes d'un espace $E$ de dimension finie. \\
      Si $u$ et $v$ commutent, alors tout sous-espace propre de $u$ est stable par $v$. 
    \end{itheorem}
    \begin{proof}
        Soit $\lambda$ une valeur propre de $u$, et $E_\lambda$ l'espace propre associé. On a :
	       \[
	      E_\lambda = \mathrm{Ker} (u - \lambda I)
	        \]
        Soit $x_\lambda$ de $E_\lambda$. $x_\lambda$ est un vecteur propre de $u$.
        Pour montrer qu'un sous-espace propre de $u$ est stable par $v$, \emphh{il faut montrer que $v(x_\lambda) \in \mathrm{Ker}(u - \lambda I)$}. Or :
        \begin{align*}
             (u - \lambda I)\circ v (x_\lambda) &= u\circ v(x_\lambda) - \lambda v( x_\lambda) \\
             &= v\circ u(x_\lambda ) - v(\lambda  x_\lambda) \\
             &= v\bigg( u(x_\lambda) - \lambda x_\lambda \bigg) 
             \intertext{D'où :}
             (u - \lambda I)\circ v (x_\lambda) &= 0
        \end{align*}
        Donc $v$ est stable par tout \gls{sep} de $u$.\qed
	\end{proof}
\begin{dfn}[Éléments propres d'une matrice]
		Soit $A$ une matrice carrée de $E$ un espace de dimension finie. \newline
		On appelle \emph{valeur propre}\index{Valeur!propre} de $A$ un scalaire $\lambda$ pour lequel il existe $X$ tel que :
		\begin{equation}
			  AX = \lambda X
        \end{equation}
        Si ce vecteur $X$ existe, on l'appelle \emph{vecteur propre}\index{Vecteur!propre} de la matrice $A$ pour la valeur propre $\lambda$.\\[\baselineskip]
        Par extension, on définit le \gls{sep} d'une matrice de manière similaire à la définition~\ref{dfn:sous-espace_propre} page~\pageref{dfn:sous-espace_propre}. L'ensemble des valeurs propres d'une matrice forme son spectre $\mathrm{sp}(A)$.
\end{dfn}
% subsection elements_propres_dim_finie (end)
% section elements_propres_d_un_endomorphisme (end)
\section{Polynôme Caractéristique}
Pour une matrice carrée $M$, on cherche un polynôme dont les valeurs propres sont les racines. C'est alors qu'est né le polynôme caractéristique. 
\begin{dfn}[Polynôme caractéristique]
Soit $u$ un endomorphisme de $E$, un $\mathbb{K}$-espace vectoriel de \emphh{dimension finie} $n$. Soit $M$ sa matrice dans une base associée $\mathcal{B}$. \\
Le \emph{polynôme caractéristique} de $u$, noté $\chi_u$, est le déterminant de l'application $\left( u - X \, \mathrm{Id}_E \right)$\newline
De même, le \emph{polynôme caractéristique}\index{Polynome@Polynôme!caracteristique@caractéristique} de la matrice $M$, noté $\chi_M$, est le déterminant de la matrice $\left( M - X \, I_n \right)$
\begin{equation}
    \chi_u = \det\left( u - X \, \mathrm{Id}_E \right) \\ 
    \chi_M = \det\left( M - X \, I_n \right)
\end{equation}
Ce polynôme est de degré $n$. Le polynôme caractéristique doit être \emphhs{unitaire}.\newline Bien sûr, on a :
\begin{equation}
\chi_u = \chi_M
\end{equation}
\end{dfn}
\begin{theorem}
     Deux matrices semblables ont même polynôme caractéristique.
\end{theorem}
\begin{proof}% Je garde ou j'enlève cette démo trop simple ?
     \textbf{(Facile)} \\
     Soient $A$ et $A'$ nos deux matrices semblables. Elles représentent le même endomorphisme $u$ dans des bases différentes. Donc 
     \[
        \left\lbrace
        \begin{array}{r c l}
          \chi_u &=& \chi_A \\
          \chi_u &=& \chi_{A'}
        \end{array}
        \right.
     \]
     D'où $\chi_A = \chi_{A'}$.\qed
\end{proof}
\begin{itheorem}[Valeurs des coefficients de degrés $0$ et $n-1$]\label{thm:coeff_polynome_caracteristique}
Pour une matrice $M$ de rang $n$, on peut obtenir quelques coefficients du polynôme caractéristique : 
\[ 
\chi_M = (-1)^n X^n + \textcolor{couleur1}{(-1)^{n-1}\,\tr\left( M \right)}\times X^{n-1} + \cdots + \textcolor{couleur2}{\det\left( M \right)}
\]
Pour une matrice de rang 2, le polynôme caractéristique est donc donné par
\begin{empheq}[box=\ibox]{equation}
 \chi_M = X^2 - \tr (M)\, X + \det (M)
\end{empheq}

\end{itheorem}
\begin{proof}
     Il suffit de développer le polynôme caractéristique, en sachant que les valeurs propres sont les racines, puis d'identifier.
\end{proof}
%TODO: multiplicité valeur propres
\begin{theorem}[Polynôme caractéristique d'une matrice triangulaire]
% Merci Wikipedia pour la matrice
     Soit $A=\begin{pmatrix}
a_{1,1} & a_{1,2} & \ldots & \ldots & a_{1,n} \\
0       & a_{2,2} & \ldots & \ldots & a_{2,n} \\
\vdots  & \ddots  & \ddots &        & \vdots  \\
\vdots  &         & \ddots & \ddots & \vdots  \\
0       & \ldots  & \ldots & 0       & a_{n,n}
\end{pmatrix}$ une matrice triangulaire supérieure.\\[0.6\baselineskip]
Le polynôme caractéristique de cette matrice est 
\begin{equation}
\chi_A = \det (A - X\, I_n) = \prod (a_{i,i} - X)
\end{equation}
\end{theorem}
\begin{theorem}[Polynôme caractéristique d'un endomorphisme induit]
     Soit $u$ un endomorphisme de $E$. Le polynôme caractéristique d'un endomorphisme induit de $u$ divise $\chi_u$.
\end{theorem}
\section{Endomorphismes et matrices carrées diagonalisables}
\label{sec:diagonalisation}
\subsection{Endomorphisme diagonalisable}
\begin{dfn}[Endomorphisme diagonalisable]
     On dit qu'un endomorphisme $u$ de $E$ est \emph{diagonalisable}\index{Diagonalisable!endomorphisme} s'il existe une base $\mathcal{B}$ de $E$ dans laquelle la matrice de $u$ est \emphhs{diagonale}.\\
     On verra au théorème~\ref{thm:caracterisation_diagonalisation} que cette base $\mathcal{B}$ est constituée des vecteurs propres.
\end{dfn}
\begin{dfn}[Quelques définitions]
Quelques définitions portant sur les polynômes : 
    \begin{description}
    \item[Racine simple] Une racine $\alpha$ du polynôme $P$ est dite simple si elle n'est pas multiple. On dit que son \uline{ordre de multiplicité est égal à $1$}.
    \item[Polynôme scindé] $P$ est scindé s'il peut s'écrire comme \uline{le produit de polynômes du premier degré}.
    \end{description}
\end{dfn}
\begin{itheorem}[Caractérisation de la diagonalisation]\label{thm:caracterisation_diagonalisation}
    On donne des équivalences à ``$u$ diagonalisable'' : 
    \renewcommand{\theenumi}{\roman{enumi}}% Compter en (i), (ii)...
    \begin{enumerate}
        \item $E$ admet une base formée des vecteurs propres de $u$ ;
        \item $E$ est somme directe des espaces propres de $u$ : $E = \bigoplus_{\lambda\in \mathrm{sp}(u)} E_\lambda$ ;
        \item $\dim E = \sum \dim E_\lambda $ ;
        \item le polynôme caractéristique $\chi_u$ de $u$ est scindé, et $\omega (\lambda ) = \dim (E_\lambda)$ ;
        \item le polynôme minimal $\Pi_u$ de $u$ est scindé à racines simples ;
        \item $u$ possède au moins un polynôme annulateur scindé à racines simples ;
        \item $u$ admet pour matrice une matrice diagonalisable.
    \end{enumerate}
\end{itheorem}
Enfin, une condition juste suffisante pour diagonaliser un endomorphisme : 
\begin{theorem}[Condition suffisante de diagonalisation]
     Si $u$ admet $n = \dim E$ valeurs propres distinctes deux à deux, alors $u$ est diagonalisable.
\end{theorem}
%
\subsection{Matrice diagonalisable}
\begin{dfn}[Matrice diagonalisable]
     Une matrice carrée $A$ est dite \emph{diagonalisable}\index{Diagonalisable!matrice} si l'endomorphisme de $\mathbb{K}^n$ canoniquement associé est diagonalisable.
\end{dfn}
On se servira plutôt du théorème suivant comme définition :
\begin{itheorem}
      Une matrice carrée $A$ est diagonalisable \ssi{} elle est semblable à une matrice diagonale $D$ : 
     \begin{equation}
      \exists P\in \mathrm{GL}_n(\mathbb{K})\, \qquad A = PDP^{-1}
      \end{equation} 
      $P$ est la matrice de passage de la base canonique de $\mathbb{K}^n$ à une base de vecteurs propres de $A$.
\end{itheorem}
On peut alors traduire matriciellement tous les théorèmes vus dans la section précédente, notamment le théorème~\ref{thm:caracterisation_diagonalisation} page~\pageref{thm:caracterisation_diagonalisation} qui caractérise la diagonalisation.
% section diagonalisation (end)
\section{Endomorphismes et matrices carrées trigonalisables} % (fold)
\label{sec:endomorphismes_et_matrices_carrees_trigonalisables} 
\begin{dfn}[Endomorphisme trigonalisable]
     On dit qu'un endomorphisme $u$ de $E$ est \emph{trigonalisable}\index{Trigonalisable!endomorphisme} s'il existe une base $\mathcal{B}$ de $E$ dans laquelle la matrice de $u$ est \emphhs{triangulaire supérieure}.
\end{dfn}
% Bizarrement, le programme officiel définit dans le sens opposé la trigonalisation. En trigonalisation, c'est à partir d'une matrice, en diagonalisation, c'est à partir d'un endomorphisme.
\begin{dfn}[Matrice trigonalisable]
     Une matrice carrée $A$ est dite \emph{trigonalisable}\index{Trigonalisable!matrice} si elle est semblable à une matrice triangulaire supérieure $T$ : 
     \begin{equation}
      \exists P\in \mathrm{GL}_n(\mathbb{K})\, \qquad A = PTP^{-1}
      \end{equation} 
\end{dfn}
\begin{theorem}[Autre ``définition'']
     Une matrice carrée est trigonalisable \ssi{} l'endomorphisme $u$ canoniquement associé est trigonalisable.
\end{theorem}
\begin{itheorem}[Caractérisation de la trigonalisation]
     Un endomorphisme $u$ est trigonalisable \ssi{} son polynôme caractéristique ou son polynôme annulateur est scindé.\\
     Plus généralement, $u$ est trigonalisable \ssi{} $u$ possède au moins un polynôme annulateur scindé.
\end{itheorem}
\begin{theorem}[Trigonalisation dans $\mathbb{C}$]
     Tout endomorphisme d'un $\mathbb{C}$-espace vectoriel est trigonalisable.
\end{theorem}
Ces deux théorèmes peuvent également se traduire matriciellement.
% section endomorphismes_et_matrices_carrees_trigonalisables (end)
\section{Endomorphismes nilpotents} % (fold)
\label{sec:endomorphismes_nilpotents}
\subsection{Définition}
\begin{dfn}[Endomorphisme nilpotent]
     On dit qu'un endormorphisme $u$ est \emph{nilpotent}\index{Nilpotent} d'indice $p\ge 1$ si $u^p = 0$ avec $u^{p-1} \neq 0$.
\end{dfn}
\subsection{Propriétés en dimension finie} % (fold)
\label{sub:proprietes_en_dimension_finie}
\begin{theorem}[Endomorphisme nilpotent trigonalisable]
     Un endomorphisme $u$ dans un espace $E$ de dimension finie est nilpotent \ssi $u$ est trigonalisable avec $0$ pour seule valeur propre.
\end{theorem}
%TODO: demo? (Très facile)
\begin{theorem}[Majoration de l'indice de nilpotence]
     Dans un espace $E$ de dimension $n$, l'indice de nilpotence d'un endomorphisme ne dépasse pas $n$. \par
     Si $u$ est nilpotent d'indice $n$, il existe une base $\mathcal{B}$ dans laquelle la matrice de $u$ est de la forme :
     \begin{equation}
     J_n=
     \begin{pmatrix}
             0   &         1       &    0   & \cdots &    0   \\
          \vdots &      \ddots     & \ddots &        & \vdots \\
          \vdots &                 & \ddots & \ddots & \vdots \\
          \vdots & \scalebox{2}{0} &        & \ddots &    1   \\
             0   &      \cdots     & \cdots & \cdots &    0  
          %   0   & \hdotsfor{3} & 0
     \end{pmatrix}
     \end{equation}
\end{theorem}
% subsection proprietes_en_dimension_finie (end)
% section endomorphismes_nilpotents (end)
\section{Polynômes d'un endomorphisme} % (fold)
\label{sec:polyn_mes_d_un_endomorphisme}
\begin{dfn}[Polynôme d'un endomorphisme]
     Soit $E$ un $\mathbb{K}$-espace vectoriel, et $u$ un endomorphisme de $E$. \newline
     Pour tout polynôme $P = \sum_{k=0}^p a_k X^k$, on définit l'endomorphisme 
     \begin{equation}
     P(u) = \sum_{k=0}^p a_k u^k
     \end{equation}
     $P(u)$ est appelé \emph{polynôme de l'endomorphisme}\index{Polynome@Polynôme!endomorphisme@\emphi{d'un }endomorphisme} $u$.
\end{dfn}
\needspace{3cm}
\begin{theorem}[Morphisme d'algèbre $P \mapsto P(u)$]
     Pour $u$ dans $\mathscr{L}(E)$, l'application $P \mapsto P(u)$ est un \emphhs{morphisme d'algèbre} de $\mathbb{K} [X]$ dans $\mathscr{L}(E)$.\\
\end{theorem}
%todo Noyau et image de ce morphisme
\todo{Noyau et Image de ce morphisme}
\subsection{Polynôme minimal}
\begin{dfn}[Polynôme minimal]
% Il existe un \uline{unique} polynôme \uline{unitaire} $\Pi_{\varphi}$ appelé le polynôme minimal tel que pour tout morphisme $\varphi{}$, $\Pi_{\varphi}(\varphi{}) = 0$ \textcolor{couleurFonce!70}{CÀD} $\left( \Pi_\varphi \right) = \mathrm{Ker}(\varphi{})$
Soit $u$ un endomorphisme de $E$, $\mathbb{K}$-espace vectoriel. \\
On appelle \emph{polynôme minimal}\index{Polynome@Polynôme!minimal} l'unique polynôme $\Pi_u$ \emphhs{unitaire} et \emphhs{non nul}, s'il existe, tel que $\Pi_u$ annule $u$ et soit de plus petit degré ($\forall~Q$ diviseur de $\Pi_u$ distinct de $\Pi_u$, $Q(u)\neq 0$).
\end{dfn}
\begin{theorem}%TODO: fix title [Base de $\mathbb{K} [u]$]
     Si $d$ est le degré du polynôme minimal $P_u$ de $u$, alors la famille $(u^k)_{k \in \left[ 0 , d-1 \right] }$ est une base de $\mathbb{K} [u]$.
\end{theorem}
\begin{proof}
     Soit $d$ le degré minimal du polynôme minimal $P_u$. La famille $(\mathrm{Id}, u, u^2, \ldots , u^{d-1} )$ est libre. 
     %TODO: Finir cette demo (p. 84 dans le livre de Vuibert)
\end{proof}
\subsection{Valeur propre et polynôme annulateur} % (fold)
\label{sub:valeur_propre_et_polynome_annulateur}
\begin{theorem}
     Soit $u$ un endomorphisme de $E$. Soit $P$ un polynôme de $\mathbb{K} [X]$.\\
     Pour toute valeur propre $\lambda$ de $u$, $P(\lambda)$ est une valeur propre de $P(u)$.
\end{theorem}
\begin{proof}
     \begin{align*}
         u(x) = \lambda x   &\implies u^k(x) = \lambda^k x \\
                            &\implies P(u)(x) = \sum_k a_k~u^k(x) = P(\lambda)x
     \end{align*}
     On remarque que le vecteur propre associé à $\lambda$ est aussi associé à $P(\lambda )$.\qed
\end{proof}
\begin{itheorem}
     Si $P$ annule $u$, toute valeur propre de $u$ est une racine de $P$.
\end{itheorem}
\begin{proof}
     On se sert du théorème précédent : 
     \begin{align*}
          P(u) = 0 &\implies P(u)(x) = 0\\
                   &\implies P(\lambda)x = P(u)(x) = 0
     \end{align*}
     Puisque $x\neq 0$, on retrouve que $\lambda$ est racine de $P$.\qed
\end{proof}
% subsection valeur_propre_et_polynome_annulateur (end)
\subsection{\textsc{Cayley}-\textsc{Hamilton}} % (fold)
\label{sub:Cayley-Hamilton}
\begin{itheorem}[\textsc{Cayley-Hamilton}%
\index{Cayley-Hamilton@\textsc{Cayley-Hamilton}}]
Si $u$ est un endomorphisme d'un espace vectoriel \uline{fini}, alors le polynôme caractéristique $\chi_u$ est un polynôme annulateur de $u$. \\
Le polynôme caractéristique est donc un multiple du polynôme minimal. 
\end{itheorem}
% subsection Cayley-Hamilton (end)
% section polyn_mes_d_un_endomorphisme (end)
\section{Lemme de décomposition des noyaux} % (fold)
\label{sec:lemme_de_decomposition_des_noyaux}
%TODO: voir démo
\begin{subequations}
\begin{theorem}[Théorème de décomposition des noyaux]
     Soient deux polynomes $P$ et $Q$ de $\mathbb{K} [X]$. Pour $P$ et $Q$ \emphh{premiers entre eux} : 
     \begin{equation}
     \mathrm{Ker}\left[ (PQ)(u)\right] = \mathrm{Ker}\left[ P(u) \right] \oplus \mathrm{Ker}\left[ Q(u) \right]
     \end{equation}
\end{theorem}
Maintenant, on va voir le théorème dans une forme plus générale, qu'on déduit par récurrence sur le théorème précédent.
\begin{itheorem}[Théorème de décomposition des noyaux]
     Soit $E$ un $\mathbb{K}$-espace vectoriel, et $u$ un endomorphisme de $E$.\newline
     Si $A_1,\cdots,A_p$ sont des polynômes de $\mathbb{K}[X]$ \emphhs{premiers deux à deux}, alors :
     \begin{equation}
     \mathrm{Ker}\bigg( (A_1\times A_2 \times \cdots \times A_p)(u) \bigg) 
            = \bigoplus_{i=1}^p \mathrm{Ker}\big( A_i (u) \big)
     \end{equation}
     
\end{itheorem}
\end{subequations}
% section lemme_de_decomposition_des_noyaux (end)
\section{Polynômes annulateurs et diagonalisabilité} % (fold)
\label{sec:polynomes_annulateurs_et_diagonalisabilite}
%
\todo{Bouger \textsc{Grass}-dude}
\begin{theorem}[Formule de \textsc{Grassman}] % Qu'est-ce que ça fout là ?
     \index{Grassman@\textsc{Grassman}}Si $V$ et $W$ sont deux espaces vectoriels de \emphh{dimension finie} de $E$ alors :
     \begin{equation}
     \dim (V+W) = \dim V + \dim W - \dim (V\cap W)
     \end{equation}
\end{theorem}
% section polynomes_annulateurs_et_diagonalisabilite (end)
\section{Endomorphismes à polynôme minimal scindé} % (fold)
\label{sec:endomorphismes_polynome_minimal_scinde}
\begin{dfn}[Sous-espace caractéristique]
     Soit $u$ un endomorphisme trigonalisable de $E$. Son polynôme caractéristique $\chi_u$ est scindé : 
     \[
         \chi_u = \prod_{i=1}^k \big( X - \lambda_i \big)^{m_i}
     \]
     On appelle \emph{sous-espace caractéristique}\index{Sous-espace!caracteristique@caractéristique} de $u$ les sous-espaces : 
     \begin{equation}
     F_{\lambda_i}(u) = \mathrm{Ker} (u-\lambda_i \mathrm{Id}_E)^{m_i}
     \end{equation}
\end{dfn}
\begin{theorem}[Décomposition avec les sous-espaces caractéristiques]
     Soit $u$ un endomorphisme pour lequel il existe un polynôme annulateur scindé (c'est-à-dire $u$ est trigonalisable). \\
     L'endomorphisme $u_i : F_{\lambda_i} \to F_{\lambda_i}$ induit par $u$ dans $F_{\lambda_i}$ est la somme d'une homothétie et d'un endomorphisme nilpotent.
\end{theorem}
\begin{proof}
     On a :
     \[ u_i = \lambda_i \mathrm{Id}_E + (u_i - \lambda_i \mathrm{Id}_E) \]
     Ici, $\lambda_i \mathrm{Id}_E$ est une homothétie.\\
     De plus, $(u_i - \lambda_i \mathrm{Id}_E)^{m_i} = 0$ par définition de $F_{\lambda_i}$, donc $(u_i - \lambda_i \mathrm{Id}_E)$ est un endomorphisme nilpotent.\qed
\end{proof}
% section endomorphismes_polynome_minimal_scinde (end)
% chapter Reduction (end)
\chapter{Topologie des espaces vectoriels normés}
\begin{methode}
     \titre{Application continue}
     Soient $E$ et $F$ deux espaces vectoriels normés. Soit une application continue $f : E \to F$.
     \begin{itemize}
         \item $f : A\in E\to F$ conserve : 
         \begin{itemize}
            \item les parties compactes
            \item les parties connexes par arcs
         \end{itemize}
         \item $f^{-1} : F \to E$ conserve :
         \begin{itemize}
             \item les parties fermées
             \item les parties ouvertes
         \end{itemize}
     \end{itemize}
\end{methode}
\section{Normes et espaces vectoriels normés}
\subsection{Rappels} % (fold)
\label{sub:rappels}
\begin{dfn}[Espace vectoriel]
     Soit $\mathbb{K}$ un corps ($\mathbb{R}$ ou $\mathbb{C}$).\newline
     $E$ est un $\mathbb{K}$-\emph{espace vectoriel}\index{Espace!vectoriel} s'il respecte les conditions suivantes : 
     \begin{equation}
        \left| 
      \begin{array}{c l l l}
       (i)  &(E,+) \text{ est un }&\text{\textcolor{couleur4}{\bf\underline{groupe abélien}}}&\textbf{Groupe pour la loi }+\\
       (ii) &\forall x\in E,& \textcolor{couleur4}{{1\cdot x = x}}&\textbf{Neutre multiplicatif}\\
       (iii) &\forall (\alpha, \beta) \in \mathbb{K}^2, \forall x\in E,& \textcolor{couleur4}{{(\alpha + \beta) x = \alpha x + \beta x}} &\textbf{Distributivité pour }\boldsymbol{\mathbb{K}} \\
       (iv) &\forall \alpha \in \mathbb{K}, \forall (x,y) \in E^2,& \textcolor{couleur4}{{\alpha (x+y) = \alpha x + \alpha y}} &\textbf{Distributivité pour }\boldsymbol{E}\\
       (v) &\forall \alpha \in \mathbb{K}, \forall (x,y) \in E^2,& \textcolor{couleur4}{{\alpha \cdot (\beta \cdot y) = (\alpha \cdot \beta ) \cdot x}} &\textbf{Associativité}
      \end{array}
        \right.
     \end{equation}
     
\end{dfn}
% subsection rappels (end)
\subsection{Norme}
\begin{dfn}[Définition de la norme]
Soit $E$ un espace vectoriel de
$\mathbb{K}$. Une \emph{norme}\index{Norme} 
est une application $N : E \to \mathbb{R}^+$ vérifiant : 

\begin{equation}
\left| 
\begin{array}{c l l}
(i)&\forall x \in E, N(x)=0 \implies x=0 &\textbf{Séparation}\\
(ii)&\forall x \in E, \forall \lambda \in \mathbb{K}, N(\lambda x)=|\lambda|N(x) &\textbf{Homogénéité}\\
(iii)&\forall (x,y) \in E^2, N(x+y)\le N(x)+N(y)&\textbf{Inégalité triangulaire}\\
\end{array}
\right.
\end{equation}
\index{inegalite@Inégalité!triangulaire}

Et le couple $(E,N)$ est l'\index{Espace!norme@normé}\emph{espace
vectoriel normé} associé.
\end{dfn}
\begin{theorem}[Norme $N_2$\index{Norme!Euclidienne}]
$N_2 : f \mapsto \left( \int \limits_{[a,b]} |f|^2 \right)^{\frac{1}{2}}$ est une norme sur $\mathcal{C}\left( [a,b], \mathbb{K} \right)$
\end{theorem}
\subsection{Distance} % (fold)
\label{sub:distance}
Il y a plusieurs manières de définir la distance. Si on se place dans un espace vectoriel normé, on peut utiliser la norme pour définir la distance, comme dans la définition~\ref{dfn:Distance_Norme}. Sinon, si l'espace est quelconque, la distance peut avoir la définition générale suivante : 
\begin{dfn}[Distance dans un espace quelconque]
Soit $E$ un ensemble. On appelle \emph{distance} dans $E$ toute application $d : E\times E \to \mathbb{R}^+$ telle que :
     \begin{equation}
        \left| 
        \begin{array}{c l l l}
        (i)&\forall x \in E,& d(x,y)=0 \implies x=y &\textbf{Séparation}\\
        (ii)&\forall (x,y) \in E^2,& d(x,y) = d(y,x) &\textbf{Symétrie}\\
        (iii)&\forall (x,y,z) \in E^3,& d(x,z)\le d(x,y)+d(y,z)&\textbf{Inégalité triangulaire}\\
        \end{array}
        \right.
        \end{equation}
\end{dfn}
\begin{dfn}[Distance associée à une norme]\label{dfn:Distance_Norme}
     Soit $(E,\| \cdot \|)$ un espace vectoriel normé\newline
     La \emph{distance}\index{Distance} $d$ associée à la norme $\| \cdot \|$ est l'application :
     \begin{equation}
     \begin{array}{r @{\,}c@{\,} c @{\,}c@{\,} @{\,} l}
     d &: &E^2 &\to &\mathbb{R}^+ \\
        & &(x,y) &\mapsto & \| x - y \| 
        \end{array}
     \end{equation}
\end{dfn}
% subsection distance (end)
\subsection{Boules} % (fold)
\label{sub:boules}
\begin{dfn}[Boule]
Dans une espace vectoriel normé $(E,N)$, on définit les boules centrées en $a$ et de rayon $r$
    \begin{tabular}{>{\begin{bf}}r<{\end{bf}}@{ de rayon $r$ centrée en $a$ : $\quad$ } l}
    Boule ouverte & ${x \in E | N(x-a)< r } $ \\
    Boule fermée & ${x \in E | N(x-a)\le r } $ \\
    Sphère & ${x \in E | N(x-a)= r } $ \\
    \end{tabular}
\end{dfn}
% subsection boules (end)
\subsection{Parties, suites, fonctions bornées} % (fold)
\label{sub:parties_suites_fonctions_bornees}
\begin{dfn}[Partie bornée]
     Une partie $A$ de $E$ est dite \emph{bornée} s'il existe une boule $B(a,r)$ la contenant : 
     \begin{equation}
     \exists(a,r),\, A \subset B(a,r)
     \end{equation}
\end{dfn}
\begin{theorem}[\acrshort{CNS} d'une partie bornée]
     Une partie $A$ de $E$ est \emph{bornée} \ssi{} il existe un réel $M$ tel que :
     \begin{equation}
     \forall x\in A,\, \| A \| \le M
     \end{equation}
\end{theorem}
\Attention{Le caractère borné d'une partie dépend de la norme. Il peut donc arriver qu'une partie soit bornée pour une norme et pas pour une autre.}
\begin{dfn}[Application bornée]
     Une application $f:X\to E$ est dite \emph{bornée} si l'ensemble $\lbrace f(x),\, x\in X\rbrace $ est borné.
\end{dfn}
% subsection parties_suites_fonctions_bornees (end)
\subsection{Produit scalaire} % (fold)
\label{sub:produit_scalaire}
\begin{dfn}[Produit scalaire]\label{dfn:ProduitScal}
Le \emph{produit scalaire}\index{Produit scalaire} est défini comme suit : 
\[
\begin{array}{c r l l}
(i)&\forall x \in E\backslash \{ 0 \} , &\uline{\varphi(x,x)>0}&\text{\textbf{(définie positive)}}\\
(ii)&\forall (x,y) \in E^2, &\uline{\varphi (x,y) = \varphi (y,x)} &\text{\textbf{(symétrie)}}\\
(iii)&\forall x \in E, &\uline{y \mapsto \varphi(x,y) \text{ est linéaire}}&\text{\textbf{(linéaire à droite)}}
\end{array}
\]
On peut associer un produit scalaire $(\cdot | \cdot )$ à un espace vectoriel $E$. L'espace $(E, (\cdot | \cdot ))$ est appelé \emph{espace préhilbertien}\index{Espace!prehilbertien@préhilbertien}.
\end{dfn}
\textbf{Remarque :} La symétrie et la linéarité impliquent la linéarité à gauche, donc la bilinéarité du produit scalaire.
\begin{theorem}[Continuité du produit scalaire]
     Le produit scalaire est une application bilinéaire \emphhs{continue} de $E\times E$ dans $\Reel{}$.
\end{theorem}
\begin{itheorem}[Norme associée]\label{NormeAssocieProdScal} 
$x\mapsto \sqrt{(x|x)}$ définit une norme sur $E$. On la note $||x||$, et $||x||^2 = (x|x)$.
\end{itheorem}
\begin{itheorem}[Inégalité \textsc{Cauchy-Schwarz}\index{inegalite@Inégalité!cauchy@de \textsc{Cauchy-Schwarz}}]
\begin{subequations}
\begin{align}
\textcolor{Red}{|}(x|y)\textcolor{Red}{|} &\le \sqrt{(x|x)} \times \sqrt{(y|y)}
\intertext{qu'on peut aussi écrire :}
\textcolor{Red}{|}(x|y)\textcolor{Red}{|}&\le \| x\| \times \| y\|
\end{align}
\end{subequations}
avec égalité si et seulement si $(x,y)$ est une famille liée.
\end{itheorem}
% subsection produit_scalaire (end)
\section{Suites d'éléments d'un espace vectoriel normé} % (fold)
\label{sec:suites_evn}
\begin{dfn}[Convergence d'une suite]
    On dit que la suite des $(x_n)_n $ converge si l'une des conditions suivantes est vérifiée : 
    \begin{equation}
    \begin{array}{c l }
    (i) & \exists l \in E \text{ tel que } \big(N(x_n - l) \big) \xrightarrow[n\to +\infty]{} 0 \\
    (ii) & \forall \varepsilon > 0, \exists n_0 \text{ tel que } \big( n \geq n_0 \implies N(x_n - l) < \varepsilon \big) \\
    (iii) & \forall \varepsilon > 0, \exists n_0 \text{ tel que } \big( n \geq n_0 \implies x_n \in B(l,\varepsilon) \big)
    \end{array}
    \end{equation}
\end{dfn}
% section suites_evn (end)
\section{Comparaison des normes} % (fold)
\label{sec:comparaison_des_normes}
\begin{dfn}[Normes équivalentes]
     On dit que deux normes $N_1$ et $N_2$ sont équivalentes si
     \begin{equation}
     \exists (c,C)\in {\mathbb{R}^{\textcolor{couleurImp}{+}}}^2,\, \forall x \in E,\, cN_1(x) \le N_2(x) \le CN_1(x)
     \end{equation}     
\end{dfn}
\begin{itheorem}[Dimension finie]
     Dans un espace vectoriel de dimension finie, toutes les normes sont équivalentes.
\end{itheorem}
% section comparaison_des_normes (end)
\section{Complets} % (fold)
\label{sec:complets}
\begin{dfn}[Complet]\index{Complet}
$A$ est un \emph{complet} si toute suite de Cauchy $(c_n)_n \in A$ admet une limite $l\in A$\\
CÀD si \uline{toute suite de Cauchy est convergente}
\end{dfn}
\textbf{Remarque :} Intuitivement, un espace est complet s'il n'a pas de \enquote{trou}. $\mathbb{Q}$ n'est pas complet, car $\sqrt{2}$ n'est pas dans $\mathbb{Q}$.

\begin{dfn}[Espace de \textsc{Banach}]
Un \emph{espace de \textsc{Banach}}\index{banach@\textsc{Banach}!Espace} est un espace-vectoriel \uline{normé et complet}.
\end{dfn}
% section complets (end)
\section{Parties compactes d'un espace normé} % (fold)
\label{sec:compacts}
\begin{itheorem}[\textsc{Bolzano-Weierstrass}]
     Toute suite réelle \emphhs{bornée} possède au moins une valeur d'adhérence.\par
     Par extension : toute suite bornée dans un espace vectoriel normé de dimension finie possède au moins une valeur d'adhérence.
     \index{Bolzano@\textsc{Bolzano}!Weierstrass@-\textsc{Weierstrass}}
\end{itheorem}
On va utiliser cette propriété pour définir un compact :
\begin{dfn}[Compact]\index{Compact}
$A$ est un \emph{compact} si toute suite d'éléments $(x_n)_n \in A$ a au moins une valeur d'adhérence\\
CÀD \uline{on peut extraire de $(x_n)_n$ une suite $\left( x_{\varphi(n)} \right)
_n$ qui converge dans $A$}
\end{dfn}
\begin{itheorem}
     Soit $E$ un espace vectoriel. Les parties compactes de $E$ sont fermées et bornées.
\end{itheorem}
\begin{theorem}[Compacts en dimension finie]
     Soit $E$ un espace vectoriel de dimension finie. \newline
     Une partie de $E$ est compacte \ssi elle est fermée bornée.
\end{theorem}
\begin{theorem}
     Toute partie fermée d'un compact est compact
\end{theorem}
\begin{theorem}
     Soit $E$ un espace vectoriel normé. Soit $A$ un compact de $E$.\\
     Une suite d'éléments de $A$ converge \ssi elle possède une unique valeur d'adhérence : 
     \begin{equation}
     \forall (x_n)_n \in A,\, x_n \text{ converge } \Leftrightarrow \exists ! l,\, x_n\to l
     \end{equation}
\end{theorem}
% section compacts (end)
\section{Applications continues sur une partie compacte} % (fold)
\label{sec:applications_continues_sur_une_partie_compacte}
\begin{itheorem}[Image d'une partie compact]
     Soit $A$ une partie d'un espace vectoriel normé $E$. Soit $F$ un espace vectoriel normé. Soit $f : A \in E \to F$.\\
     Si $f$ est continue, l'image de tout compact de $A$ est un compact de $F$.
\end{itheorem}
\begin{theorem}
     Soit $E$ un espace vectoriel normé. Soit une application $f : E \to \mathbb{R}$. \\
     Si $f$ est continue, alors elle est bornée et \emphhs{atteint ses bornes}.
\end{theorem}
\begin{itheorem}[Théorème de \textsc{Heine}\index{Heine@\textsc{Heine}}]
    Si $(E,N)$ et $(F,N)$ sont des espaces vectoriels normés, $A$ une partie \emphh{compacte} de $E$, \linebreak si $f\in \mathcal{C}(A,F)$, alors $f$ est \emphh{uniformément continue}.
\end{itheorem}
% section applications_continues_sur_une_partie_compacte (end)
\section{Connexité par arcs} % (fold)
\label{sec:connexe_par_arcs}
\subsection{Convexité}\label{sub:convexe_connexite}
\begin{dfn}[Convexe]
    \begin{minipage}{0.6\textwidth}
        Un ensemble $E$ est \emphh{convexe}\index{Convexe} si : 
    \begin{equation}
    \forall (x,y) \in E^2, \forall t \in [0,1], \boxed{tx + (1-t)y \in E}
    \end{equation}
    Intuitivement, un ensemble est convexe si on peut relier deux points avec une ligne contenue dans cet ensemble.
    \end{minipage}\hspace{5mm}
    \begin{minipage}{0.3\textwidth}
    \begin{tikzpicture}
        \draw[thick, color=couleurImp] (0.2,0.2) -- (2.7, 0.5);
        \filldraw[fill = couleurClaire, decorate, rounded corners=5mm]%
        (0,0) -- (-0.3,2) -- (3, 2.5) -- (3,0) -- (1.5,1) -- (0.2,-0.5) -- (0,0);
        \begin{scope}
        \clip[decorate, rounded corners=5mm] (0,0) -- (-0.3,2) -- (3, 2.5) -- (3,0) -- (1.5,1) -- (0.2,-0.5) -- (0,0);
        \draw[thick] (0.2,0.2) node{$\bullet$} -- (2.7, 0.5) node{$\bullet$};
        \end{scope}
    \end{tikzpicture}
    \captionof{figure}{Un ensemble non convexe}
    \end{minipage}
\end{dfn}
\begin{dfn}[Fonction convexe]
\label{dfn:convexe_connexite}
   \begin{minipage}{0.75\textwidth}
    Une fonction $f : I \mapsto \Reel{}$ est dite convexe si :
   \begin{equation}
   \forall (a,b) \in I^2, \forall t \in ]0,1[, \boxed{f(ta + (1-t)b) \le tf(a) + (1-t)f(b)}
   \end{equation}
    \end{minipage}\hspace{2mm}
    \begin{minipage}{0.2\textwidth}
    \begin{tikzpicture}
        \draw [->, >=stealth] (0,0) -- (3.2,0);
        \draw [->, >=stealth] (0,0) -- (0,3.2);
        \draw [thick] (0,0).. controls (0.8,0.2) and (2,1) .. (3,3);
        \draw [color=couleurNoirClair, densely dashed] (0,0) -- (3,3);
    \end{tikzpicture}
    \captionof{figure}{Une fonction convexe}
    \end{minipage}
\end{dfn}
\begin{theorem}[Convexe dans \Reel]
    $I$ de $\Reel{}$ est convexe \emphh{si et seulement si} $I$ est un intervalle de $\Reel{}$
\end{theorem}
\subsection{Connexité}
\begin{dfn}[Connexe par arcs]
    Une partie $A$ d'un espace vectoriel normé $(E,N)$  est \emph{connexe par arcs}\index{Connexe par arcs} \linebreak si, pour tous points $a$ et $b$ de $E$, il existe une fonction $f:[0,1] \to E$ \emphh{\uline{continue}} telle que 
    \begin{equation}
    \left\lbrace
    \begin{gathered}
    f(0)=a \\
    f(1)=b \\
    f\big( [0,1] \big) \subset A
    \end{gathered}
    \right.
    \end{equation}
\end{dfn}
\begin{theorem}[Relation d'équivalence]
     \og{} Il existe un chemin continu d'un point $x$ à un point $y$\fg{} %shameless copy from Vuibert. PS guys... I love you
     est une relation d'équivalence sur une partie $A$ de $E$.\\
     Les \emph{composantes connexes}\index{Composante connexe} sont les classes d'équivalences de $A$.
\end{theorem}
\begin{theorem}[Connexe dans \Reel{}]
    $A$ non vide de $\Reel{}$ est connexe par arcs \emphh{si et seulement si} $A$ est un intervalle de $\Reel{}$
\end{theorem}
\begin{theorem}[Image continue d'une partie connexe]
     Soient $E$ et $F$ deux espaces vectoriels. Soit l'application $f : A\in E \to F$.\\
     Si $f$ est continue, alors l'image de toute partie connexe par arcs est connexe par arcs dans $F$.
\end{theorem}
\begin{itheorem}[Théorème des valeurs intermédiaires]
     Soit $E$ un espace vectoriel normé. Soit $A$ une partie connexe par arcs de $E$. \\
     Soit $f : A \to \mathbb{R}$ une application continue qui atteint $(c,d)\in \mathbb{R}$. Alors $f$ atteint toute valeur $f(x) \in [c,d]$.
\end{itheorem}
% section connexe_par_arcs (end)
\section{Topologie} % (fold)
\label{sec:topologie}
Voici plusieurs définitions utiles à l'étude d'espaces vectoriels normés :
\begin{dfn}[Ouvert]
Une partie $E$ est un \emph{ouvert}\index{Ouvert} si, pour tout élément $x$ de $E$, il existe une boule centrée en $x$ inclue dans $E$ (\textit{cf. }\textsc{Figure}~\ref{fig:OuvertFerme})
\end{dfn}
\begin{dfn}[Fermé]
Un espace vectoriel $F$ est dit \emph{fermé}\index{ferme@Fermé} si son complémentaire $\overline{F}$ est un ouvert
\end{dfn}
%
\begin{minipage}{0.5\textwidth}
Pour différencier un ouvert d'un fermé, prenons le schéma ci-contre : \\
La partie en \textbf{\textcolor{Red}{rouge}} est un ouvert noté $O$, celle en \textbf{\textcolor{Blue}{bleue}} est un fermé noté $F$.\\
En effet : il n'existe aucun disque centré en $y \in O+F$ inclus dans la partie $O+F$, donc $O+F$ n'est pas un ouvert. Par contre, pour tout point $x \in O$, on peut trouver une boule inclue dans $O$.
\end{minipage}\hspace{6pt}
\begin{minipage}{0.4\textwidth}
\begin{tikzpicture}[scale=1]
    \draw [Black!30, thin, fill = Black!2] (-3.5,-2.3) -- (-3.5,2.3) -- (3.5,2.3) -- (3.5,-2.3) -- (-3.5,-2.3);
    \draw [Black, densely dashed, pattern=north west lines, pattern color=Blue!60] (30:2) circle(0.7);
    \fill [Red] (0,0) circle(2);
    \draw [Red!30] (120:1.7) node{$O$};
    \draw [line width=3pt, Blue] (0,0) circle(2);
    \draw [Blue] (-120:2.3) node{$F$};
    \fill [White, opacity = 0.3] (30:2) circle(0.7);
    \begin{scope}
        \clip (0,0) circle(2.05);
        \draw [White, densely dashed] (30:2) circle(0.7);
    \end{scope}
    \draw (30:2) node{$+$} (30:2) node[below left]{$y$};
    \fill [White, opacity = 0.3] (-120:0.4) circle(0.7);
    \draw [White!70, densely dashed] (-120:0.4) circle(0.7);
    \draw (-120:0.4) node{$+$} (-120:0.4) node[above left]{$x$};
\end{tikzpicture}
\captionof{figure}{Ouvert / Fermé}
\label{fig:OuvertFerme}
\end{minipage}
%
\begin{itheorem}[Caractérisation d'un fermé]
$F\subset E$ est un \emph{fermé}\index{ferme@Fermé} \uline{\emph{ssi}} toute suite convergente de F a sa limite dans F
\end{itheorem}

\begin{dfn}[Intérieur, Adhérence]
L'\emph{intérieur}\index{interieur@Intérieur} de $B$, noté $\overset{\circ}{B}$, est la \uline{réunion} des parties ouvertes \uline{contenues} dans $B$. \newline
L'\emph{adhérence}\index{Adherence@Adhérence} de $A$, notée $\overline{A}$ est \uline{l'intersection} des parties fermées \uline{contenants} $A$.
\end{dfn}

\begin{prop}
\begin{itemize}
\item $A$ fermé $\Leftrightarrow A=\overline{A}$
\item $\underbrace{\mathrm{Fr}(A)}_{\mathclap{\text{frontière}}}$ est un fermé
\item $\underset{\text{finie}}{\cup} \text{ fermés} = \text{fermé}$
\item $\cap \text{ fermés} = \text{fermé}$
\end{itemize}
\end{prop}

\begin{itheorem}
    Un \emphh{complet} $A$ d'un espace vectoriel normé $E$ est \emphh{\uline{fermé}}.\\*
    La réciproque (\textit{Les parties complètes sont les parties fermées}) est vraie si $E$ est un espace de \textsc{Banach}.
\end{itheorem}
% section topologie (end)
\chapter{Espaces Préhilbertiens réels}
Dans cette section, on se placera dans \Reel{}.
\begin{methode}
\titre{Définitions rapides}
\begin{description}
\item[Produit Scalaire] \textit{cf.} définition \ref{dfn:ProduitScal} page \pageref{dfn:ProduitScal}
\item[Éléments orthogonaux] $x$ et $y$ sont orthogonaux si $( x | y ) = 0 $
\item[Famille orthogonale] $(e_i|e_j) = \delta{i,j}$
\item[Distance de $x$ à une partie $F$] $d(x,F) = \| x - p_f(x) \|$
\end{description}
\end{methode}
\section{Orthogonalité}
\begin{dfn}[Éléments orthogonaux]
Deux éléments $x$ et $y$ sont orthogonaux si $(x|y)=0$
\end{dfn}
\begin{theorem}[Pythagore]
Si $(x_1, \cdots , x_n)$ est une famille d'éléments de $E$ deux à deux \emphhs{orthogonaux}, alors 
\begin{equation}
\left| \left| \sum_{i=0}^{n} x_i \right| \right| ^2 = \sum_{i=0}^n  \| x_i\| ^2
\end{equation}
\Attention{Ce sont bien des normes car $x_i$ au carré n'existe pas (qu'est-ce que le produit de deux vecteurs ?), du coup on utilise $\| \cdot \| ^2 = (\cdot | \cdot )$}
\end{theorem}
\begin{proof}
     On procède par récurrence. Avec une famille à deux éléments : 
     \begin{align*}
         \| x_1 + x_2 \|^2 &= \|x\|^2 + 2(x|y)+ \|y\|^2
         \intertext{Puisque $x_1$ et $x_2$ sont orthogonaux par hypothèse, on obtient :}
         \| x_1 + x_2 \|^2 &= \|x\|^2 + \|y\|^2
     \end{align*}
\end{proof}
\begin{itheorem}[Procédé d'orthonormalisation de \textsc{Schmidt}]\index{Procede@Procédé! Orthonormalisation@d'Orthonormalisation de \textsc{Schmidt}}
Pour toute base de $(e_i)$, il existe une base $(\varepsilon_i)$ telle que :
\begin{equation}
\left\lbrace
\begin{array}{l}
(\varepsilon_i)\text{ est une base \uline{orthonormée}}\\
\mathrm{Vect}(e_1,\cdots ,e_n) = \mathrm{Vect}(\varepsilon_1, \cdots ,\varepsilon_n)\\
(e_i|\varepsilon_i) > 0
\end{array}
\right.
\end{equation}

On aura souvent recours à compléter une base $(e_i)_k$ avec $n-k$ vecteurs orthonormaux aux $(\varepsilon_i)_k$ par le théorème de la base incomplète.
\end{itheorem}
\begin{theorem}[Inégalité de \textsc{Bessel} \index{inegalite@Inégalité!Bessel@de \textsc{Bessel}}]
Si $(e_i)$ est une base orthonormée : 
$\boxed{\sum_i (e_i|x)^2 \le \| x \|^2}$
\end{theorem}

\section{Automorphismes ortogonaux}
$u$ est un endomorphisme, donc il est \uline{linéaire}.
\begin{dfn}
    \renewcommand{\theenumi}{\roman{enumi}}% Compter en (i), (ii)...
    \begin{enumerate}
        \item $u$ un endomorphisme, il existe \emphh{un \uline{unique}} endomorphisme $u^*$ tel que
        \begin{equation*}
            \forall (x,y) \in E, (u(x)|y) = (x | u^*(y))
        \end{equation*}
        $u^*$ est l'\emph{adjoint}\index{Adjoint} de $u$.
        \item $u$ est \emph{autoadjoint}\index{autoadjoint@Autoadjoint} (symétrique) si $u^* = u$ 
        \item $u$ est un \emph{automorphisme orthogonal}\index{Automorphisme orthogonal} si $u^* = u^{-1}$. On note $u \in \mathcal{O}(E)$
    \end{enumerate}
\end{dfn}

\begin{prop}
    \renewcommand{\theenumi}{\roman{enumi}}% Compter en (i), (ii)...
    \begin{enumerate}
        \item Si $M_\mathcal{B}(u) = A$, alors $M_\mathcal{B}(u^*) = {}^tA$
        \item $\mathrm{Ker}(u^*) = \left[ \mathrm{Im}(u) \right]^\perp$
        \item $\mathrm{Im}(u^*) = \left[ \mathrm{Ker}(u) \right]^\perp$
        \item $\chi_u = \chi_{u^*}$
        \item $(u\circ v)^* = v^* \circ u^*$
    \end{enumerate}
\end{prop}

\begin{theorem}[Caractérisation d'un automorphisme orthogonal]
    $u$ est un automorphisme orthogonal si les assertions suivantes (équivalentes) sont vérifiées :
    \renewcommand{\theenumi}{\roman{enumi}}% Compter en (i), (ii)...
    \begin{enumerate}
        \item $u$ conserve la norme
        \item $u$ conserve le produit scalaire
        \item $u\left(\mathcal{B}_\text{orthonormée}\right) = \mathcal{B}'_\text{orthonormée}$
        \item $\forall \mathcal{B'}_\text{orthonormée}, \exists \mathcal{B}_\text{orthonormée}$ telle que $u\left(\mathcal{B}_\text{orthonormée}\right) = \mathcal{B}'_\text{orthonormée}$
        \item $\exists \mathcal{B}_\text{orthonormée} $ telle que $\left| \begin{gathered}
            U{}^tU = I_n\\
            \text{ ou }\\
            {}^tUU = I_n
        \end{gathered}\right|$ où $U = M_\mathcal{B}(u)$
    \end{enumerate}
\end{theorem}
\begin{itheorem}[Théorème spectral]
    Tout \emphh{endomorphisme auto-adjoint} est diagonalisable, et il existe une base orthonormale de vecteurs propres de $u$.\par

    Toute \emphh{matrice symétrique \uline{réelle}} est diagonalisable. On peut aussi dire : 
    \begin{equation}
    \forall A \in S_n, \exists  \left| \begin{gathered} P \in \mathcal{O}(n) \\ D \in \mathcal{D}_n(\Reel) \end{gathered} \right. \text{ tel que } A = PDP^{-1} = PD{}^tP
    \end{equation}
\end{itheorem}
\section{Isométries vectorielles d'un espace euclidien} % (fold)
\label{sec:isometries_vectorielles_d_un_espace_euclidien}
\begin{dfn}[Isométrie vectorielle d'un espace euclidien]
     Soit $E$ un espace euclidien.\\
     On dit que $u\in \mathcal{L}(E)$ est une \emph{isométrie vectorielle}\index{Isometrie@Isométrie!vectorielle} si $u$ est un endomorphisme qui \emphhs{conserve la norme} :
     \begin{equation}
     \forall x\in E,\qquad \|u(x)\| = \| x\|
     \end{equation}
\end{dfn}
\begin{theorem}[Bijectivité]
     Une isométrie vectorielle est bijective.
\end{theorem}
\begin{proof}
     Une isométrie vectorielle est injective : 
     \begin{align*}
         u(x) = 0 &\implies \| u(x)\| = 0\\ 
                  &\implies \| x \| = \| u(x)\| = 0\\ 
                  &\implies x=0
     \end{align*}
     Puisque $E$ est un espace euclidien, il est de dimension finie. Ainsi injectivité~$\Leftrightarrow$~bijectivité, et $u$ est bijective.\qed
\end{proof}
\begin{theorem}[\acrshort{CNS} avec le produit scalaire]
     $u\in \mathcal{L}(E)$ est une \emphh{isométrie vectorielle} \ssi{} $u$ conserve le produit scalaire : 
     \begin{equation}
     \forall (x,y)\in E^2,\qquad \Big( u(x) \Big| u(y) \Big) = \big( x|y\big) 
     \end{equation}
\end{theorem}
\begin{proof}
     Commençons par le sens \og{}conserve le produit~scalaire~$\implies$~isométrie~vectorielle\fg{} : 
     \begin{equation*}
          \|u(x) \| = \sqrt{\big( u(x) | u(x) \big) }= \sqrt{\big( x | x \big) } = \| x \|
     \end{equation*}
     Réciproquement, montrons que \og{}conserve le produit~scalaire~$\Longleftarrow$~isométrie~vectorielle\fg{}.\newline
     Si $u$ est une isométrie vectorielle, d'après les identités de polarisation : 
     \begin{align*}
          \left( u(x) | u(y) \right) 
    &= \frac{1}{4} \left( \| u(x) + u(y) \|^2 - \| u(x) - u(y) \|^2 \right) \\
    &= \frac{1}{4} \left( \| x+y \|^2 - \| x-y \|^2 \right) \\ % Par linéarité de u
    &= (x|y)
     \end{align*}
     On a alors montré l'équivalence.\qed
\end{proof}
% section isometries_vectorielles_d_un_espace_euclidien (end)
\chapter{Espaces Préhilbertiens complexes}
\section{Structure Préhilbertienne complexe}
On se place dans $\Cmplx$ et on établit de nouveau le \uline{produit scalaire} comme à la définition \ref{dfn:ProduitScal} page \pageref{dfn:ProduitScal} du chapitre précédent. À une différence prêt, la symétrie est appelée \textbf{symétrie hermitienne}
\begin{dfn}[Produit scalaire]\label{dfn:ProduitScalCmplx}
Le produit scalaire \index{Produit scalaire} est défini comme suit : 
\[
\begin{array}{c r l l}
(i)&\forall x \in E\backslash \{ 0 \} , &{\varphi(x,x)>0}&\text{\textbf{(définie positive)}}\\
(ii)&\forall (x,y) \in E^2, &{\varphi (x,y) = \overline {\varphi (y,x)}} &\text{\textbf{(symétrie hermitienne)}}\\
(iii)&\forall x \in E, &{y \mapsto \varphi(x,y) \text{ est linéaire}}&\text{\textbf{(linéaire à droite)}}
\end{array}
\]
On peut associer un produit scalaire $(\cdot | \cdot )$ à un espace vectoriel $E$. L'espace $\left( E, (\cdot | \cdot ) \right)
$ est appelé \emph{espace préhilbertien}\index{Espace!prehilbertien@préhilbertien}.
\end{dfn}
\section{Orthogonalité}
\begin{theorem}[Inégalité de \textsc{Bessel} \index{inegalite@Inégalité!Bessel@de \textsc{Bessel}}]
Si $(e_i)$ est une base orthonormée : 
$\boxed{\sum_i \textcolor{Red}{|}(e_i|x)\textcolor{Red}{|}^2 \le \textcolor{Red}{N_2}^2 (x)}$
\end{theorem}
\section{Séries de \textsc{Fourier}}
\begin{methode}
\titre{Coefficients}
\begin{description}
\item[Exponentiels] $c_n(f) = \dfrac{1}{2\pi} \int_{-\pi}^{\pi}e^{-int} f(t)\, \d t$
\item[Trigonométriques] \hfill \\[-10pt]
    \begin{itemize}\itemsep2pt
        \item $a_n(f) = \dfrac{1}{\pi} \int_{-\pi}^{\pi} \cos (nt) f(t)\, \d t$
        \item $b_n(f) = \dfrac{1}{\pi} \int_{-\pi}^{\pi} \sin (nt) f(t)\, \d t$
    \end{itemize}
\end{description}
\end{methode}
On note $\boldsymbol{ \mathcal{C}_{2\pi}}$ l'espace vectoriel des applications $f : \Reel \to \Cmplx$ \textbf{{$\boldsymbol{ 2\pi}$-périodiques}} et \textbf{continues} sur $[0,2\pi]$.\\
On note $\boldsymbol{ \mathcal{CM}_{2\pi}}$ l'espace vectoriel des applications $f : \Reel \to \Cmplx$ \textbf{{$\boldsymbol{ 2\pi}$-périodiques}} et \textbf{continues par morceaux} sur $[0,2\pi]$.\\[3pt]
\Attention{Sur $\mathcal{CM}_{2\pi}$, $N_1, N_2$ et $N_\infty$ ne sont plus des normes, mais elles fonctionnent toujours de la même manière}

Cette sous-section se base sur le théorème suivant : 
\begin{theorem}
Pour toute fonction dans $\mathcal{CM}_{2\pi}$, il existe une suite $\left( f_n \right)_n$ d'éléments de $\mathcal{C}_{2\pi}$ telle que $\boxed{N_2(f-f_n)\xrightarrow[n\to\infty]{} 0}$
\end{theorem}

L'objectif des séries de \textsc{Fourier} est de \og transposer \fg une fonction dans une base de $\mathcal{C}_{2\pi}$. \\

On prendra pour bases $(e^{it},e^{2it},\cdots e^{int})$ ou $\left( cos(t),cos(2t),\cdots ,cos(nt)\right)$ par exemple, et grâce à un produit scalaire, on obtient la décomposition de notre fonction dans la base.\\
C'est ainsi qu'on définit les coefficients : 


\begin{dfn}[Coefficients exponentiels]
Pour une fonction $f$ dans $\mathcal{CM}_{2\pi}$ :\\
\begin{center}
 \ibox{
$
c_n(f) = \color{Black!50}{\underbrace{\color{Black}{\dfrac{1}{2\pi}}}_{\mathclap{\text{on est sur }\mathcal{CM}_{2\pi}}}} \color{Black}{\int \limits_0^{2\pi}e^{-i\cdot n\cdot t} f(t)\, dt}
$}
 \end{center}

En effet : $c_n(f) = (e_n | f)$ avec $e_n = e^{int}$. Or le produit scalaire pour des fonctions est $(g|f)=\int_0^{2\pi} \overline{g(t)}\; f(t) \: dt$, d'où $(e_n|f)=\int_0^{2\pi} \overline{e^{int}}\; f(t) \: dt=\int_0^{2\pi} e^{\emph{-}int}\; f(t) \: dt$
\end{dfn}

On aurait très bien pu intégrer sur $[-\pi,\pi]$ au lieu de $[0,2\pi]$. C'est ce qu'on fera plus tard avec les coefficients trigonométriques.
\begin{prop}
\begin{itemize}
\item $g : t \mapsto f(-t),\: c_n(g) = c_{-n}(f)$
\item $f_a : t \mapsto f(t + a),\: c_n(f_a) = e^{ina}c_n(f)$
\end{itemize}
\end{prop}
\begin{theorem}[Dérivée de $f$]
$c_n(f') = in\, c_n(f)$ \\
d'où, par récurrence : $\boxed{c_n\left( f^{(k)}\right) = (in)^k c_n(f)}$
\end{theorem}
\begin{dfn}[Coefficients trigonométriques]
Pour une fonction $f$ dans $\mathcal{CM}_{2\pi}$ :\\
\begin{center}
 \fcolorbox{couleurNoirClair}{couleurClaire!20}{
$
a_n(f) = \dfrac{1}{\pi} \int \limits_{-\pi}^{\pi} \cos(nt) f(t)\, dt
$} et  \fcolorbox{couleurNoirClair}{couleurClaire!20}{
$
b_n(f) = \dfrac{1}{\pi} \int \limits_{-\pi}^{\pi} \sin(nt) f(t)\, dt
$}
 \end{center}
Ici, c'est $\dfrac{1}{\pi}$ en facteur, car $N_2^2 \left( \cos(n \bullet)\right) = \frac{1}{2}$

\end{dfn}
\begin{prop}
\begin{itemize}
\item $a_n(f) = c_n(f) + c_{-n}(f)$ et $b_n(f) = i(c_n(f) + c_{-n}(f))$
\item Si $f$ est paire, alors $b_n = 0 \: \forall n$
\item Si $f$ est impaire, alors $a_n = 0 \: \forall n$
\end{itemize}
\end{prop}
En général, on utilisera ces coefficients si $f$ présente une parité.
\begin{dfn}[Série de \textsc{Fourier}]
On appelle \emph{série de \textsc{Fourier}}\index{fourier@\emphi{Série de }\textsc{Fourier}} de $f$ la série $\sum u_n$ où $\left|
\begin{array}{l l l l}
u_0 &=& c_0(f) \; e_0&\\
u_n &=& c_n(f) \; e_n + c_{-n}(f) \; e_{-n}
\end{array}
\right.$\\
$S_n(f)$ est appelée \emph{somme partielle de rang $\boldsymbol n$ de la série de \textsc{Fourier}}
\end{dfn}
\begin{theorem}[Inégalité de Bessel\index{inegalite@Inégalité!Bessel@de \textsc{Bessel}}]
Si $f \in \mathcal{CM}_{2\pi}$, alors : \[\sum_{k=\textcolor{Red}{-n}}^{n} \left| c_n(f) \right| ^{\textcolor{Red}{2}}\le N_2^{\textcolor{Red}{2}}(f)\]
\end{theorem}
\begin{theorem}[Théorème de convergence \textsc{Parseval}]
Si $f$\index{Parseval@\textsc{Parseval}!convergence@\emphi{théorème de }convergence} est une fonction de \uline{$\mathcal{CM}_{2\pi}$}, alors $\boxed{N_2\left(\vphantom{{3^3}^3} f-S_n(f) \right)_n}$ converge vers 0
\end{theorem}
Avec ce nouveau théorème, on trouve le cas d'égalité de l'inégalité de Bessel :
\begin{itheorem}[Égalité de Parseval\index{Parseval@\string\textsc{Parseval}!egalite@égalité}]
Si $f$ est une fonction de \uline{$\mathcal{CM}_{2\pi}$} : 
\begin{center}
\fcolorbox{couleurNoirClair}{couleurGrisFonce!15}{
$\sum_{n\textcolor{Red}{\in \mathbb{Z}}} \left| c_n(f) \right| ^{\textcolor{Red}{2}}=N_2^{\textcolor{Red}{2}}(f)$ 
}
\end{center}
En réel, cette égalité devient : 
\[
N_2^2(f) = \left( \dfrac{a_0(f)}{2} \right) ^2 + \frac{1}{2}\sum_{n=1}^{+\infty} \left[ a_n^2(f) + b_n(f)^2 \right]
\]
\end{itheorem}
\begin{theorem}[Calcul des Coefficients]
    Si on a la suite $s_n$ telle que $\left| \begin{gathered} s_n = \sum_{k=-n}^n \alpha_k e_k \\ N_2(s_k - f) \xrightarrow[n\to +\infty ]{} 0 \end{gathered}\right. $, alors $\forall k\in \mathbb{Z}, c_n(f) = \alpha_n$
\end{theorem}
    \begin{proof}\hfill \\
        D'après l'inégalité de \textsc{Cauchy}-\textsc{Schwarz}, $|c_k(s_n - f)|\le N_2(s_n - f)$. Donc $|c_k(f) - \alpha_k|\le N_2(s_n - f) \xrightarrow[n\to +\infty]{} 0$, d'où, quand $n\to +\infty$, $c_n(f) = \alpha_n$.
    \end{proof}
\begin{itheorem}[Théorème de convergence normale]
Si $f$ est \underline{$\mathcal{C}_{2\pi}$ et de classe $\mathcal{C}^1$ par morceaux\footnote{$\mathcal{C}^1$ par morceaux c'est à dire que la dérivée est continue par morceaux, à ne pas confondre avec $f \in \mathcal{CM}$}}
\begin{tabbing}
alors \= sa série de \textsc{Fourier} \emph{converge normalement} et sa somme vaut $f$ \\
 \> sa somme partielle de sa série de \textsc{Fourier} $S_n$ \emph{converge uniformément}
\end{tabbing}
\end{itheorem}
\begin{dfn}[Noyau de \textsc{Dirichlet}]
On appelle \emph{noyau de \textsc{Dirichlet}}\index{dirichlet@\textsc{Dirichlet}!Noyau}, et on note $D_p(t)$ la somme : $D_p(t)=\sum_{k=-p}^p e^{ikt}$
\end{dfn}
\begin{itheorem}[Noyau de \textsc{Dirichlet}]
Si $f \in \mathcal{CM}_{2\pi}$ et $C^1$ par morceaux, alors sa série de \textsc{Fourier} \emph{converge simplement} sur \Reel{}. \\
\emph{Sa somme} au point $x$, notée $\overset{\sim}{f}(x)$ est égale à $\frac{1}{2}\lim_{h\to 0^+}[f(x+h) + f(x-h)]$. Si $f$ est \emph{continue}, alors $\overset{\sim}{f}(x) = f(x)$.
\end{itheorem}
% section Espaces préhilbertiens (end)
\part{Analyse}
\chapter{Séries} % (fold)
\label{cha:series}
\begin{methode}
     \titre{Étude d'une série}
Prenons le cas d'étude d'une série : $ \sum u_n$
\begin{enumerate}
    \item Vérifier que $\sum u_n$ est positive. \\ Si elle ne l'est pas, on peut prendre $N(\sum u_n)$. Dans \Reel{}, on prendra $\left| \sum u_n \right|$.
    \item Utiliser un théorème de comparaison pour ramener à des séries facilement étudiables. On peut : 
    \begin{itemize}
        \item Trouver un équivalent (en utilisant des \glspl{DL})
        \item Trouver une domination en $o(\cdots)$ ou en $O(\cdots)$
        \item Majorer/Minorer explicitement, mais c'est rare
    \end{itemize}
\end{enumerate}
\end{methode}
\section{Définitions}
\begin{dfn}
La série $S$ de terme général $\color{couleurFonce!90} u_n$ est la suite $\left( S_n \right)_{n\in\mathbb{N}}$ où on définit $S_n$ de manière suivante.
    \[
    \underbrace{S_n = \color{couleurNoirClair} \underbrace{\sum_{k=0}^{n} \color{couleurFonce!90}\underbrace{u_n}_{\text{Terme général de la série}}}_{\text{Somme partielle}}}_{\text{Suite des sommes partielles}}
    \]
\end{dfn}

\begin{dfn}[Convergence\index{Convergence!serie@série} d'une série]
    On dit que la série des $u_n$ converge s'il existe $l$ tel que $l = \lim_{n\to \infty} S_n$ existe.\\
    S'il la suite $(u_n)_n$ ne converge pas vers $0$, on dit que sa série $S_n$ \emph{diverge grossièrement}\index{Diverge!grossierement@grossièrement}
\end{dfn}
\begin{itheorem}[Théorème suite-série]
    La série $S_n = \sum_{k=0}^n (a_{n+1}-a_n)$ converge \emph{\uline{si et seulement si}} la suite $(a_n)$ \uline{converge}.
\end{itheorem}
\section{Séries à termes positifs} % (fold)
\label{sec:SATP}
\begin{dfn}[\acrlong{SATP}\index{SATP}]
    On appelle \emph{\acrfull{SATP}} toute série de terme général $u_n$ réel tel que, \uline{à partir d'un certain rang}, $u_n \ge 0$.
\end{dfn}
\begin{itheorem}[Convergence des \gls{SATP}]
    Si $\sum u_n$ est une \gls{SATP}, alors :
    \begin{equation}
        \sum u_n \text{ converge} \Leftrightarrow \sum_{k=0}^n u_n \text{ est majorée}
    \end{equation}
\end{itheorem}
% section SATP (end)
\section{Complément sur les séries numériques} % (fold)
\label{sec:complement_sur_les_series_numeriques}
\subsection{Règle de d'\textsc{Alembert}} % (fold)
\label{sub:regle_de_d_alembert}
\begin{lemme}
    Pour toute suite $(u_n)$ \uline{strictement positive}, s'il existe une suite $(\alpha_n)$ \uline{strictement positive} telle que $\dfrac{u_{n+1}}{u_n}\le \dfrac{\alpha_{n+1}}{\alpha_n}$, alors 
    \begin{equation}
    u_n \underset{+\infty}{=} O(\alpha_n)
    \end{equation}
\end{lemme}
\begin{itheorem}[Règle de d'\textsc{Alembert}]\label{th:Regle_de_dAlembert}
    Si, à partir d'un certain rang, $\left\lbrace
    \begin{aligned}
        u_n &> 0    \\
        \dfrac{u_{n+1}}{u_n} &\to l
    \end{aligned}
    \right. $ alors :
    \begin{equation}
    \left\lbrace
    \begin{aligned}
        \text{Si } l >1, & \sum u_n \text{ diverge grossièrement}   \\
        \text{Si } l <1, & \sum u_n \text{ converge} 
    \end{aligned}
    \right.
    \end{equation}
\end{itheorem}
Ce théorème est peu utile car il est \og{}trop vrai\fg{}.
% subsection regle_de_d'alembert (end)
\subsection{Séries alternées} % (fold)
\label{sub:series_alternees}
\begin{dfn}[Série alternée]
    La série $\sum u_n$ est une \emph{série alternée}\index{Serie@Série!alternee@alternée} s'il existe $(\alpha_n)$ une suite positive et $\varepsilon \in \lbrace -1 ; 1 \rbrace$ tels que $u_n = \varepsilon (-1)^n\alpha_n$
\end{dfn}
\begin{itheorem}[Théorème spécial des séries alternées\index{TSSA} (TSSA)]
    Si $\left\lbrace \begin{aligned} \sum u_n &\text{ est une série alternée}\\ (|u_n|) &\text{ est décroissante} \\ (|u_n|) &\xrightarrow[+\infty]{} 0 \end{aligned}\right.$, alors $\sum u_n$ \uline{converge} et $\forall n \ge 0, \textcolor{couleurImp}{\left| \vphantom{\sum_{k=n}^{+\infty}} \right. } \sum_{k=n}^{+\infty}u_k \textcolor{couleurImp}{\left. \vphantom{\sum_{k=n}^{+\infty}} \right| } \le \textcolor{couleurImp}{|} u_n\textcolor{couleurImp}{|}$
\end{itheorem}
\Attention{Ne pas oublier les valeurs absolues pour l'étude des séries alternées.}
% subsection series_alternees (end)
\subsection{Comparaison série intégrale} % (fold)
\label{sub:comparaison_serie_integrale}
\needspace{7cm}
\begin{itheorem}[Comparaison d'une série à une intégrale]
    Si $f$ est une fonction continue par morceaux, \emphh{positive} et \emphh{décroissante}  sur $\Reel^+$, alors la série de terme général $\left( \int_{n-1}^n f \right) - f(n) $ est une \gls{SATP} convergente.\\
    \begin{equation}
        f \in \mathcal{L}^1 \Leftrightarrow \sum f(n) \text{ converge}
    \end{equation}
    En cas de divergence, $\int f \underset{n \to +\infty}{\sim}\sum f(n)$\\[\baselineskip]

    Si $f$ est une fonction continue par morceaux, \emphh{positive}, \emphh{croissante} et \textcolor{couleurImp}{\uline{majorée}} sur $\Reel^+$, alors la série de terme général $\left( \int_n^{n+1} f \right) - f(n)$ est une \gls{SATP} convergente.
\end{itheorem}
% subsection comparaison_serie_integrale (end)
\subsection{Comparaison des \gls{SATP}} % (fold)
\label{sub:comparaison_des_satp}
\begin{itheorem}[Théorème de comparaison des \glspl{SATP}]
    Si on a deux suites $u_n$ et $v_n$ de $\Reel{}^\mathbb{N}$ telles qu'on ait une des conditions suivantes : $
    \left\lbrace
    \begin{aligned}
        u_n &\le v_n    \\
        u_n &= o(v_n)   \\
        u_n &= O(v_n)
    \end{aligned}
    \right. $, alors $\sum v_n$ converge $\implies \sum u_n$ converge. \par

    On a également la contraposée : 
    $\sum u_n$ diverge $\implies \sum v_n$ diverge
\end{itheorem}
%\textbf{Remarque : }Si $u_n \sim v_n$, alors $u_n$ et $v_n$ sont de même nature.
\begin{itheorem}[2\ieme{} théorème de comparaison des \glspl{SATP}]
     Soient $(a_n)_{n\in \mathbb{N}}$ et $(b_n)_{n\in \mathbb{N}}$ \emphhs{deux suites positives}. \newline
     Si $u_n \sim v_n$, alors $u_n$ et $v_n$ sont de même nature.
\end{itheorem}
\needspace{7cm}
\begin{theorem}[\textsc{Césaro}\index{Cesaro@\textsc{Césaro}}]
    Si $\sum \alpha_n$ est une \gls{SATP} \uline{divergente}, et que $(\beta_k)$ est une suite \uline{complexe convergente} vers $\beta$, alors la suite $(S_n)$ de terme général : 
    \[
        S_n = \dfrac{\displaystyle \sum_{k=0}^n \alpha_k \beta_k}{\displaystyle \sum_{k=0}^n \alpha_k}
    \]
    (définie à partir d'un certain rang) converge vers $\beta$.\par
    La moyenne de \textsc{Césaro} apparait en prenant la suite $\alpha_k = 1$ $\forall k$, car alors $\sum \alpha_k = n$ et : 
    \[
        S_n = \dfrac{\displaystyle \sum_{k=0}^n \beta_k}{n}
    \]
    C'est la moyenne des termes de la suite $\beta_k$, et on sait qu'elle converge si $\beta_k$ converge.
\end{theorem}
\begin{proof}
     L'idée est de séparer la somme en deux. Prenons $\varepsilon > 0$. Soit $l$ la limite de la suite $\beta_k$.\newline 
     Soit $N$ tel que $\forall n\ge N,\, | \beta_n - l | < \varepsilon$. Alors :
     \begin{align*}
          S_n - l &= \dfrac{\displaystyle \sum_{k=0}^N \alpha_k (\beta_k - l)}{\displaystyle \sum_{k=0}^n \alpha_k} + \dfrac{\displaystyle \sum_{k=N}^n \alpha_k (\beta_k - l)}{\displaystyle \sum_{k=0}^n \alpha_k}
          \intertext{La somme allant jusqu'à $N$ ne dépendant plus de $n$ :}
          S_n - l &\le \dfrac{\text{constante}}{\displaystyle \sum_{k=0}^n \alpha_k} + \dfrac{\displaystyle \sum_{k=N}^n \alpha_k}{\displaystyle \sum_{k=0}^n \alpha_k}\cdot \varepsilon \\
          S_n - l &\le \dfrac{\text{constante}}{\pm\infty } + 1\cdot \varepsilon
     \end{align*}
     On a majoré par quelque chose d'aussi petit qu'on veut, et alors $S_n$ converge \qed
\end{proof}
% subsection comparaison_des_satp (end)
% section complement_sur_les_series_numeriques (end)
\section{Hors programme} % (fold)
\label{sec:hors_programme}
Tous les théorèmes vus ici sont à démontrer.
\begin{itheorem}[Transformation d'\textsc{Abel}]
    Soient deux suites $(a_n)$ et $(b_n)$ de $\mathbb{K}^\mathbb{N}$. On note $B_n = \sum_{k=0}^n b_k$. Alors :
    \begin{equation}
    \sum_{k=0}^n a_k b_k = a_n B_n + \sum_{i=0}^{n\textcolor{couleurImp}{-1}} (a_i - a_{i+1}) B_i
    \end{equation}
    On peut en déduire, si on a les conditions $\left\lbrace \begin{array}{l}
        \sum (a_i - a_{i+1}) \text{ CVA vers }0 \\
        B_n = \sum_{k=0}^n b_k \text{ est bornée}
    \end{array}\right.$, que $\sum a_k b_k$ converge.
\end{itheorem}
\begin{proof}
    On remarque que $b_i = B_{i} - B_{i-1}$, avec $b_0 = B_0$. Il vient : 
    \begin{align*}
        \sum_{k=0}^n a_k b_k    &= a_0 b_0 + \sum_{k=1}^n a_k \left( B_k - B_{k-1} \right) \\
                                &= a_0 b_0 + \sum_{k=1}^n a_k B_k - \sum_{k=1}^n a_k B_{k-1} \\
                                \intertext{On change d'indice sur la deuxième somme, et comme $b_0 = B_0$ : }
                                &=  a_0 B_0 + \sum_{k=1}^n a_k B_k - \sum_{k=0}^{n-1} a_{k+1} B_k \\
                                &= \sum_{k=0}^{n-1} a_k B_k + a_n B_n - \sum_{k=0}^{n-1} a_{k+1} B_k \\
                                &= a_n B_n + \sum_{k=0}^{n-1} (a_{k} - a_{k+1}) B_k
    \end{align*}
\end{proof}
% section hors_programme (end)
% chapter series (end)
\chapter{Familles sommables de nombres complexes} % (fold)
\label{cha:familles_sommables_de_nombres_complexes}
\section{Dénombrement} % (fold)
\label{sec:denombrement}
\begin{dfn}[Ensemble fini]
     On dit que $E$ est un \emph{ensemble fini}\index{Ensemble!fini} de cardinal $n$ si $E$ est en bijection avec $[\![ 0 , n [\![$
\end{dfn}
\begin{dfn}[Equipotence]
     Deux ensembles $E$ et $F$ sont dits \emph{équipotents}\index{Ensemble!equipotent@équipotent} (ou en bijection) s'il existe une application $\varphi : E \to F$ telle que $\varphi$ soit bijective.
\end{dfn}
\begin{dfn}[Ensemble dénombrable]
     On dit que $E$ est un \emph{ensemble dénombrable}\index{Ensemble!denombrable@dénombrable} s'il est équipotent à $\mathbb{N}$
\end{dfn}
\begin{theorem}
     Toute partie \emphh{infinie} de $\mathbb{N}$ est dénombrable.
\end{theorem}
\begin{theorem}
     Un ensemble est fini ou dénombrable \emphh{si et seulement si} il est équipotent à une partie de $\mathbb{N}$
\end{theorem}
\begin{itheorem}
     L'ensemble $\mathbb{N}^2$ est dénombrable.
\end{itheorem}
\begin{proof}
     On utilise la \emphh{fonction de couplage de \textsc{Cantor}}\index{Cantor@\textsc{Cantor}} : 
     \begin{equation}
     f(p,q) = q + \sum_{i=0}^{p+q} i
     \end{equation}
     On montre que cette fonction est bijective.
     %TODO Fonction de Cantor
\end{proof}
\todo{ Fonction de Cantor}
\begin{theorem}
     Une réunion finie ou dénombrable d'ensembles dénombrables est dénombrable.\\[5mm]
     Ainsi, $\mathbb{Q}$ est dénombrable : \[\mathbb{Q} = \bigcup_{q\in\mathbb{N}^*}\left\lbrace \vphantom{\mathbb{N}} \frac{p}{q}, p \in \mathbb{Z} \right\rbrace\]
\end{theorem}
\begin{theorem}
     L'ensemble \Reel{} n'est pas dénombrable.
\end{theorem}
% section denombrement (end)
\section{Familles sommables} % (fold)
\label{sec:familles_sommables}
\subsection{Pour les réels positifs} % (fold)
\label{sub:pour_les_reels_positifs}
\begin{dfn}[Famille sommable de réels positifs]
     Soit une famille $(u_i)_{i\in I}$ de nombres réels positifs. Une famille est \emph{sommable}\index{Famille!sommable} s'il existe un réel $M$ tel que, pour toute partie finie $J \subset I $, on ait :
     \begin{subequations}
     \begin{equation}
     \sum_{i \in J} u_i \le M
     \end{equation}
     On définit la somme de cette famille par
     \begin{equation}
     \sum_{i\in I} u_i = \sup_J \sum_{n\in J} u_n
     \end{equation}
     \end{subequations}
\end{dfn}
\begin{theorem}[Sommation par paquets]
     Soit une famille $(u_i)_{i\in I}$ de nombres réels positifs.\newline
     Soit $(I_n)_{n\in\mathbb{N}}$ une partition \emphhs{dénombrable} de $I$.\par
     La famille $(u_i)_{i\in I}$ est sommable \emphh{si et seulement si} :
     \begin{itemize}
         \item la sous-famille $(u_i)_{i\in I_n}$ est sommable pour tout $n$
         \item la série $\sum_n \left( \sum_{i\in I_n} u_i \right)$ est convergente
     \end{itemize}
     Dans ce cas : 
     \begin{equation}
     \sum_{n=0}^{+\infty} \left( \sum_{i\in I_n} u_i \right) = \sum_{n\in I} u_n
     \end{equation}
\end{theorem}
\begin{theorem}[Sommation triangulaire]
     Soit $(u_{p,q})_{(p,q)\in \mathbb{N}^2}$ une famille de réels positifs.\newline
     \begin{equation}
          \begin{array}{r c l}
               (u_{p,q})_{(p,q)\in \mathbb{N}^2} \text{ est sommable} & \Leftrightarrow & \displaystyle\sum_n \left( \displaystyle\sum_{p+q = n} u_{p,q} \right)\text{ converge} \\
               \displaystyle\sum_{(p,q)\in \mathbb{N}^2} u_{p,q} & = & \displaystyle\sum_n \left( \displaystyle\sum_{p+q = n} u_{p,q} \right) \\
          \end{array}
     \end{equation}
\end{theorem}
% subsection pour_les_reels_positifs (end)
\subsection{Pour les réels et les complexes} % (fold)
\label{sub:famille_sommable_complexe_reel}
\begin{dfn}[Famille sommable]
     Une famille $(u_i)_{i\in I}$ de $\mathbb{K}^\mathbb{N}$ est \emph{sommable}\index{Famille!sommable} si la famille de réels positifs $(|u_i|)_{i\in I}$ est sommable.
\end{dfn}
\begin{theorem}
     $(u_i)_{i\in I}$ est \emphh{sommable} $\Leftrightarrow \sum_{i\in I} u_i$ \emphh{ absolument covergente}
\end{theorem}
\begin{itheorem}[Sommation par paquets]
     Soit une famille $(u_i)_{i\in I}$ de nombres réels positifs.\newline
     Soit $(I_n)_{n\in\mathbb{N}}$ une partition \emphhs{dénombrable} de $I$.\par
     La famille $(u_i)_{i\in I}$ est sommable \emphh{si et seulement si} :
     \begin{itemize}
         \item la sous-famille $(u_i)_{i\in I_n}$ est sommable pour tout $n$
         \item la série $\sum_n \left( \sum_{i\in I_n} \textcolor{couleurImp}{|}u_i \textcolor{couleurImp}{|} \right)$ est convergente
     \end{itemize}
     Dans ce cas : 
     \begin{equation}
     \sum_{n=0}^{+\infty} \left( \sum_{i\in I_n} u_i \right) = \sum_{n\in I} u_n
     \end{equation}
\end{itheorem}
\begin{theorem}
     Soient $(a_p)_{p\in \mathbb{N}}$ et $(b_q)_{q\in \mathbb{N}}$ deux familles sommables. Alors la famille $\big( a_p b_q\big)_{(p,q)\in \mathbb{N}^2}$ sommable et 
     \begin{equation}
     \sum_{(p,q)\in \mathbb{N}^2} a_p b_q= \left( \sum_{p\in \mathbb{N}} a_p \right) \left( \sum_{q\in \mathbb{N}} b_q \right)
     \end{equation}
\end{theorem}
\begin{proof}
     Ce théorème est issu du théorème de \textsc{Fubini} (\textit{cf.} théorème \ref{th:fubini} page \pageref{th:fubini}) : les deux suites $(a_p)_{p\in \mathbb{N}}$ et $(b_q)_{q\in \mathbb{N}}$ sont sommables, donc leurs séries convergent absolument, et les hypothèse du théorème de \textsc{Fubini} sont alors vérifiées.
\end{proof}
\begin{itheorem}[Sommation triangulaire]
     Soit $(u_{p,q})_{(p,q)\in \mathbb{N}^2}$ une famille de réels positifs.\newline
     \begin{equation}
          \begin{array}{r c l}
               (u_{p,q})_{(p,q)\in \mathbb{N}^2} \text{ est sommable} & \Leftrightarrow & \displaystyle\sum_n \left( \displaystyle\sum_{p+q = n} |u_{p,q}| \right)\text{ converge} \\
               \displaystyle\sum_{(p,q)\in \mathbb{N}^2} u_{p,q} & = & \displaystyle\sum_n \left( \displaystyle\sum_{p+q = n} u_{p,q} \right) \\
          \end{array}
     \end{equation}
\end{itheorem}
\Attention{Faire attention à bien mettre des modules partout}
% subsection famille_sommable_complexe_reel (end)
% section familles_sommables (end)
% chapter familles_sommables_de_nombres_complexes (end)
\chapter{Variables aléatoires discrètes} % (fold)
\label{cha:variables_aleatoires_discretes}
\section{Espace probabilisé} % (fold)
\label{sec:espace_probabilise}
\begin{dfn}[Univers]
     Un \emph{univers}\index{univers} $\Omega$ est l'ensemble des issues d'une expérience aléatoire.\\
     L'ensemble des parties de l'univers $\Omega$ est noté $\mathcal{P}(\Omega{})$
\end{dfn}
\begin{dfn}[Tribu, espace probabilisable, évènement]
     Une \emph{tribu}\index{Tribu} sur un univers $\Omega$ est un sous-ensemble $\mathscr{T}$ de $\mathcal{P}(\Omega{})$ vérifiant les propriétés :
     \begin{itemize}
         \item $\Omega \in \mathscr{T}$
         \item $\forall A \in \mathscr{T},\: \overline{A} \in \mathscr{T}$
         \item $\forall \left( A_n \right)_{n\in \mathbb{N}} \in \mathscr{T}^\mathbb{N},\: \bigcup_{n\in \mathbb{N}} A_n \in \mathscr{T}$
     \end{itemize}
     Le couple $(\Omega, \mathscr{T})$ est appelé \emph{espace probabilisable}\index{Espace!probabilisable}.\\
     Un élément de la tribu $\mathscr{T}$ est appelé un \emph{évènement}\index{Evenement@Évènement}
\end{dfn}
\begin{prop}
     Si $(\Omega, \mathscr{T})$ est un espace probabilisable, et $A_i$ un élément de $\mathscr{T}$ :
     \begin{itemize}
         \item $\varnothing \in \mathscr{T}$
         \item $\bigcup_\text{finie} A_i \in \mathscr{T}$
         \item $\bigcap_\text{finie} A_i \in \mathscr{T}$
         \item $\forall (A_n)_{n\in \mathbb{N}},\, \bigcap_{n\in \mathbb{N}} A_i \in \mathscr{T}$
         \item $\forall (A,B) \in \mathscr{T}^2,\, A\cap \overline{B} = A \backslash B$
     \end{itemize}
\end{prop}
\begin{dfn}[Incompatibilité, implication]
     Deux évènements $A$ et $B$ sont dits \emph{incompatibles}\index{Evenements@Évènements!incompatibles} si $A \cap B = \varnothing$\\
     On dit que $A \underset{\text{implique}}{\implies} B$ quand $A \subset B$
\end{dfn}
\Attention{Ne pas confondre ($A \cap B = \varnothing$) et ($P(A\cap B) = 0$)}
\begin{dfn}[Système complet d'évènements]
     Soit $(\Omega, \mathscr{T})$ un espace probabilisable.\\
     Soit $I$ fini ou égal à $\mathbb{N}$\newline
     Un \emph{système complet d'évènements}\index{Systeme complet@Système complet\emphi{ (probabilités)}} est \emphhs{une suite d'évènements} $(A_i)_{i\in I}$ telle que :
     \begin{itemize}
         \item $\bigcup_{i\in I} A_i = \Omega$
         \item $\forall (p,q) \in I^2,\, p\neq q,\, A_p \cap A_q = \varnothing$
     \end{itemize}
\end{dfn}
\begin{dfn}[Probabilité]\label{dfn:probabilite}
     Soit $(\Omega, \mathscr{T})$ un espace probabilisable. La \emph{probabilité}\index{Probabilite@Probabilité} est l'application $\mathbb{P} : \mathscr{T} \to [0,1]$ telle que :
     \begin{itemize}
         \item $\mathbb{P}(\Omega) = 1$
         \item Pour toute suite $(A_n)_{n\in\mathbb{N}} \in \mathscr{T}^\mathbb{N}$ d'évènements \emphhs{deux à deux incompatibles} : 
         \begin{itemize}
             \item $\sum \mathbb{P}(A_n)$ converge
             \item $\mathbb{P}\left( \bigcup_{n=0}^{+\infty} A_n \right) = \sum_{n=0}^{+\infty} \mathbb{P}(A_n)$\\
             ($\sigma$-additivité)
         \end{itemize}
     \end{itemize}
     Le triplet $(\Omega, \mathscr{T},\mathbb{P})$ est appelé \emph{espace probabilisé}\index{Espace!probabilise@probabilisé}
\end{dfn}
\Attention{Espace propabilisable $(\Omega, \mathscr{T}) \neq$ Espace probablilisé $(\Omega, \mathscr{T},\mathbb{P})$}\\[\baselineskip]
\begin{theorem}[Caractérisation par une famille sommable]
     Soit $\Omega$ un univers fini ou dénombrable. \newline
     Soit une application $\left\lbrace \begin{array}{r c l} \Omega &\longto& \mathbb{R}^+ \\ \omega &\longmapsto& p_\omega \end{array}\right.$ telle que $(p_\omega)_{\omega\in\Omega}$ soit une \emphhs{famille sommable} de somme $1$.\newline
     Soit $\left\lbrace \begin{array}{r c l} \mathbb{P} : \mathcal{P}(\Omega)&\longto& \mathbb{R}^+ \\ A &\longmapsto& \sum_{\omega \in A} p_\omega \end{array}\right.$\\
     Alors $\big( \Omega, \mathcal{P}(\Omega ), \mathbb{P}\big)$ est un espace probabilisé.
\end{theorem}
% section espace_probabilise (end)
\section{Propriétés élémentaires des probabilités} % (fold)
\label{sec:proprietes_elementaires_des_probabilites}
\begin{theorem}[Théorème de la limité monotone]
     Soit $\big( \Omega , \mathscr{T}, \mathbb{P}\big)$ un espace probabilisé.\\
     \begin{subequations}
     \begin{itemize}[leftmargin=5.5mm]
         \item Si $A_n$ est une suite croissante d'évènements ($A_n\subset A_{n+1}$), alors :
         \begin{equation}
         \mathbb{P}(A_n) \xrightarrow[n\to +\infty]{} \mathbb{P}\big( \bigcup_{n=0}^{+\infty} A_n\big) 
         \end{equation}
         \item Si $A_n$ est une suite décroissante d'évènements ($A_{n+1}\subset A_{n}$), alors :
         \begin{equation}
         \mathbb{P}(A_n) \xrightarrow[n\to +\infty]{} \mathbb{P}\big( \bigcap_{n=0}^{+\infty} A_n\big) 
         \end{equation}
     \end{itemize}
     \end{subequations}
\end{theorem}
Dans la définition~\ref{dfn:probabilite} de la probabilité, la propriété de $\sigma$-additivité nécessite que les évènements soient incompatibles. Cette propriété existe sous forme d'inégalité quand les évènements ne sont pas incompatibles :
\begin{itheorem}[Inégalité de \textsc{Boole}]
\index{Inegalite@Inégalité!Boole@\emphi{de} Boole}
Soit $(\Omega, \mathscr{T},\mathbb{P})$ un espace probablilisé. \\
Soit $(A_n)_{n\in\mathbb{N}} \in \mathscr{T}^\mathbb{N}$ telle que $\sum \mathbb{P}(A_n)$ converge. Alors :
     \begin{equation}
     \mathbb{P}\left( \bigcup_{n=0}^{+\infty} A_n \right) \le \sum_{n=0}^{+\infty} \mathbb{P}(A_n)
     \end{equation}
\end{itheorem}
\begin{proof}
     On se ramène à une suite d'évènements deux à deux disjoints en introduisant la suite $(C_n)$ telle que :
     \begin{equation*}
          C_0 = A_0 \\ C_n = A_n \backslash \bigcup_{k=0}^{n-1}A_k
     \end{equation*}
     Puisque $C_n \subset A_n$, on a : 
     \begin{equation*}
          \mathbb{P} \left( \bigcup_{n=0}^{+\infty} A_n \right) = \sum_{n=0}^{+\infty} \mathbb{P}(C_n) \le \sum_{n=0}^{+\infty} \mathbb{P}(A_n)
     \end{equation*}
\end{proof}
%TODO Évènements négligeables/presque sûrs
\todo{Évènements négligeables / presque sûrs}
% section proprietes_elementaires_des_probabilites (end)
\section{Indépendance} % (fold)
\label{sec:independance}
\begin{dfn}[Évènements indépendants]
     Deux évènements $A$ et $B$ sont dits \emph{indépendants}\index{Evenements@Évènements!independants@indépendants} lorsque 
     \begin{equation}
     \mathbb{P}(A\cap B) = \mathbb{P}(A)\times \mathbb{P}(B)
     \end{equation}
\end{dfn}
% section independance (end)
\section{Espérance} % (fold)
\label{sec:esperance}
\subsection{Définitions} % (fold)
\label{sub:esperance_definitions}
\begin{dfn}[Espérance d'une famille finie]
     Soit $X$ une variable aléatoire prenant un nombre fini de valeurs $x_1, x_2, \cdots , x_n$. L'\emph{espérance}\index{Esperance@Espérance} de $X$ est donnée par la somme finie :
     \begin{equation}
     E(X) = \sum_{i=1}^n x_i \mathbb{P}_X(x_i)
     \end{equation}
\end{dfn}
On peut étendre la définition précédente au cas où les valeurs de $X$ forment une famille sommable :
\begin{dfn}[Espérance d'une variable aléatoire discrète]
     Soit $X$ une variable aléatoire \emphh{réelle discrète}. On dit que $X$ est d'\emph{espérance} finie si la famille $\left( x\mathbb{P}_X(x)\right)_{x\in X(\Omega)}$ est sommable.
\end{dfn}
% subsection esperance_definitions (end)
\subsection{Propriétés} % (fold)
\label{sub:esperance_proprietes}
\begin{prop}
     \begin{itemize}
         \item L'ensemble $\mathscr{E}$ des variables aléatoire de $\Omega$ dans $\mathbb{R}$ et dont l'espérance est finie est un espace vectoriel
         \item L'espérance est une forme linéaire sur $\mathscr{E}$
         \item $\forall X \in \mathscr{E},\, X(\Omega) \subset \mathbb{R}^+ \implies E(X) \ge 0$ (l'espérance est positive)
         \item Soit $Y$ une variable aléatoire d'espérance finie, si $X$ est une variable aléatoire telle que $|X| \le Y$, alors $X$ est d'espérance finie.
         \item Si $X$ et $Y$ sont deux lois \emphh{indépendantes} \emphh{et admettant chacune une espérance finie}, alors $XY$ est d'espérance finie et 
         \begin{equation*}
         E(XY) = E(X)\: E(Y)
         \end{equation*}
     \end{itemize}
\end{prop}
% subsection esperance_proprietes (end)
% section esperance (end)
\section{Variance} % (fold)
\label{sec:variance}
\subsection{Moment} % (fold)
\label{sub:variance_moment}
\begin{dfn}[Moment d'ordre $n$]
     Soit $X$ une variale aléatoire discrète. On dit que $X$ admet un \emph{moment}\index{Moment@Moment\emphi{ (Probabilités)}} d'ordre $m$ si $X^m$ admet \emphh{une espérance finie}.
\end{dfn}
\begin{theorem}
     Si $X$ est une variable aléatoire admettant un moment d'ordre $2$, alors $X$ admet une espérance finie.
\end{theorem}
\begin{theorem}[Inégalité de \textsc{Cauchy-Schwarz}\index{inegalite@Inégalité!cauchy@de \textsc{Cauchy-Schwarz}}]
     Si $X$ et $Y$ sont deux variables aléatoires discrètes de moment d'ordre $2$, alors $XY$ a une espérance finie et
     \begin{equation}
     \bigg( E(XY)\bigg)^2 \le E(X^2)\,E(Y^2)
     \end{equation}
\end{theorem}
% subsection variance_moment (end)
\subsection{Variance et écart-type} % (fold)
\label{sub:variance_et_ecart_type}
\begin{dfn}[Variance et Écart-type]
     Soit $X$ une variable aléatoire discrète \emphh{admettant un moment d'ordre 2}.\par
     On définit la \emph{variance}\index{Variance} par :
     \begin{subequations}
     \begin{equation}
          V(X) = E\bigg( \big( X - E(X) \big)^2 \bigg)
     \end{equation}
     et l'\emph{écart-type}\index{ecart-type@Écart-type} par :
     \begin{equation}
     \sigma (X) = \sqrt{ V(X) }
     \end{equation}
     \end{subequations}
\end{dfn}
\begin{theorem}[Formule de \textsc{König-Huygens}]
     Soit $X$ une variable aléatoire discrète admettant un moment d'ordre 2. Alors :
     \begin{equation}
     V(X) = E(X^2) - E(X)^2
     \end{equation}
\end{theorem}
% subsection variance_et_ecart_type (end)
\subsection{Covariance} % (fold)
\label{sub:covariance}
\begin{dfn}[Covariance]
     Soient $X$ et $Y$ deux variables aléatoires discrètes \emphh{admettant un espérance finie}.\par
     Si elle existe, on définit la \emph{covariance}\index{Covariance} de $X$ et de $Y$ par :
     \begin{equation}
     \mathrm{cov}(X,Y) = E\bigg( \big( X - E(X)\big) \: \big( Y - E(Y)\big) \bigg)
     \end{equation}
\end{dfn}
\begin{itheorem}[Existence de la covariance]
     Soient $X$ et $Y$ deux variables aléatoires \emphh{admettant un moment d'ordre 2}. Alors la covariance de $X$ et de $Y$ existe et on a :
     \begin{equation}
     \mathrm{cov}(X,Y)=E(XY) - E(X)E(Y)
     \end{equation}
\end{itheorem}
\todo{ Existence}
\begin{proof}
     %TODO Existence
     On a : 
     \begin{align*}
         \mathrm{cov}(X,Y) &= E\bigg( \big( X - E(X)\big) \: \big( Y - E(Y)\big) \bigg)
         \intertext{On développe :}
                           &= E\bigg( XY - E(X)Y - E(Y)X + E(X)E(Y) \bigg)
                           \intertext{Par linéarité de l'espérance : }
                           &= E\big( XY \big) - E\big( E(X)Y \big) - E\big( E(Y)X \big) + E\big( E(X)E(Y) \big) \\
     %TODO Finir la demo
                           &= \; ??? \\
         \mathrm{cov}(X,Y) &= E(XY) - E(X)E(Y)
     \end{align*}
\end{proof}
\todo{ Finir la demo}
% subsection covariance (end)
% section variance (end)
\section{Lois usuelles} % (fold)
\label{sec:lois_usuelles}
\begin{dfn}[Loi de \textsc{Bernoulli}]
     La \emph{loi de \textsc{Bernoulli}}\index{Bernoulli@\textsc{Bernoulli}} est une distribution discrète qui prend la valeur $1$ avec une probabilité $p$ et la valeur $0$ avec la probabilité $(1-p)$. \\
     Une variable alétoire qui suit cette loi est appelée variable de \textsc{Bernoulli}
\end{dfn}
\subsection{Loi binomiale} % (fold)
\label{sub:loi_binomiale}
\begin{dfn}
     La loi binomiale, de paramètres $n$ et $p$, est la loi de probabilité d'une variable aléatoire $X$ égale au nombre de succès rencontrés au cours d'une répétition de $n$ épreuves de Bernoulli, $p$ étant la probabilité de succès d'une épreuve de Bernoulli.\par
     Autrement, c'est une variable aléatoire $X$ telle que 
     \[
         X = Y_1 + Y_1 + \cdots + Y_n
     \]
     où les $Y_i$ sont des variables aléatoires \emphhs{indépendantes} de loi de \textsc{Bernoulli}.\par
     La variable aléatoire suit une loi de probabilité : 
     \begin{equation}
     \mathbb{P}(X = k) = \begin{pmatrix} n \\ k \end{pmatrix} p^k(1-p)^{n-k}
     \end{equation}
\end{dfn}
\begin{theorem}[Espérance et variance]
     Soit $X$ une variable aléatoire qui suit une loi binômiale. Alors :
     \begin{equation}
     E(X) = np \\ V(X) = np(1-p)
     \end{equation}
\end{theorem}
\section{Fonctions génératrices} % (fold)
\label{sec:fonctions_generatrices}
%TODO: rajouter le theoreme du transfert! 
\begin{dfn}[Fonction génératrice] % Grosse copie depuis le livre de Vuibert p640
     Soit $X$ une variable aléatoire à valeurs dans $\mathbb{N}$. On définit la \emph{fonction génératrice}\index{Fonction génératrice} $G_X$ de $X$ par : 
     \begin{equation}
     G_X (t) = E(t^X) = \sum_k \mathbb{P} (X=k) t^k
     \end{equation}
     pour les valeurs de $t$ telles que la variable aléatoire $t^X$ admette une espérance finie.
\end{dfn}
% section fonctions_generatrices (end)
% subsection loi_binomiale (end)
% section lois_usuelles (end)
% chapter variables_aleatoires_discretes (end)
\chapter{Suites de fonctions}
\begin{methode}
    En général, pour la \uline{convergence simple}, on \emph{fixe $x$}. Pour la \uline{convergence uniforme}, puisqu'on cherche la norme $N_\infty$, on \emph{dérive $f_n(x)$} pour étudier ses variations.
\end{methode}
\section{Convergence de suites de fonctions}
\begin{dfn}
Définitions simplifiées des différents types de convergence
\begin{equation}\label{CV fonctions}
\begin{array}{r c l}
\text{La suite des }\left(f_n\right) \text{ converge \emph{simplement} vers } f&\Leftrightarrow &\forall x, \left(f_n(x)\right)_n \to f(x)\\
\text{La suite des }\left(f_n\right) \text{ converge \emph{uniformément} vers } f&\Leftrightarrow &N^A_\infty\left(\underbrace{f_n-f}_{\in B(A,F)}\right)\xrightarrow[\textcolor{RoyalBlue}{n}\to +\infty]{}0\\
\left(f_n(x)\right)_n\text{{\footnotesize  vérifie le \emph{critère de Cauchy de CU}}}&\Leftrightarrow&\exists \varepsilon>0 | n\ge n_0(\varepsilon) \Rightarrow N^A_\infty\left((f_n-f)\right)< \varepsilon\\
\end{array}
\end{equation}
Le critère de Cauchy de convergence uniforme est équivalent à la convergence uniforme. $\boxed{\text{Critère de Cauchy}\Leftrightarrow\text{Convergence Uniforme}}$
\end{dfn}
Pour illustrer, on peut faire les shémas suivants : 
\begin{figure}[h t]

\subfloat[Convergence Simple]{
	\begin{tikzpicture}
		
\draw [thin,gray,->] (-1,0) -- (3,0) node[right]{$x$}; 
\draw [thin,gray,->] (0,-1) -- (0,3) node[above]{$y$};
\draw [thick] (-1,-0.5) .. controls (1,-0.3) and (2,0) .. (3,3) node[near end, left]{$f(x)$};
\draw [thin,red!20] (-1,-0.8) .. controls (1,-0.7) and (2,-0.5) .. (3,1.7);
\draw [thin,red!40] (-1,-0.7) .. controls (1,-0.5) and (2,-0.2) .. (3,2);
\draw [thin,red!90] (-1,-0.6) .. controls (1,-0.4) and (2,-0.1) .. (3,2.5) node[near end, right]{$f_n(x)$};
\draw [thick,color=RoyalBlue, ->,>=stealth] (3.1,1.5) .. controls (3.3,2) and (3.3,2.6) .. (3.1,2.9) node[midway, right]{$n$ croissant};
	\end{tikzpicture}
}
\subfloat[Convergence Uniforme]{
	\begin{tikzpicture}
		
\draw [thin,gray,->] (-1,0) -- (3,0) node[right]{$x$}; 
\draw [thin,gray,->] (0,-1) -- (0,3) node[above]{$y$};
\draw [thick] (-1,-0.5) .. controls (1,-0.3) and (2,0) .. (3,3) node[near end, left]{$f(x)$};
\draw [thin,red!20] (-1,-0.8) .. controls (1,-0.7) and (2,-0.5) .. (3,1.7);
\draw [thin,red!40] (-1,-0.7) .. controls (1,-0.5) and (2,-0.2) .. (3,2);
\draw [thin,red!90] (-1,-0.6) .. controls (1,-0.4) and (2,-0.1) .. (3,2.5) node[near end, right]{$f_n(x)$};
\draw [thick,color=RoyalBlue, ->,>=stealth] (3.1,1.5) .. controls (3.3,2) and (3.3,2.6) .. (3.1,2.9) node[midway, right]{$n$ croissant};
\draw [very thick,<->] (1,-0.6) -- (1,0) node[midway, right]{$N_\infty$};
	\end{tikzpicture}
}
\caption{Les différents types de convergence de fonction}
\end{figure}
\\
\begin{prop}[de la simple convergence]
\begin{list}{-}{ }
\item $\left\lbrace
\begin{array}{l c l}
f:A\subset\mathbb{R}\to\mathbb{R}& & \\
\left(f_n(x)\right)_n\text{ est croissante}&\Leftrightarrow&f\text{ est croissante}\\
\left(f_n(x)\right)_n\ \xrightarrow[x\in{}A]{CVS}f& & \\
\end{array}\right.$
\item (autres propriétés analogues de $f_n$ appliquées à $f$ par CVS)
\end{list}
\end{prop}
%
\needspace{3cm}
\begin{theorem}[Convergence par changement de base]
Si $\left(f_n(x)\right)_n$ converge simplement ou uniformément \uline{\textbf{ssi}} $\left(f_{n,i}(x)\right)_n$ converge de la même manière dans la base $\mathcal{B}=(e_i)$
\end{theorem}
\begin{theorem}[Conditions nécessaire de CU\footnote{\textbf{CU} pour \textbf{C}onvergence \textbf{U}niforme}]
\[
\begin{array}{r c}

\left.
\begin{array}{r}
\left(f_n(x)\right)_{n\in\mathbb{N}}\xrightarrow{\emph{CU}}f\\
\left(f_n(x)\right)_n\ \text{ est \emph{bornée}}\\
\end{array}\right\rbrace & \implies
\end{array}
f \text{ est \textbf{uniformément convergente} \textbf{bornée}}
\]
\end{theorem}
\begin{theorem}[Conditions nécessaire de \uline{Non}-CU]
Il suffit que : 
$\exists (x_n)\text{ tel que }f(x_n)\underset{x\to{}\infty}{\nrightarrow}0$
\end{theorem}
On notera les fonctions $f$ dont la dérivée est continue de $A\to B$ comme appartenant à l'ensemble $\mathcal{C}(A,B)$

\begin{theorem}[Continuité par convergence]

\begin{align*}
\left.
\begin{array}{r}
\left(f_n(x)\right)_{n}\text{ \emph{continue} en a}\\
\left(f_n(x)\right)_n\text{ \emph{converge uniformément} vers }f\\
\end{array}\right\rbrace &\implies f\text{ est continue en }a\\
\left.
\begin{array}{r r}
&\left(f_n(x)\right)_{n}\in\mathcal{C}(A,F)\\
\left(f_n(x)\right)_n&\text{ \emph{converge uniformément} vers}\\
&f \text{ sur tout compact} \subset A\\
\end{array}\right\rbrace &\implies f\in \mathcal{C}(A,F)\\
\end{align*}
\end{theorem}
\begin{itheorem}[Théorème de la double limite]\label{Double Limite Fonctions} 
Si $f_n(x)$ \boxed{\text{\emph{converge uniformément}}}
\[\boxed{
\lim_{\textcolor{Red}{n\to\infty}}\left(\lim_{\textcolor{RoyalBlue}{x\to a}} f_n(x)\right)=\lim_{\textcolor{RoyalBlue}{x\to a}}\left(\lim_{\textcolor{Red}{n\to\infty}} f_n(x)\right)
}\]
\end{itheorem}
\begin{theorem}[Théorème d'approximation de \textsc{Weierstrass}\index{Weierstrass@\textsc{Weierstrass}!approximation}]
Toute fonction $f\in \mathcal{C}([a,b],\mathbb{C})$ est \emph{limite uniforme} d'une suite $\left(\mathcal{P}_n(X)\right)_n$ de fonctions polynômes.
\end{theorem}
Le même théorème existe pour les fonctions (T-périodiques à valeurs complexes) limites d'une suite de polynômes trigonométriques.
\section{Convergence des Séries}
\begin{dfn}
Définitions simplifiées des convergences de Séries de fonctions :\\
\begin{equation}\label{CV Serie}
\begin{array}{l c l}
\sum{f_n}\text{ converge \emph{simplement}}&\text{si}&\forall{}x\in{}A\text{, la série } \sum{f_n(x)} \text{ converge}\\[0.3cm]
\sum{f_n}\text{ converge \emph{uniformément}}&\text{si}&
\left\lbrace 
\begin{array}{l}
\begin{split}
x\in{}A\text{, la série } &(S_n)=\displaystyle\sum_0^n{f_n(x)}\\[-2mm] &\text{{\small  converge uniformément}}\end{split}\\[1cm]  
\begin{split}
x\in{}A\text{, la série } &(R_n)=\displaystyle\sum_n^\infty{f_n(x)}\\[-2mm] &\text{\small converge uniformément}\end{split}\\
\end{array}
\right. \\[2cm]
\sum{f_n}\text{ converge \emph{normalement}}&\text{si}&\sum{N_\infty (f_n)}\text{ converge}
\end{array}
\end{equation}
Pour les définitions de convergence de fonctions, se référer aux définitions \ref{CV fonctions}.
\end{dfn}
On retrouve certaines propriétés des fonctions :
\begin{theorem}

\[
\left.
\begin{array}{r}
\left(u_n(x)\right)_{n}\in\mathcal{C}(A,F)\\
\left(u_n(x)\right)_n \text{ \emph{converge uniformément} sur }A\\
\end{array}\right\rbrace \implies \sum u_n \text{ est continue sur }A\\
\]
\end{theorem}
\begin{itheorem}[Théorème de la double limite]
Si $\sum f_n$ \emph{converge uniformément}, et qu'il existe $(v_n)$ telle que $v_n=\displaystyle\lim_{x\to a}f_n(x)$, alors $\sum v_n$ converge et on a :
\[\boxed{\sum^\infty_{n=0}\underbrace{\left(\lim_{x\to a} f_n(x)\right)}_{\mathclap{=v_n}}=\lim_{x\to a}\sum_{n=0}^\infty\left(f_n(x)\right)} \]
\end{itheorem}
\begin{proof}
C'est le théorème \ref{Double Limite Fonctions} de la double limite de suites de fonctions appliqué aux séries
\end{proof}
\section{Propriétés de la somme}
\begin{itheorem}[Intégration sous le signe somme]
\begin{equation}
\begin{array}{r c l}
\left\lbrace
\begin{array}{l}
u_k \in \mathcal{CM}(I,\mathbb{C})\text{ et intégrable sur }I\\
S \in \mathcal{CM}(I,\mathbb{C})\\
\sum u_n \to S\\
\boxed{\sum |u_n| \text{ converge}} 
\end{array}
\right.&\Rightarrow &
\begin{array}{l}
S\text{ est intégrable, et}\\
\int_I S = \sum_{n=0}^{+\infty} \left( \int_I u_n \right)
\end{array}
\end{array}
\end{equation}
\end{itheorem}
\section{Séries doubles} % (fold)
\label{sec:series_doubles}
\begin{itheorem}[Interversion des sommations de \textsc{Fubini}\index{Fubini@\string\textsc{Fubini}}]\label{th:fubini}
    Si 
    $\left\lbrace \begin{array}{l}
        \big( u_{p,q} \big)_{p,q} \text{ est une suite double complexe} \\
        \forall q \in \Cmplx{}, \left( \sum \left| u_{p,q}\right| \right)_{p,q} \text{ converge} \\
        \forall q \in \Cmplx{}, \sum \left( \sum_{p=0}^{+\infty} \left| u_{p,q}\right| \right) \text{ converge}
    \end{array}\right| $, alors : 
    \begin{equation}
        \sum_{q=0}^{+\infty} \left( \sum_{p=0}^{+\infty} u_{p,q} \right) = \sum_{p=0}^{+\infty} \left( \sum_{q=0}^{+\infty}  u_{p,q} \right) 
    \end{equation}
    Avec les deux séries $\left( \sum \left( \sum_{p=0}^{+\infty} u_{p,q} \right)\right)_q$ et $\left( \sum \left( \sum_{q=0}^{+\infty}  u_{p,q} \right) \right)_p$  qui convergent
\end{itheorem}
% section series_doubles (end)
\chapter{Séries Entières}
Certains objets mathématiques ont des développements de Taylor exacts. C'est le cas notamment des polynômes, qui sont déjà des développements de Taylor. Les séries géométriques par exemple ont également un développement de Taylor exact : pour la série de terme général $q^k$, on a $\sum_0^\infty q^k = \dfrac{1}{1-q}$. Sinon, la plupart du temps, le développement de Taylor est précis jusqu'à un dernier terme qu'on ne peut calculer, mais qu'on peut quand même approximer en $o(\cdots)$ ou en $O(\cdots)$
\\
\section{Généralités}
\begin{dfn}
Soit $(a_n)_n$ une suite de nombres complexes. On appelle \emph{série entière}\index{Serie@Série!entiere@entière} de la variable complexe $z$ la série de fonctions $\sum a_n z^n$. \par
Le \emph{rayon de convergence}\index{Rayon de convergence}
est la borne supérieure de $I=\{z\in\mathbb{R}_{+},\sum|a_{n}|z^{k}\text{ converge}\}$.
C'est en fait la valeur maximale de $z$ pour laquelle la série converge.\end{dfn}
\begin{minipage}{0.6\linewidth}
Pour calculer le rayon de convergence, il importera peu de l'étudier pour les valeurs absolues, les nombres imaginaires, etc... car c'est seulement un rayon. Dans les réels, on appèlera l'intervalle $\textcolor{Red}{\left] -R,R\right[}$ l'intervalle ouvert de convergence.
\end{minipage}\hspace{0.1\linewidth}
\begin{minipage}{0.3\linewidth}
\begin{tikzpicture}
    \draw [thick, pattern=north west lines, pattern color=gray!40] (0,0) circle(1) (-120:0.9) node[below left]{$R$};    
    \draw [->,gray] (-1.2,0) -- (1.5,0) node[right]{Réels};
    \draw [->,gray] (0,-1.5) -- (0,1.2) node[above]{Imaginaires};
    \draw (0,0) -- (-60:1.3) node{$+$} node[below right]{\resizebox{.45\hsize}{!}{$\sum a_n z^n$ DV grossièrement}};
    \draw (0,0) -- (+30:0.8) node{$+$} node[above right]{\resizebox{.30\hsize}{!}{$\sum a_n z^n$ CVA}};
    \draw [very thick,Red] (-1,0) node{$]$} -- (1,0) node{$[$};
    
\end{tikzpicture}
\end{minipage}\\
\begin{lemme}[d'\textsc{Abel}]
S'il existe $\rho$ tel que la suite $\left( a_n \rho^n \right)$ soit bornée, alors 
\begin{equation}
    \forall z<\rho,\left| a_n z^n \right| \le M\left( \dfrac{|z|}{\rho} \right)^n\\ \text{et }\sum a_n z^n\text{ converge absolument}
\end{equation}
\end{lemme}
\subsection{Rayon de Convergence}
\begin{itheorem}[Relations de comparaisons]
     Si $\sum a_n z^n$ et $\sum b_n z^n$ sont deux séries entières de rayons de convergence respectifs $R_a$ et $R_b$, alors : 
     \renewcommand{\theenumi}{\roman{enumi}}
     \begin{enumerate}
         \item Si $|a_n| \le |b_n|$, alors $R_a \ge R_b$
         \item Si $a_n = O(b_n)$, alors $R_a \ge R_b$
         \item Si $a_n = o(b_n)$, alors $R_a \ge R_b$
         \item Si $a_n \sim b_n$, alors $R_a = R_b$
     \end{enumerate}
\end{itheorem}
\begin{theorem}
Sur le disque ouvert $D_R$ de convergence $]-R,R[$, la série entière $\sum a_n z^n$ converge \emphh{absolument}.
\end{theorem}
\begin{theorem}
Sur le disque ouvert $D_R$ de convergence $]-R,R[$, la série entière $\sum a_n z^n$ converge \emphh{uniformément} et sa somme est une fonction \emphh{continue}.
\end{theorem}
\begin{theorem}
     Les séries entières $\sum a_n z^n$ et $\sum na_n z^n$ ont le même rayon de convergence.
\end{theorem}
Pour déterminer le rayon, il existe plusieurs méthodes, mais on se servira principalement du théorème suivant étant une conséquence de la règle de d'\textsc{Alembert} :\\

\subsection{D'\textsc{Alembert}}
\begin{itheorem}[Règle de d'\textsc{Alembert}]
Pour la série entière $\sum a_n z^{k}$ \uline{non nulle à partir d'un certain rang}, et qu'il existe $l$ tel que 
\[
\boxed{\left|\dfrac{a_{n+1}}{a_n}\right|\xrightarrow[n\to +\infty]{}l} \text{ alors } l = \frac{1}{R}
\]
La réciproque est fausse : si on connait $R$, on n'a pas $\left|\dfrac{a_{n+1}}{a_n}\right|$\\
Sinon, on a aussi $\sqrt[n]{|a_n|}\xrightarrow[n\to +\infty]{}l$
\end{itheorem}

\begin{proof}
Il suffit d'appliquer d'Alembert (\textit{cf.} théorème~\ref{th:Regle_de_dAlembert} page~\pageref{th:Regle_de_dAlembert}) à la série de terme général $|a_n z^k|$ (qui est une \gls{SATP}) qu'on prend à partir d'un certain rang tel que $a_n \neq 0$. Donc il existe une limite $l$ telle que \[\dfrac{|a_{n+1}z^{n+1}|}{|a_nz^z|}\xrightarrow[n\to +\infty]{} l|z|\]
Donc $|a_nz^n|$ converge si $l|z| < 1$, et diverge si $l|z| > 1$.
\par
Puisque toute série entière de rayon de convergence $R>0$ est absolument convergente dans son disque ouvert de convergence.
\end{proof}
% Remarque importante du livre de Vuibert (page 441) : 
\Attention{En général, il vaut mieux utiliser le théorème original (théorème~\ref{th:Regle_de_dAlembert} page~\pageref{th:Regle_de_dAlembert}). En effet, ce théorème ne s'applique que pour des séries entières fonctions de $z^n$. Pour une série entière du type $\sum a_n z^{n^2}$, il n'est plus valable !}
\section{Série entière d'une variable réelle} % (fold)
\label{sec:serie_entiere_d_une_variable_reelle}
\subsection{Primitivation} % (fold)
\label{sub:primitivation_DSE}
\begin{theorem}
     Soit $\sum a_n z^n$ une série entière de rayon de convergence $R$. Sa primitive est :
        \begin{equation}
        \forall z\in ]-R;R[,\qquad \int \limits_0^x \left( \sum_{n=0}^{+\infty} a_n t^n \right)\d t = \sum_{n=0}^{+\infty} \dfrac{a_n x^{n+1}}{n+1} 
        \end{equation}
        Et la série entière $\sum \dfrac{a_n x^{n+1}}{n+1} $ a le même rayon de convergence $R$.
        \end{theorem}
% subsection primitivation_DSE (end)
\subsection{Dérivation} % (fold)
\label{sub:derivation_DSE}
\begin{theorem}
     La somme d'une série entière est $C^\infty$ sur l'intervalle ouvert de convergence.
\end{theorem}
\begin{itheorem}[Dérivation terme à terme]
     %TODO: compléter dérivation SE
     Soit $\sum a_n z^n$ une série entière de rayon de convergence $R$. Sa dérivée est :
     \begin{equation}
     \forall z\in ]-R;R[,\qquad \left( \sum_{n=0}^{+\infty} a_n z^n \right)' = \sum_{n=\textcolor{couleurImp}{1}}^{+\infty} n a_n x^{n-1} 
     \end{equation}
     Et la série entière $\sum_{n=\textcolor{couleurImp}{1}}^{+\infty} n a_n x^{n-1}$ a le même rayon de convergence.
\end{itheorem}
\Attention{Le théorème de dérivation n'est valable qu'à l'intérieur du disque de convergence !}
% subsection derivation_DSE (end)
% section serie_entiere_d_une_variable_reelle (end)
\section{Fonctions développables en série entière} % (fold)
\label{sec:fonctions_DSE}
\begin{dfn}[Fonction développable en série entière]
$f:\mathbb{K}\to\mathbb{K}$ admet un DSE\index{Developpement en serie entiere@Développement en Série Entière} en $0$ s'il existe une série entière $\sum a_n z^n$ telle que
\begin{equation}
\forall z \in \textcolor{gray}{\overset{\mathclap{\text{\begin{tiny}
boule ouverte de centre 0
\end{tiny}}}}{V}}, f(z) = \sum a_n z^{n}
\end{equation}
Cette boule ouverte de centre $0$ est inclue dans le disque de convergence du \gls{DSE} : $V \subset ]-R ; R[$.
\end{dfn}

\begin{theorem}[\gls{CNS} d'un DSE] % OLD -> revoir, maybe? 

\[
f:\mathbb{R}\rightarrow K\text{ est un DSE en 0}\Leftrightarrow \boxed{\displaystyle \int_0^x \dfrac{(x-t)^n}{n!}f^{n+1}(t) dt \underset{n\to\infty}{\to}0}
\]

\end{theorem}
% section fonctions_DSE (end)

% OLD
\section{Propriétés de la somme}
\subsubsection*{Continuité}
\begin{theorem}
La somme d'une série entière est continue sur son disque ouvert de convergence $R$
\end{theorem}
\subsubsection*{Dérivabilité}
\begin{theorem}[Dérivées successives]
Les dérivées successives d'une série entière de rayon de convergence $R>0$ ont toutes le même rayon de convergence $R$
\end{theorem}
\begin{theorem}
Pour des séries entières avec $R = \min (R_a,R_b)$, alors : 
\[
\forall x \in ]-R,R[, \sum a_n x^n = \sum b_n x^n \implies \forall n \in \mathbb{N}, a_n = b_n
\]
\end{theorem}

\chapter{Calcul Différentiel et Intégral}
\section{Dérivation}
\begin{dfn}[Dérivabilité\index{Derivabilite@Dérivabilité}]
$f : I \to \Reel$ est dérivable en $a \in I$ si $\lim_{x\to a} \dfrac{f(x) - f(a)}{x - a}$ existe.
\end{dfn}
On notera $\mathcal{D}(I,\Reel )$ l'ensemble des applications dérivables de $I$ dans \Reel{}.

\begin{theorem}
$f$ dérivable $\Leftrightarrow \exists l$ tel que $f(x) \underset{x\to a}{=} f(a) + (x-a)l + (x-a)\varepsilon (x)$\\
Alors, $l$ est la dérivée en $a$ de $f$
\end{theorem}

\begin{prop}
\begin{description}
    \item[Continuité] $f$ dérivable $\implies f$ continue
    \item[Linéarité] $\left( f + \alpha g \right)' = f' + \alpha g'$
    \item[Dérivées usuelles] \hfill \\[-0.5cm]
        \begin{description}
            \item[Application linéaire] $u$ linéaire, $f$ dérivable; $\left( u \circ f \right)'(x) = u\circ f'(x)$
            \item[Application multi-linéaire] $\varphi$ une application $n$-linéaire;\\ 
            $\bigg( \varphi (f_1, \cdots, f_n) \bigg)'(x) = \sum_{i=0}^n \varphi \bigg( f_1(x), \cdots, f_{i-1}(x), \textcolor{Red}{f_i'(x)},f_{i+1}(x), \cdots , f_n(x) \bigg)$
            \item[Quotient] $u$ et $v$ dérivables; $\left( \dfrac{u}{v} \right)' = \dfrac{u'v - uv'}{v^2}$
            \item[Composition] $f$ et $g$ dérivables; $\left( f \circ g \right)' = f'\; g'(f)$
        \end{description}
\end{description}
\end{prop}

\begin{dfn}[Application $\mathcal{C}^1$]
$f \in \mathcal{C}^1\big( E,F \big)$ si l'application $f' : a \mapsto f'(a)$ existe et est continue.
\end{dfn}


\begin{dfn}[Dérivée $k$-ième]
On définit récursivement la dérivée $k$-ième $f^{(k)}$ : 
\begin{equation}
    f^{(k)}=\big( f^{(k-1)} \big) '
\end{equation}
\end{dfn}
\begin{dfn}[Application de classe $\mathcal{C}^k$]
$f$ est $\mathcal{C}^k$ si $f$ est $k$ fois dérivable et si $f^{(k)}$ est continue.
\end{dfn}

\begin{theorem}[\textsc{Leibniz}]
Soit $\varphi$ une application bilinéaire, alors :
\begin{equation}\tag{Leibniz}\label{Leibniz}
\varphi^{(n)}\big( f,g \big) = \sum_{k=0}^n \begin{pmatrix} n \\ k \end{pmatrix} \varphi \big( f^{(k)},g^{(n-k)} \big)
\end{equation}
\end{theorem}
\section{Intégration}
\subsubsection{Inégalité de la moyenne} % (fold)
\label{ssub:inegalite_de_la_moyenne}
\begin{itheorem}[Cas réel]
    Si $f$ est \emphh{continue} sur un intervalle $[a,b]$ et qu'il existe $m$ et $M$ tels que : 
    \[
        m \le f(x) \le M
    \]
    Alors 
    \begin{equation}\label{eq:inegalite_de_la_moyenne}
    m(b-a) \le \int_a^b f(x) \mathrm{d}x \le M(b-a)
    \end{equation}
\end{itheorem}
    
% subsubsection inegalite_de_la_moyenne (end)
\section{Primitive}
\begin{dfn}[Primitive]
$F$ est une \emph{primitive}\index{Primitive} de $f$ si $\forall x, F'(x)=f(x)$. 
\end{dfn}
\begin{theorem}
%TODO Compléter le théorème
Si $F$ est la primitive de $f$,
\begin{equation}
    \int \limits_a^x f(t) dt = F(x) - F(a)
\end{equation}
\end{theorem}

\section{Accroissements finis}
\subsection{Cas réel} % (fold)
\begin{theorem}[Accroissements finis\index{Accroissements finis}]
\label{thm:accroissements_finis}
\begin{minipage}{0.6\textwidth}
$f \in \mathcal{D}\big( ]a,b[,\Reel{} \big)$, alors 
\begin{equation}
\exists \: c \in ]a,b[ \text{ tel que }\boxed{f(b) - f(a) = (b-a) f'(c)}
\end{equation}
\end{minipage}\hspace{1cm}
\begin{minipage}{0.3\textwidth}
\begin{tikzpicture}
    \draw [dashed, thick, gray] (0,0.875) -- (3,1.875) node[midway, above]{$f'(c)$};
    \draw [thick] (0,0) .. controls (0.3,0) and (0.7,1.1) .. (1,1.2) .. controls (1.4,1.3) and (2.5,0.8) .. (3,1);
    \draw (0,0) node{$\bullet$} -- (3,1) node{$\bullet$};
    \draw (0,-0.2) node[below]{$a$} -- (3,-0.2) node[below]{$b$} node[midway, below]{$[a,b]$};
\end{tikzpicture}
\end{minipage}
\end{theorem}
\subsection{Cas vectoriel}
\begin{itheorem}[Accroissements finis\index{Accroissements finis}]
\begin{minipage}{0.6\textwidth}
$f \in \mathcal{C}\big( ]a,b[,\Reel{} \big)$\\ 
Si $\exists \: \lambda$ tel que $ \forall t\in ]a,b[, \textcolor{couleurImp}{N}(f'(t))\leq \lambda$, alors
    \begin{equation}
        \boxed{\textcolor{couleurImp}{N\big( } f(b) - f(a)\textcolor{couleurImp}{\big) } \leq \textcolor{couleurImp}{\lambda} \big( (b-a)\big)}
    \end{equation}
\end{minipage}\hspace{1cm}
\begin{minipage}{0.3\textwidth}
\begin{tikzpicture}
    \draw [color=couleurImp!70!Black] (0,0.575) -- (3,2.5) node[midway, above]{$\lambda$};
    \draw [dashed, thick, gray] (0,0.875) -- (3,1.875);
    \draw [thick] (0,0) .. controls (0.3,0) and (0.7,1.1) .. (1,1.2) .. controls (1.4,1.3) and (2.5,0.8) .. (3,1);
    \draw (0,0) node{$\bullet$} -- (3,1) node{$\bullet$};
    \draw (0,-0.2) node[below]{$a$} -- (3,-0.2) node[below]{$b$} node[midway, below]{$[a,b]$};
\end{tikzpicture}
\end{minipage}
\end{itheorem}

\section{Formules de Taylor}
\begin{itheorem}[Formules de Taylor\index{Taylor!Formules@\string\textit{(Formules)}}]
$f\in\mathcal{C}^{k+1}(I,E)$, avec $(a,b)\in I^2$
\[
\begin{array}{l c r c l}
\text{\textbf{Taylor-Young}\index{Taylor!Young}}&f(x)\underset{x\to a}{=}&\displaystyle \sum_{k=0}^{n} \dfrac{(x-a)^k}{k!}f^{(k)}(a)&+&o\left( (x-a)^n \right)   \\[0.7cm]
\text{\textbf{Taylor-Laplace}\index{Taylor!Laplace}}&f(x)=&\overbrace{\sum_{p=0}^k \dfrac{(x-a)^p}{p!}f^{(p)}(a)}^{\mathclap{T_k(x)}} & + & \underset{=}{\overbrace{\int_a^x \dfrac{(x-t)^{k}}{k!}f^{(k+1)}(t)dt}^{\mathclap{\text{Reste intégral }R_k(x)}}}\\
&&&&\resizebox{.35\hsize}{!}{$(x-a)^{k+1}\int_0^1\dfrac{(1-u)^k}{k!}f^{(k+1)}\left( (1-u)a+ux \right) du$}\\
\end{array}\\
\]

\[
\text{\textbf{Taylor-Lagrange} \index{Taylor!Lagrange}} f(x)= \resizebox{.7\hsize}{!}{$N\left( f(b) - f(a) - \displaystyle\sum_{p=1}^k\dfrac{(b-a)^p}{p!}f^{(p)}(a) \right)\le \dfrac{|b-a|^{k+1}}{(k+1)!}N_\infty^{[a,b]}\left( f^{(k+1)} \right)$}
\]

\end{itheorem}
%\begin{theorem}[Théorème d'interversion des sommations de \textsc{Fubini}]
%
%\end{theorem}

\needspace{19cm}
\chapter{Intégrales sur un intervalle}
%
\begin{methode}
\titre{Définitions rapides}
\begin{description}\itemsep2pt
\item[Intégrabilité]$f(x)$ intégrable si $\int \textcolor{Red}{|}f(x)\textcolor{Red}{|} < +\infty$
\item[Norme de la convergence en moyenne] \hfill \\ $N_1 : \int_I |f|$
\item[Norme de la convergence en moyenne quadratique] \hfill \\ $N_2 : f \to (f|f)^{\frac{1}{2}}$ (\textit{cf} théorème \ref{NormeAssocieProdScal} de la page \pageref{NormeAssocieProdScal})
\end{description}

\titre{Pour prouver l'intégrabilité}
\begin{enumerate}\itemsep3pt
	\item Vérifier la continuité sur l'intervalle étudié
	\item Remplacer la fonction par sa \uline{valeur absolue}
	\item Étudier les problèmes aux bornes
	\begin{itemize}\itemsep2pt
		\item Trouver un équivalent
		\item Trouver un $o(\cdots)$
		\item Avoir une primitive dont la limite vers la borne est finie
		
	\end{itemize}
	\item Comparer la fonction \hfill \\ On pourra utiliser :
	\begin{itemize}\itemsep2pt
		\item La fonction \emph{exponentielle} ; % WHAT ? Pourquoi j'ai écrit ça ??? Croissances comparées peut-être ? 
        \item Les croissances comparées ;
		\item L'intégrale de \emph{\textsc{Riemann}} : $\frac{1}{t^\alpha}$ (théorème \ref{thm:integrale_riemann} page~\pageref{thm:integrale_riemann}) \\
        \item Une extension de \textsc{Riemann} : par exemple, $f$ est intégrable sur $]0,1[$ si $\exists \alpha < 1$ tel que $f(x)\, x^\alpha \xrightarrow[x \to 0]{} 0$, car alors, quand $x$ tend vers $0$, $x^\alpha f(x) \le 1$ et on retrouve \textsc{Riemann}.
		\item L'intégrale de \emph{\textsc{Bertrand}} (hors programme) : $\dfrac{1}{|\ln (t)|^\beta t^\alpha}$
	\end{itemize}
\end{enumerate}
\titre{Pour intégrer}
On utilisera
\begin{enumerate}
	\item Les intégrations par partie
	\item Un changement de variable
	\item Cauchy-Schwarz
	\item L'intégrale d'un polynôme est un polynôme
\end{enumerate}
Souvent, on ne peut intégrer sur tout un intervalle comme $[0;+\infty ]$. Pour y remédier, on peut poser $a>0$ tel que notre fonction soit intégrable sur $[a;+ \infty [$. Alors, la fonction est intégrable sur $\cup [a;+\infty [ = \Reel{}^+$
\titre{Formes usuelles}
\begin{tabulary}{\textwidth}{| c | C |}
     \emphh{Forme} & \emphh{Méthode} \\
     $\dfrac{\sqrt{a} - \sqrt{b}}{x}$ & Multiplier le dénominateur et le numérateur par le conjugué $\sqrt{a}+\sqrt{b}$ pour obtenir $a-b$ en haut \\
     $\dfrac{\ln x}{x^\alpha}$ & On remarque que $x^{\frac{\alpha + 1}{2}}\dfrac{\ln x}{x^\alpha} = \dfrac{\ln x}{x^{\frac{\alpha - 1}{2}}}$ qui tend vers $0$ en $+\infty$. On a alors $\dfrac{\ln x}{x^\alpha}\underset{+\infty}{=}o \left( \dfrac{1}{x^{\frac{\alpha + 1}{2}}} \right) $. Plus qu'à appliquer \textsc{Riemann}. \\
     $\dfrac{1}{(x+a)\cdots(x+z)}$&Décomposition en éléments simples !\\
\end{tabulary}
\titre{Applications classiques des théorèmes}
\begin{tabulary}{\textwidth}{| r@{ - (} c @{) }| C |}
     \small{Dérivation d'une intégrale à paramètre}&\ref{thm:derivation_d_une_integrale_parametres}&Montrer que $\lim\left( \int \cdots\right) = \int \left( \lim \cdots\right) $\\
     \small{Intégration terme-à-terme}&\ref{thm:integration_terme_terme}&Montrer une égalité du type $\int f_\text{\gls{DSE}} = \int \left( \sum (\gls{DSE}) \right) = \sum \left( \int \cdots \right)$\\
     Convergence dominée&\ref{thm:convergence_dominee}&Dériver une fonction de type $f(x)=\int_I \varphi(x,t)\d t$
\end{tabulary}
\end{methode}
\section{Intégrale généralisée sur un intervalle de la forme $[a,+\infty[$} % (fold)
\label{sec:integrale_generalisee}
\subsection{Définition} % (fold)
\label{sub:definition}
\begin{dfn}[Intégrale convergente]
    \label{dfn:integrale_convergente}
     Pour $f : [a,+\infty[ \to \mathbb{K}$ une fonction $\mathcal{CPM}$, l'intégrale $\int_a^{+\infty} f$ est dite \emph{convergente}\index{Integrale@Intégrale!convergente} si la fonction $F : x \mapsto \int_a^x f$ a une limite finie en $x\to +\infty$.\\
     Si tel est le cas, on note $\int_a^{+\infty} f$ cette limite.
\end{dfn}
% subsection definition (end)
\subsection{Propriétés de l'intégrale} % (fold)
\label{sub:proprietes_integrale}
\begin{dfn}[Notation $\mathcal{L}^\frac{1}{2}$]
     Soient $E$ et $F$ deux intervalles de $\mathbb{K}$.\\
     On note $\mathcal{L}^\frac{1}{2}(E,F)$ l'ensemble des fonctions $f \in \mathcal{CPM}(E,F)$ dont la valeur absolue $|f|$ est intégrable sur $E$.
\end{dfn}
\Attention{Cette notation n'existe pas, je l'ai inventée pour alléger la suite}
\begin{theorem}[Linéarité de l'intégration]
    \label{thm:integration_linearite}
     $\mathcal{L}^\frac{1}{2}\left( [a,+\infty [,\mathbb{K} \right)$ est un $\mathbb{K}$-espace vectoriel.\\
     L'application $\left\lbrace \begin{array}{r c l}\mathcal{L}^\frac{1}{2}\left( [a,+\infty [,\mathbb{K} \right) &\longto &\mathbb{K}\\f&\longmapsto &\int_a^{+\infty} f\end{array} \right\rbrace$ est \emphh{linéaire}.
\end{theorem}
\begin{theorem}[Positivité]
    \label{thm:integration_positivite}
     Soit $f \in \mathcal{L}^\frac{1}{2}\bigg( [a,+\infty [,\mathbb{K}\bigg)$.\\
     Si $f$ est positive, alors : 
     \begin{equation}
     \int_a^{+\infty} f \ge 0
     \end{equation}
     De plus, si $f$ est \emphh{continue} sur $[a,+\infty[$ et si $\int_a^{+\infty} f = 0$, alors $f$ est nulle sur $[a,+\infty[$.
\end{theorem}
\begin{theorem}[Dérivation]
     Soit $f$ une application \emphhs{continue} de $\mathcal{L}^\frac{1}{2}\left( [a,+\infty [,\mathbb{K}\right)$.\\
     L'application $\left\lbrace \begin{array}{r c l} [a,+\infty [ &\longto &\mathbb{K}\\x&\longmapsto &\int_x^{+\infty} f\end{array} \right\rbrace$ est \emphh{dérivable} et de dérivée $(-f)$ sur $[a,+\infty [$.
\end{theorem}
% subsection proprietes_integrale (end)
% section integrale_generalisee (end)
\section{Intégrabilité sur un intervalle de la forme $[a,+\infty [$} % (fold)
\label{sec:integrabilite}
\begin{dfn}[Fonction intégrable]
    \label{dfn:integrable}
     Soit $f$ une fonction $\mathcal{CPM}$ de $[a,+\infty[$ dans $\mathbb{K}$.\\
     On dit que $f$ est \emph{intégrable}\index{Integrable@Intégrable} sur $[a,+\infty[$ si $\int_a^{+\infty} |f|$ est convergente : 
     \begin{equation}
     f\text{ intégrable si } |f|\in\mathcal{L}^1\bigg([a,+\infty[,\mathbb{K}\bigg)
     \end{equation}
\end{dfn}
\emphh{Remarque : }Cela revient à dire que $f$ est intégrable si $\int_a^{+\infty} f$ converge absoluement. 
\begin{dfn}[Notation $\mathcal{L}^1$]
     Soient $E$ et $F$ deux intervalles de $\mathbb{K}$.\\
     On note $\mathcal{L}^1(E,F)$ l'ensemble des fonctions $f \in \mathcal{CPM}(E,F)$ dont la valeur absolue $|f|$ est intégrable sur $E$.
\end{dfn}
\Attention{Une fonction dont l'intégrale converge n'est pas forcément intégrable (\textit{cf.} définition \ref{dfn:integrable} page \pageref{dfn:integrable}), c'est sa valeur absolue qui doit avoir une intégrale convergente !}\\
% Remarque de Vuibert : 
Si $\int_a^{+\infty} f$ est seulement convergente, l'intégrale est dite \emph{semi-convergente}\index{Integrale@Intégrale!semi-convergente}
\begin{theorem}
     Si $f$ est intégrable sur $[a,+\infty[$, alors $\int_a^{+\infty} f$ converge.
\end{theorem}
% section integrabilite (end)
\section{Intégration des fonctions positives sur un intervalle de la forme $[a,+\infty[$} % (fold)
\label{sec:integration_fonction_positives}
\begin{itheorem}[Fonction positive intégrable]
    \label{thm:fonction_positive_integrable}
     Soit $f$ une fonction positive sue $[a,+\infty [$.\\
     L'intégrale $\int_a^{+\infty} f$ converge \ssi{} $x \longmapsto \int_a^{+\infty} f$ est majorée : 
     \begin{equation}
     \int_a^{+\infty} f\text{ converge }\Longleftrightarrow \exists M\in \mathbb{R}^+\text{ tel que }\int_a^{+\infty} f \le M
     \end{equation}
\end{itheorem}
\begin{itheorem}[Intégrabilité de $x \mapsto \frac{1}{x^\alpha}$]
\label{thm:integrale_riemann}
     Soit $\alpha$ dans \Reel{}.\\
     La fonction $x \longmapsto \dfrac{1}{x^\alpha}$ est intégrable sur $[1,+\infty [$ \ssi{} $\alpha > 1$.\\[1.3\baselineskip] 
     On étudie ici des intervalles du type $[a,+\infty [$, mais il peut être bon de savoir que la fonction $x \longmapsto \dfrac{1}{x^\alpha}$ est intégrable sur $[0,1 [$ \ssi{} $\alpha < 1$.
\end{itheorem}
\begin{itheorem}[Relations de comparaison]
    \label{thm:relations_de_comparaison_fonction_positive}
     Soient $f$ et $g$ deux fonctions \emphhs{réelles}, \emphhs{positives}, et \emphhs{continues par morceaux} sur $[a,+\infty [$ : 
     % \begin{itemize}
         % \item si $0\le f \le g$, l'intégrabilité de $g$ sur $[a,+\infty [$ implique celle de $f$ ;
         % \item si $f(x) \underset{x\to +\infty}{=} O\big( g(x)\big)$, l'intégrabilité de $g$ sur $[a,+\infty [$ implique celle de $f$ ;
         % \item si $f(x) \underset{x\to +\infty}{\sim} g(x)$, l'intégrabilité de $g$ sur $[a,+\infty [$ \emphh{équivaut} à celle de $f$.
     % \end{itemize}
     % Version plus explicite : 
     \\[0.4\baselineskip]
     \begin{tabular}{@{si }c@{, \qquad}c }
         $0\le f \le g$&$g \in \mathcal{L}^1\left( [a,+\infty [,\Reel{}\right)\implies f \in \mathcal{L}^1\left( [a,+\infty [,\Reel{}\right)$ \\[6mm]
             $f(x) \, \underset{\mathclap{x\to +\infty}}{=} \, \mathcal{O}\big( g(x)\big)$& $g \in \mathcal{L}^1\left( [a,+\infty [,\Reel{}\right)\implies f \in \mathcal{L}^1\left( [a,+\infty [,\Reel{}\right)$\\[6mm]
                 $f(x) \;\underset{\mathclap{x\to +\infty}}{\textcolor{couleurImp}{\sim}}\; g(x)$&$g \in \mathcal{L}^1\left( [a,+\infty [,\Reel{}\right)\textcolor{couleurImp}{\Longleftrightarrow} f \in \mathcal{L}^1\left( [a,+\infty [,\Reel{}\right)$
     \end{tabular}\\
\end{itheorem}
% section integration_fonction_positives (end)
\section{Intégration sur un intervalle quelconque} % (fold)
\label{sec:integration_intervalle_quelconque}
\subsection{Sur un intervalle semi-ouvert} % (fold)
\label{sub:sur_un_intervalle_semi_ouvert}
% Je vais pas réecrire/copier tout en changeant juste un signe, ça va alourdir pour rien, donc : 
Les définitions et théorèmes vus dans les sections précédentes s'appliquent en remplaçant $+\infty$ par un réel $b$ quelconque de \Reel{}. On effectue alors l'étude sur un semi-ouvert $[a,b[$. On retrouve alors : 
\begin{itemize}[label=$-$]
    \item la définition \ref{dfn:integrale_convergente} d'une intégrale convergente ;
    \item la définition \ref{dfn:integrable} d'une fonction intégrable ;
    \item le théorème \ref{thm:fonction_positive_integrable}, \gls{CNS} d'une fonction positive intégrable ;
    \item le théorème \ref{thm:relations_de_comparaison_fonction_positive} des relations de comparaisons.
\end{itemize}
\begin{itheorem}[Intégrabilité de $x\mapsto \frac{1}{(x-a)^\alpha}$]
     Soit $\alpha$ dans \Reel{}.\\
     La fonction $x \longmapsto \dfrac{1}{(x-a)^\alpha}$ est intégrable sur $]a,b]$ \ssi{} $\alpha < 1$ ($x-a > 0$). \\
     La fonction $x \mapsto \dfrac{1}{|x-a|^\alpha}$ est intégrable sur $[b,a[$ \ssi{} $\alpha < 1$ \newline ($x-a < 0$).
\end{itheorem}
% subsection sur_un_intervalle_semi_ouvert (end)
\subsection{Sur un intervalle de la forme $]a,b[$} % (fold)
\label{sub:intervalle_ouvert}
\begin{dfn}[L'ensemble $\overline{\Reel}$]
     On note $\overline{\Reel}$ l'ensemble $\Reel{} \cup \{ -\infty, +\infty \}$. 
\end{dfn}
\Attention{L'ensemble $\overline{\Reel}$ n'a aucune des propriétés algébriques familières qu'on a dans $\Reel{}$ : par exemple, $a+b=a+c\implies b=c$ n'est plus vrai dans $\overline{\Reel}$.} \\ % Remarque de Aleg sur 'http://www.les-mathematiques.net/phorum/read.php?4,317735,317744#msg-317744'
\begin{dfn}[Intégrale convergente sur un ouvert]
     Soit $f$ une fonction $\mathcal{CPM}\big( ]a,b[,\mathbb{K}\big)$. Soit $(a,b) \in \overline{\Reel}^2$. \\
     L'intégrale $\int_a^b f$ est dite \emph{convergente}\index{Integrale@Intégrale!convergente} s'il existe $c\in ]a,b[$ tel que les deux intégrales $\int_a^c f$ et $\int_c^b f$ convergent sur leur intervalle semi-ouvert. On pose alors : 
     \begin{equation}
     \int_a^b f = \int_a^c f + \int_c^b f
     \end{equation}
\end{dfn}
% Idem : je vais pas réecrire/copier tout en changeant juste un signe, ça va alourdir pour rien, donc : 
Les définitions et théorèmes vus dans les sections précédentes s'appliquent en remplaçant $+\infty$ par un réel $b$ quelconque de \Reel{}. On effectue alors l'étude sur un ouvert $]a,b[$ et on retrouve : 
\begin{itemize}[label=$-$]
    \item la définition \ref{dfn:integrale_convergente} d'une intégrale convergente ;
    \item la définition \ref{dfn:integrable} d'une fonction intégrable ;
    \item le théorème \ref{thm:fonction_positive_integrable}, \gls{CNS} d'une fonction positive intégrable.
\end{itemize}
% subsection intervalle_ouvert (end)
\subsection{Sur un intervalle $I$ quelconque} % (fold)
\label{sub:sur_un_intervalle_i}
\begin{theorem}[Relation de \textsc{Chasles}]
     Soit $f \in \mathcal{CPM}(I,\mathbb{K})$ dont l'intégrale sur $I$ converge. Pour tout $(a,b,c)\in \overline{I}^3$ dans $\overline{\Reel{}}$, la \emph{relation de \textsc{Chasles}}\index{Chasles@\textsc{Chasles}} nous donne : 
     \begin{equation}
     \int_a^b f + \int_b^c f = \int_a^c f \tag{Chasles}
     \end{equation}
\end{theorem}
Soit $I$ un intervalle quelconque entre deux bornes $a$ et $b$. \\
Soit $f\in \mathcal{L}^1(I,\mathbb{K})$, on note $\int_I f$ l'intégrale $\int_a^b f$. On retrouve :
\begin{itemize}[label=$-$]
    \item le théorème \ref{thm:integration_linearite} de linéarité de l'intégration : $\mathcal{L}^1(I,\mathbb{K})$ est un $\mathbb{K}$-espace vectoriel ;
    \item le théorème \ref{thm:integration_positivite} de positivité de l'intégration.
\end{itemize}
\begin{theorem}[Inégalité triangulaire]
    Soit $f$ et $g$ deux fonctions de $\mathcal{L}^p(I,\mathbb{K})$ avec $p\in \mathbb{R}$. L'\emph{inégalité triangulaire} des intégrale donne :
     \begin{equation}
     \left(\int_I|f+g|^p \right)^{\frac 1 p} \leq \left(\int_I|f|^p\right)^{\frac 1 p}+\left(\int_I|g|^p\right)^{\frac 1 p}
     \end{equation}
     C'est l'inégalité de \textsc{Minkowski} appliquée aux intégrales. Elle est au programme dans le chapitre~\ref{cha:fonctions_convexes} des fonctions~convexes (page~\pageref{cha:fonctions_convexes}), en tant qu'\og{}exemple d'inégalités de convexité\fg{}.
\end{theorem}
%todo DEMO MINKOWSKI
\todo{Démo de \textsc{Minkowski}}
\begin{itheorem}[Changement de variable]
     Soit $f$ une fonction \emphh{continue} sur $]a,b[$ et soit ${\varphi : ]\alpha , \beta [ \to ]a,b[}$ une fonction \emphhs{bijective} (donc strictement monotone) et de classe $\mathscr{C}^1$. \\
     Les intégrales $\int_a^b f$ et $\int_\alpha^\beta f\circ \varphi \cdot |\varphi'|$ sont de même nature. Si elles convergent, elles sont égales : 
     \begin{equation}
     \int \limits_a^b f(t)\d t = \int \limits_\alpha^\beta f\Big(\varphi(u)\Big)\cdot \varphi'(u) \d u
     \end{equation}
     \emphh{Remarque :} On a nécessairement $\lim_{x\to \alpha} \varphi (x) = a$ et $\lim_{x\to \beta} \varphi (x) = b$.
\end{itheorem}
\begin{itheorem}[Intégration par parties]
     Soit $(a,b)\in \overline{\Reel}^2$. Soient $f$ et $g$ deux fonctions $\mathscr{C}^1$ sur $[a,b]$. \\
     Si $f\cdot g$ possède des limites finies aux bornes $a$ et $b$ de l'intervalle, alors les intégrales $\int_a^b f\cdot g'$ et $\int_a^b f'\cdot g$ sont de même nature.\\
     Si les intégrales convergent : 
     \begin{equation}
     \int \limits_a^b f(t)g'(t)\d t = \left[ fg \right]_a^b - \int \limits_a^b f'(t)g(t)\d t
     \end{equation}
     où $\left[ fg \right]_a^b = \lim_{x\to b} (f\cdot g)(x) - \lim_{x\to a} (f\cdot g)(x) $
\end{itheorem}
% subsection sur_un_intervalle_i (end)
% section integration_intervalle_quelconque (end)
\section{Intégration des relations de comparaison} % (fold)
\label{sec:integration_des_relations_de_comparaison}
\begin{itheorem}[1\ier{} théorème d'intégration des relations de comparaison]
    Soient $f$ et $g$ deux fonctions $\mathcal{CPM}\Big( [a,b[, \Reel^+ \Big)$ telles qu'on ait une des conditions suivantes : $
    \left\lbrace
    \begin{aligned}
        f &\le g    \\
        f &= o(g)   \\
        f &= O(g)
    \end{aligned}
    \right. $ (\textit{cf}. définition~\ref{dfn:domination_negligeabilite} page~\pageref{dfn:domination_negligeabilite}% C'est une définition avec les suites, mais c'est pas grave :D
    ), alors :
    \begin{subequations}
    \begin{equation}
     \int_a^b g\text{ converge} \implies \int_a^b f\text{ converge}
    \end{equation}
    On a également la contraposée : 
    \begin{equation}
    \int_a^b f\text{ diverge} \implies \int_a^b g\text{ diverge}
    \end{equation}
    \end{subequations}
\end{itheorem}
\emphh{Remarque :} D'après le programme officiel, seule la fonction de référence doit être positive. % Le problème c'est que je ne sais pas c'est laquelle moi :D Je pense que c'est g
\begin{itheorem}[2\ieme{} théorème d'intégration des relations de comparaison]
     Soient $f$ et $g$ deux fonctions $\mathcal{CPM}\Big( [a,b[, \Reel^+ \Big)$. \newline
     Si $f \sim g$, alors $f$ et $g$ sont de même nature.
\end{itheorem}
% section integration_des_relations_de_comparaison (end)
\section{Passage à la limite sous l'intégrale} % (fold)
\label{sec:passage_la_limite_sous_l_integrale}
\subsection{Convergence dominée} % (fold)
\label{sub:convergence_dominee}
\begin{itheorem}[Convergence dominée\index{Convergence!dominee@dominée} ]
\label{thm:convergence_dominee}
    Soit $n\in \mathbb{N}$. Si $\big( f_n \big)_n$ est une suite de fonction de $\mathcal{CPM}(I,K) $ convergeant simplement vers une fonction $f \in \mathcal{CPM}(I,K)$, et s'il existe $\varphi \in \mathcal{L}^1 $ telle que \ibox{$\forall n, | f_n | \le \varphi$},\newline
    alors $f \in \mathcal{L}^1$ et $\int_I f = \lim_{n \to +\infty} \int_I f_n$
\end{itheorem}
\begin{theorem}[Convergence dominée de fonction à paramètre réel]
\label{thm:convergence_dominee_reel}
    Soit $\lambda\in J\subset \Reel{}$. On suppose que :
    \begin{itemize}
        \item $\big( f_\lambda \big)_\lambda$ est une suite de fonction de $\mathcal{CPM}(I,K)$ ;
        \item pour tout $x\in I$, pour tout $\lambda_0\in \overline{J}$, $f_\lambda (x) \xrightarrow[\lambda \to \lambda_0]{} f (x)$ avec $f \in \mathcal{CPM}(I,K)$ ;
        \item il existe $\varphi \in \mathcal{L}^1 $ telle que \ibox{$\forall n, | f_\lambda | \le \varphi$}.
    \end{itemize}
    Alors, $f \in \mathcal{L}^1$, et $\int_I f = \lim_{\lambda \to \lambda_0} \int_I f_\lambda$
\end{theorem}
\todo{Démo !}
% TODO: Démo du paramètre réel. On la fait par caractérisation séquentielle sur le théorème de CVD.
% subsection convergence_dominee (end)
\subsection{Intégration terme à terme} % (fold)
\label{sub:integration_terme_terme}
\begin{itheorem}[Intégration terme à terme]
\label{thm:integration_terme_terme}
    Si $
        \displaystyle
        \left.
        \begin{array}{c c l}
        \displaystyle\sum & \underbrace{f_n} & \xrightarrow{\text{CVS}} f \\
                     & f_n \in \mathcal{L}^1 & \\
        \displaystyle\sum  \int_I \! \vphantom{\overbrace{f_n}} & \textcolor{couleurImp}{|}\overbrace{f_n}\textcolor{couleurImp}{|} & \text{converge}
        \end{array}
        \right\rbrace$, alors $f$ est intégrable et :
        \begin{equation}
        \int_I f = \sum_{n=0}^{+\infty} \int_I f_n
        \end{equation}
\end{itheorem}
\emphh{Remarque :} L'hypothèse de convergence de $\displaystyle\sum  \int_I |f_n|$ est en faite une hypothèse de domination.
% subsection integration_terme_terme (end)
% section passage_la_limite_sous_l_integrale (end)
\section{Continuité d'une intégrale à paramètre} % (fold)
\label{sec:continuite_d_une_integrale_parametre}
\begin{theorem}[Continuité]
La fonction $g : x \mapsto \int_I f(x,t)$ est \uline{\emph{définie et continue}} sur $A$ si 
\begin{equation}\left\lbrace
\begin{array}{l l c}
\forall x\in A,& t \mapsto f(x,t) \in \mathcal{CM}&\boldsymbol{( \mathcal{CPM} }\text{\textbf{ pour }}\boldsymbol{t)}\\
\forall t\in I,& x \mapsto f(x,t) \in \mathcal{C}&\text{\textbf{(Continue pour }}\boldsymbol{x)}\\
\begin{array}{r}
    \forall (x,t) \in (A\times I)\\
    \exists \varphi \in \mathscr{L}^{1}\text{ tel que } \\
\end{array}& |f(x,t)|\le \varphi&\text{\textbf{(Domination)}}\\
\end{array}
\right.
\end{equation}
On a aussi la version avec $\mathcal{C}^{k}$ mais ce n'est pas au programme
\end{theorem}
% section continuite_d_une_integrale_parametre (end)
\section{Dérivation d'un intégrale à paramètre} % (fold)
\label{sec:derivation_d_un_integrale_parametre}
\begin{itheorem}[Dérivabilité]
\label{thm:derivation_d_une_integrale_parametres}
La fonction $g : x \mapsto \int_I f(x,t)$ est \uline{\emph{dérivable et continue $\left( \mathcal{C}^1 \right)$}} sur $A$ si 
\begin{equation}
\left\lbrace
\begin{array}{l}
\forall x\in A, t \mapsto f(x,t) \in \mathcal{CM}\quad \boldsymbol{(f} \text{\textbf{ intégrable pour }}\boldsymbol{t)}\\
f \text{ admet une dérivée partielle qui vérifie} \\
\left\lbrace
\arraycolsep=0.5pt\def\arraystretch{1.8}
\begin{array}{r l l}
\forall x \in A,& t\mapsto \dfrac{\partial f}{\partial x}(x,t) \in \CM &\boldsymbol{( \mathcal{CPM} }\text{\textbf{ pour }}\boldsymbol{t)}\\
\forall t\in I,& x\mapsto \dfrac{\partial f}{\partial x}(x,t) \in \mathcal{C} &\text{\textbf{(Continue pour }}\boldsymbol{x)}\\
\forall (x,t) \in A \times \underbrace{[a,b]}_{\mathclap{\in I}},& \textcolor{Red}{\left|\textcolor{Black}{\dfrac{\partial f}{\partial x}(x,t)}\right|} \le \varphi(t) \in \mathscr{L}^1 & \text{\textbf{(Domination)}}
\end{array}
\right.
\end{array}
\right.
\end{equation}

Alors, la dérivée de $g$ est $g'(x) = \int_I \dfrac{\partial f}{\partial x}(x,t) \d t$
\end{itheorem}
\todo{Intégrale $\mathcal{C}^k$}
% section derivation_d_un_integrale_parametre (end)
\section{Intégrabilité (Ancienne version)}
À l'origine, on donne plusieurs définitions de l'intégrabilité : d'abord pour les fonctions positives, puis pour les autres en disant que c'est si $|f|$ est intégrable. On donne ici une définition plus générale : 
\begin{dfn}[Fonction intégrable]
$f(x)\in \mathcal{CM}$ est intégrable sur $I$ si \\
$\forall x \in \underbrace{J}_{\mathclap{\text{segment}}} \subset I, \exists M \in \mathbb{R}^+$ \textbf{tel que} $\int_J |f| \le M$
\end{dfn}
\needspace{5cm}
\begin{itheorem}[\gls{CNS} de l'intégration]
$f(x)$ est intégrable sur $I$ si \\
$\forall x \in I, \boxed{\int_I |f| \le \varphi}$ où $\varphi \in \mathscr{L}^1$
\end{itheorem}
\begin{theorem}
Si $f$ est une fonction intégrable sur $I$,
    \begin{equation}\label{eq:SommeIntegrale}
        \left| \int_I f \right| \le \int_I \left| f \right| 
    \end{equation}
\end{theorem}
\section{Intégrales classiques}
\begin{theorem}
Si $-\infty < a < b < +\infty$, alors $f:t\mapsto \dfrac{1}{(b-a)^\alpha}$ est intégrable sur $[a,b]$ si et seulement si $\boxed{\alpha < 1}$
\end{theorem}
\begin{theorem}[Intégrale de Riemann]
$f : t \mapsto \dfrac{1}{t^\alpha}$ est l'intégrale de Riemann,
$
\left\lbrace
\begin{array}{l c c l}
\int_0^1f(x)dx&\text{existe}&\Leftrightarrow&\alpha < 1\\[0.7cm]
\int_1^{+\infty}f(x)dx&\text{existe}&\Leftrightarrow&\alpha > 1
\end{array}
\right.
$
\end{theorem}
\section{Espaces vectoriels normés de fonction intégrables}
\begin{itheorem}[Convergence dominée\index{Convergence!dominee@dominée} ]
    Si $\big( f_n \big)_n$ est une suite de fonction de $\mathcal{CM}(I,K) $ convergeant simplement vers une fonction $f \in \mathcal{CM}(I,K)$, et s'il existe $\varphi \in \mathscr{L}^1 $ telle que \ibox{$\forall n, | f_n | \le \varphi$},\newline
    alors $f \in \mathscr{L}$ et $\int_I f = \lim_{n \to +\infty}, \int_I f_n$
\end{itheorem}
\begin{itheorem}[Intégration terme à terme]
    \begin{equation}
        \displaystyle
        \left.
        \begin{array}{c c l}
        \displaystyle\sum_{n=0}^{+\infty} & \underbrace{f_n} & \xrightarrow{\text{CVS}} f \\
                     & f_n \in \mathscr{L}^1 & \\
        \displaystyle\sum  \int_I \vphantom{\overbrace{f_n}} & \textcolor{couleurImp}{|}\overbrace{f_n}\textcolor{couleurImp}{|} & \text{converge}
        \end{array}
        \right\rbrace 
        \implies 
        \begin{array}{l}
        f \in \mathscr{L}^1 \\
        \sum f_n \text{ converge} \\
        \int_I f = \sum_{n=0}^{+\infty} \int_I f_n
        \end{array}
    \end{equation}
\end{itheorem}
\section{Fonction Gamma}
\begin{dfn}[Fonction Gamma]
On définit $\Gamma$\index{Gamma} de $]0,+\infty[$ par
$\Gamma : x \mapsto  \int_0^{+\infty}t^{x-1}e^{-t}dt$
\end{dfn}
Cette fonction est convexe (comme produit de deux applications $x \mapsto t^{x-1}$ et $x \mapsto e^{-t}$ convexes), donc continue.
\begin{itheorem}[Étude de $\Gamma$]
Pour tout $n\in\mathbb{N}$ on a :\\
 $\bullet \quad \Gamma{}(n+1)=n!$\\
 $\bullet \quad \Gamma \left( \dfrac{1}{2} \right) =\int_{0}^{+\infty}e^{-t}\dfrac{dt}{\sqrt{t}}=\cdots=\sqrt{\pi}$
\end{itheorem}
\begin{proof}
Vérifions que $\Gamma$ est une fonction continue. On utilise le théorème de continuité de fonctions paramétrés, $t \mapsto t^{x-1}e^{-t} $ est continue par morceaux, $x \mapsto t^{x-1}e^{-t} $ est continue. \\*
Pour dominer $t^{x-1}e^{-t}$, avec $x \in [a,b]$, on prend $\varphi (t)= e^{-t}(t^{a-1}+t^{b-1}) $
\par
Maintenant qu'on a étudié la continuité, on peut faire une intégration par partie de $\Gamma (n+1) = \int_0 ^\infty t^{n} e^{-t} dt$ en posant : 
\[
\begin{array}{l}
  \left\lbrace
		\begin{array}{r c l}
		u(t) = t^n &\implies & u'(t) = nt^{n-1}\\
		v(t) = -e^{-t} &\implies & v'(t) = e^{-t}
		\end{array}
  \right. \\
\implies \Gamma(n+1)=\underbrace{\left[ -t^n e^{-t} \right]_0^\infty}_{\mathclap{= -0 + 0}} - \underbrace{\int_0^\infty -t^{n-1}e^{-t}}_{\Gamma(n)}\\
\implies \Gamma(n+1)=n\Gamma(n)=n(n-1)\Gamma(n-1)=n!\underbrace{\Gamma(1)}_{=1}
\end{array}
\]
d'où $\Gamma(n+1)=n!$	CQFD.
\end{proof}
\section{Intégrales doubles}

\begin{dfn}[Intégrale double]
$f$ une fonction continue de $[\alpha{},\beta{}]\times [a,b]$ dans $\mathbb{C}$. \\
Alors $\displaystyle\int_{\alpha}^{\beta}\left(\displaystyle\int_{a}^{b}f(x,t)dt\right)dx=\displaystyle\int_{a}^{b}\left(\displaystyle\int_{\alpha}^{\beta}f(x,t)dx\right)dt$
\end{dfn}
\part{Équations Différentielles}
\chapter{Équations Différentielles Linéaires}
\begin{methode}
\titre{Résoudre une équation différentielle}
\begin{description}
    \item[Scalaire du 1\ier{} ordre]
                Méthode algorithmique, \textit{cf.} preuve \ref{ResolEquDiffScal1} page \pageref{ResolEquDiffScal1}
    \item[Vectorielles du 1\ier{} ordre] \hfill
    \begin{itemize}\itemsep2pt %TODO Terminer la méthode
                    \item Avec les coefficients constants
                    \item Avec une matrice Diagonalisable
                    \item Avec une matrice Trigonalisable
    \end{itemize}

    \item[Scalaire du second ordre]
\end{description}

\end{methode}
Dans ce chapitre, $I$ désigne un intervalle de $\mathbb{R}$, et $E$ est un $\mathbb{K}$-espace~vectoriel \emphhs{de dimension finie} $n$.
\section{Généralités} % (fold)
\label{sec:ED_generalites}
\subsection{Équation différentielle linéaire} % (fold)
\label{sub:equa_diff_lineaire}

\begin{dfn}[Équation différentielle linéaire d'ordre $1$]
     On appelle \emph{équation différentielle d'ordre $1$}\index{equation differentielle@Équation différentielle} l'équation~\eqref{eq:equa_diff_lineaire_ordre_1} :
     \begin{equation}\tag{\ensuremath{\mathcal{L}}}\label{eq:equa_diff_lineaire_ordre_1}
     x' = a(t)(x) +b(t)
     \end{equation}
     avec $a\in \mathcal{C}\big( I,\mathscr{L}(E)\big)$ et $b\in \mathcal{C}\big( I,E\big)$.\\
     L'application $x : I\to E$, dérivable, est \emph{solution} de l'équation~\eqref{eq:equa_diff_lineaire_ordre_1} si : 
     \begin{equation}
     \forall t\in I,\qquad x'(t) = a(t)\Big( x(t)\Big) +b(t)
     \end{equation}
\end{dfn}
\Attention{
$a$ est une application de $I$ dans \uline{$\mathscr{L}(F)$}. Donc $a(t)$ est une application linéaire, pas un scalaire}
%
\begin{dfn}[Forme matricielle]
     En choissisant une base de $E$, l'équation peut s'écrire matriciellement :
     \begin{equation}
     X'(t)=A(t)X(t)+B(t)
     \end{equation}
     avec $A\in \mathcal{C}\Big( I,M_n (\mathbb{R})\Big) $ et $B \in \mathcal{C}\big( I,\mathbb{R}^n\big) $
\end{dfn}
\begin{dfn}[Équation homogène associée]
     On appelle \emph{équation homogène}\index{equation homogene@Équation homogène} associée à l'équation~différentielle~\eqref{eq:equa_diff_lineaire_ordre_1} l'équation : 
     \begin{equation}\tag{\ensuremath{\mathcal{H}}}\label{eq:equa_diff_homogene_ordre_1}
     x'=a(t)(x)
     \end{equation}
\end{dfn}
\begin{theorem}[Principe de superposition]
     \begin{tabbing}
          Si \=$x_1$ est solution de \=$x'=a(t)~x+b_1(t)$\\
          et \>$x_2$ \rule{6em}{0.4pt} \>$x'=a(t)~x+b_2(t)$
     \end{tabbing}
     alors $x_1+x_2$ est solution de 
     \begin{equation}
     x' = a(t)(x) + b_1(t) + b_2(t)
     \end{equation}
\end{theorem}
% subsection equa_diff_lineaire (end)
\subsection{Problème de \textsc{Cauchy}} % (fold)
\label{sub:probleme_de_cauchy}
\begin{dfn}[Problème de \textsc{Cauchy}]
     On appelle \emph{problème de \textsc{Cauchy}}\index{Probleme de cauchy@Problème de \textsc{Cauchy}} pour l'équation différentielle~\eqref{eq:equa_diff_lineaire_ordre_1} l'ensemble constitué par \eqref{eq:equa_diff_lineaire_ordre_1} et un couple $(t_0,x_0)$ formant une condition initiale :
     \begin{equation}\label{eq:probleme_cauchy}
     \left\lbrace
     \begin{array}{rcl@{\qquad}c}
          x'(t) &=& a(t)\big( x(t)\big) +b(t)  &\eqref{eq:equa_diff_lineaire_ordre_1}\\
          x(t_0) &=& x_0                       &\text{(Condition initiale)}
     \end{array}
     \right.
     \end{equation}
\end{dfn}
\begin{theorem}[Mise sous forme intégrale]
     $x\in \mathcal{C}(I,F)$ est \emphh{solution du problème~de~\textsc{Cauchy}}~\eqref{eq:probleme_cauchy} \ssi{} :
     \begin{equation}
     x(t) = x_0 + \int_{t_0}^t \Big( a(u)\big(x(u)\big) + b(u)\Big) \d u
     \end{equation}
\end{theorem}
% subsection probleme_de_cauchy (end)
\subsection{Équation scalaire linéaire d'ordre $n$} % (fold)
\label{sub:equation_scalaire_lineaire_d_ordre_n}
\begin{dfn}[Équation scalaire linéaire]
     Avec les notations précédentes, une équation de la forme
     \begin{equation}
     x' = a(t)x + b(t)
     \end{equation}
     est scalaire si $n=\dim E = 1$.
\end{dfn}
\begin{dfn}[Équation différentielle linéaire d'ordre $n$]
On appelle \emph{équation différentielle d'ordre $n$}\index{equation differentielle@Équation Différentielle} l'équation~\eqref{eq:ED_lineaire_ordre_n} :
\begin{equation}\tag{\ensuremath{\mathscr{L}}}\label{eq:ED_lineaire_ordre_n}
a_0(t)y^{(n)}+a_1(t) y^{(n-1)}+\cdots + a_{n-1}(t) y' + a_n(t) y = b(t)
\end{equation}
$\left( \vphantom{3^3} a_0,\cdots ,a_n \right)$ est dans $\Big( \mathcal{C}(I,\Reel{})\Big)^{n+1} $. \\
Cette équation est équivalente à un système différentiel linéaire de la forme 
    $$ X'(t) = A(t)X(t) + B(t) $$ 
avec $A\in \mathcal{C}\Big( I,M_n (\mathbb{R})\Big) $ et $B \in \mathcal{C}\big( I,\mathbb{R}^n\big) $
\end{dfn}
% subsection equation_scalaire_lineaire_d_ordre_n (end)
% section ED_generalites (end)
\section{Solutions d'une équation différentielle linéaire} % (fold)
\label{sec:solutions_d_une_equation_differentielle_lineaire}
\begin{itheorem}[Théorème de \textsc{Cauchy-Lipshitz}\index{Cauchy-Lipshitz@\textsc{Cauchy-Lipshitz}} linéaire]\label{th:CauchyLipschitz}
Soit l'équation différentielle 
\begin{equation}\tag{$E$}
 x'(t) = a(t) x(t) + b(t)
\end{equation}
où $a \in \mathcal{C}\left( I,\mathscr{L}(F)\right)$, $b\in \mathcal{C}(I,F)$, alors
\begin{equation}
    \forall (t_0,x_0) \in (I,F), \exists ! \: \varphi \text{ telle que }
    \left|
    \begin{array}{l}
        \varphi \text{ soit solution de l'équation }(E)\\
        \varphi(t_0)=x_0
    \end{array}
    \right.
\end{equation}
\end{itheorem}
\todo{Cas des équations d'ordre $n$}% TODO: Cas des équations d'ordre $n$
% section solutions_d_une_equation_differentielle_lineaire (end)
\section{Exponentielle d'un endomorphisme, d'une matrice} % (fold)
\label{sec:exponentielle_d_un_endomorphisme_d_une_matrice}

% section exponentielle_d_un_endomorphisme_d_une_matrice (end)
\section{Systèmes différentiels linéaires homogènes à coefficients constants} % (fold)
\label{sec:systemes_differentiels_lineaires_homogenes_coefficients_constants}

% section systemes_differentiels_lineaires_homogenes_coefficients_constants (end)
\section{Méthode de variation des constantes} % (fold)
\label{sec:methode_de_variation_des_constantes}

% section methode_de_variation_des_constantes (end)
\section{Équations différentielles scalaires du second ordre} % (fold)
\label{sec:equations_differentielles_scalaires_du_second_ordre}

% section equations_differentielles_scalaires_du_second_ordre (end)
%%%%%%%
%%%%%%%%%%%%%%%%% Equations Différentielles %%%%% -> OLD <- %%%%%%%%%%%%%%%%%%%
%%%%%%%
\section{Équations Différentielles \emph{Scalaires} d'ordre 1}
\begin{theorem}[Solution de l'équation différentielle scalaire]\index{equation differentielle@Équation Différentielle!scalaire}
Si $y'=a(t) y + b(t)$, alors $S_I(\mathscr{L})$ est un sous-espace affine
\end{theorem}
\begin{proof}[Algorithmique]\label{ResolEquDiffScal1}
Par hypothèse, $a \in \mathcal{C}(I,\Reel{})$, donc $a(t)$ admet une primitive $P(t)=\int \limits_{t_0}^t a(s) \; ds$.
\[\arraycolsep=1.4pt\def\arraystretch{1}
\begin{array}{r c c l l c c}
\dfrac{\d}{\d t}\left( e^{-P(t)}\, y(t) \right) &=& -P'(t) & e^{-P(t)}y(t) &+& e^{-P(t)}& y'(t)\\
&=& \overbrace{-a(t)}&y(t)\, e^{-P(t)}&+& e^{-P(t)} &\left( \overbrace{\vphantom{{3^3}^3} a(t)\, y(t) +b(t)} \right)\\
&=& e^{-P(t)}b(t)\\
\end{array}\]
Si c'est intégrable, $\exists \; C$ tel que :
\[
\begin{array}{r c l}
e^{-P(t)}\, y(t) &=& \int \limits_{t_0}^t \left( \vphantom{{3^3}^3} e^{-P(u)}\, b(u) \: du + C \right)\\
y(t) &=& \int\limits_{t_0}^t \left( \vphantom{{3^3}^3} e^{-P(u)}\, b(u) \: du + C \right) e^{P(t)}
\end{array}
\] est solution de l'équation.
\end{proof}
%
\section{Équations Différentielles \emph{Vectorielles} d'ordre 1}
\subsubsection{Problème de \textsc{Cauchy}} % (fold)
\label{ssub:probleme_de_Cauchy}
\begin{dfn}[Équation Différentielle Vectorielle]\index{equation differentielle@Équation Différentielle!vectorielle}
    C'est une équation sous la forme 
\begin{equation}\label{equaDiffVect}\tag{$\mathscr{L}$}
    x'(t) = a(t) x(t) + b(t)
\end{equation}
où $a \in \mathcal{C}\left( I,\mathscr{L}(F)\right), \: b\in \mathcal{C}(I,F)$. \\Le \emph{Problème de \textsc{Cauchy}} \index{Probleme de cauchy@Problème de \textsc{Cauchy}} revient à trouver, pour tout $(t_0,x_0=x(t_0))$ dans $I \times F$, une solution $\varphi$ de \eqref{equaDiffVect}
\end{dfn}
\Attention{
$a$ est une application de $I$ dans \uline{$\mathscr{L}(F)$}. Donc $a(t)$ est une application linéaire, pas un scalaire}
% subsubsection probleme_de_Cauchy (end)
\subsubsection{Système fondamental}
\begin{dfn}[Système Fondamental]
Un \emph{système fondamental} de solutions\index{systeme@Système!fondamental de solution@fondamental de solutions} est une base dans l'espace $S_I(\textcolor{Red}{\mathcal{H}})$ des solutions.
\end{dfn}
%
\begin{prop}
    \begin{itemize}
        \item Si $(\varphi_1, \varphi_2, \cdots , \varphi_n)$ est une base de $S_I(\mathscr{L})$, alors, $\forall t \in I$, $\left( \varphi_1(t), \varphi_2(t), \cdots , \varphi_n(t) \right)
$ est une base dans $ F$
    \end{itemize}
\end{prop}
%
\begin{dfn}[Wronskien]
Le Wronskien\index{Wronskien} est le déterminant d'un système fondamental de solution. 
\begin{equation}\label{Wronskien}\tag{Wronskien}
  W(t) = \det_\mathcal{B}\left( \vphantom{{3^3}^3} \varphi_1(t),\cdots , \varphi_n(t) \right)
\end{equation}
\end{dfn}

\Attention{Le Wronskien est une fonction de $t$}

\begin{prop}
\begin{itemize}
    \item $W'(t)=\tr (a)\; W(t)$
    \item $ W(t) = W(t_0) \; e^{\int_{t_0}^t \tr (s) \; ds}$
\end{itemize}
\end{prop}

\begin{theorem}[Variation des constantes]
    \index{constante@\emphi{Variation des} Constantes}
    Soit $\left( \vphantom{{3^3}^3} \varphi_1(t),\cdots , \varphi_n(t) \right)$ une base de $S_I(\textcolor{Red}{\mathcal{H}})$. \\
    Alors, $\forall \varphi \in \mathcal{C}^{\textcolor{Red}{1}} (I,F), 
    \left\lbrace 
    \begin{array}{l}
        \text{Il existe une \uline{unique} famille } \overbrace{\left( \lambda_1, \cdots , \lambda_n \right)}^{\text{dans }\mathcal{C}^{\textcolor{Red}{1}} (I,F)} \text{ telle que } \varphi = \sum_{i=1}^n \lambda_i \, \varphi_i \\
        \varphi \in S_I(\mathcal{H}) \Leftrightarrow \boxed{\forall t \in I, \sum_{i=1}^n \lambda_i'(t) \, \varphi_i(t) = b(t)}
    \end{array}
    \right.
    $
\end{theorem}
Pour une équation à coefficients $a$ et $b$ constants $x' = ax + b(t)$, la solution générale est 
\begin{empheq}[box=\ibox]{equation}
y(t) = e^{(t-t_0)a}x_0 + \int \limits_{t_0}^t e^{(t-s)a}b(s) \: ds
\end{empheq}

\section{Équations Différentielles linéaires du second ordre}
\begin{dfn}[Équation Différentielle Vectorielle]\index{equation differentielle@Équation Différentielle!du second ordre@du 2\ieme{} ordre}
    C'est une équation sous la forme 
\begin{equation}\label{eq:equaDiff2ndOrdre}\tag{$\mathscr{L}$}
    y''(t) + a(t) y'(t) + a(t)y(t)= \gamma (t)
\end{equation}
\hfill \\[0.5\baselineskip]
    L'équation homogène est
\begin{equation}\label{eq:equaDiff2ndOrdreHomogene}\tag{$\mathcal{H}$}
    y''(t) + a(t) y'(t) + b(t)y(t)= 0
\end{equation}
On note $f(r) = r^2 + a\times r + b$ %TODO: Créer un lien entre les constantes via TIKZ 
son polynôme caractéristique\index{polynome@Polynôme!caracteristique equa diff@caractéristique\emphi{ (Équation différentielle)}}
\end{dfn}
\subsection{Coefficients constants}
\begin{itheorem}[Résolution de l'équation]
Dans le cas de l'équation homogène \eqref{eq:equaDiff2ndOrdreHomogene}, on calcule le discriminant $\Delta$ du polynôme caractéristique. Suivant les cas, on a la solution $y(t)$ pour \uline{l'équation homogène} : 
\[
\begin{array}{r | l}
\Delta \neq 0 & y(t) = A e^{r_1t}+B e^{r_2t}\\
\hline
\Delta = 0 & y(t) = (A+Bt)e^{rt}
\end{array}
\]
Ou encore : 
\[
\begin{array}{r | l l}
\Delta > 0 & r_{\pm}=\alpha \pm \beta &y(t) = e^{\alpha t}\left( A\cdot \cosh(\beta t) + B\cdot \sinh (\beta t) \right)
\\
\hline
\Delta < 0 & r_{\pm}=\alpha \pm i\beta &y(t) = e^{\alpha t}\left( A\cdot \cos(\beta t) + B\cdot \sin (\beta t) \right)
\\
\hline
\Delta = 0 & r \text{ double}& y(t) = (A+Bt)e^{rt}
\end{array}
\]
\end{itheorem}
\begin{theorem}
    Si dans \eqref{eq:equaDiff2ndOrdre}, $\gamma (t)= P(t)e^{\lambda t}$, $P\in \Cmplx [X]$, alors on peut donner une solution :
    \begin{equation}
    t \mapsto t^{\omega(\lambda)}\, Q(t)e^{\lambda t}
    \end{equation}
    où $\omega(\lambda )$ est la multiplicité de $\lambda$ en tant que racine du polynome caractéristique de $f$ et $Q \in \Cmplx[X]$ est de même degré que $P$.
\end{theorem}
\subsection{Cas général}
\begin{itheorem}[Théorème de \textsc{Cauchy-Lipshitz}\index{Cauchy-Lipshitz@\textsc{Cauchy-Lipshitz}} linéaire]
Soit l'équation différentielle 
\begin{equation}\tag{$\mathscr{L}$}
    y''(t) + a(t) y'(t) + a(t)y(t)= \gamma (t)
\end{equation}
où $a \in \mathcal{C}\left( I,\mathscr{L}(F)\right)$, $b\in \mathcal{C}(I,F)$, alors
\[
    \forall \big( t_0,(x_0,x_0') \big) \in (I,\mathbb{K}^2), \exists ! \: \varphi \text{ telle que }
    \left|
    \begin{array}{l}
        \varphi \text{ soit solution de l'équation }(E)\\
        \varphi(t_0)=x_0 \\
        \varphi'(t_0) = x'_0
    \end{array}
    \right.
\]
\end{itheorem}
\begin{proof} \hfill \\
    Le théorème est une conséquence du théorème~\ref{th:CauchyLipschitz} si on résout plutôt $\begin{pmatrix} x \\ x' \end{pmatrix}' = \begin{pmatrix} a \\ a' \end{pmatrix}(t)\begin{pmatrix} x \\ x' \end{pmatrix} + \begin{pmatrix} b \\ b' \end{pmatrix}$ 
\end{proof}
\begin{dfn}[Wronskien]
    Si $u$ et $v$ sont des $I$-solutions, le \emph{Wronskien}\index{Wronskien} est l'application définie par
    \begin{equation}
    W = uv' - u'v \tag{Wronskien}
    \end{equation}
\end{dfn}
\begin{prop}
Dans l'équation $(\mathcal{H}) : x'' + a(t)x' + b(t) x =0$,
\begin{itemize}
    \item $W + a W = 0$
    \item $(u,v)\text{ libre }\Leftrightarrow \exists t_0 \text{ tel que } W(t_0) \neq 0 \Leftrightarrow \forall t, W(t) \neq 0$
\end{itemize}
\end{prop}
\begin{itheorem}[Méthode de variation des constantes]
    En connaissant $(u,v)$ un système fondamental de solutions, on cherche une solution de la forme $y(t) = c_1(t) u(t) + c_2(t) u(t)$. On détermine $c_1$ et $c_2$ avec :
    \begin{equation}
    c_1'\begin{pmatrix} u \\ u' \end{pmatrix} + c_2'\begin{pmatrix} v \\ v' \end{pmatrix} = \begin{pmatrix} 0 \\ \gamma \end{pmatrix}
    \end{equation}
\end{itheorem}
\chapter{Équations Différentielles non linéaires}
\section{Équations autonomes}
\begin{dfn}[Champ de Vecteur]
On appelle \emph{champ de vecteurs}\index{vecteur@\emphi{Champ de} Vecteur} l'application qui à un point $M \left( \begin{matrix} x\\ y \end{matrix} \right)$ associe $\overrightarrow{V(M)} \left( \begin{matrix} f(x,y) \\ g(x,y) \end{matrix} \right)$ : 
\[
\begin{array}{r c l}
U\in \Reel{}^2 &\xrightarrow{\mathcal{C}^1} &\Reel{}^2\\
M\begin{pmatrix} x \\ y \end{pmatrix} & \mapsto &\overrightarrow{V(M)} \left( \begin{matrix} f(x,y) \\ g(x,y) \end{matrix} \right)
\end{array}
\]
\end{dfn}

\begin{dfn}[Système Autonome]
On appelle \emph{système autonome}\index{systeme@Système!autonome} associé au champ de vecteur $\overrightarrow{V(M)}$ le système différentiel 
\begin{empheq}[box=\ibox]{equation}
    \dfrac{\d M}{\d t}=\overrightarrow{V(M)}
\end{empheq}
\end{dfn}
Le mot \textit{autonome} témoigne de la non-dépendance en $t$ du champ de vecteur $\overrightarrow{V(M)}$
%
\begin{theorem}[\textsc{Cauchy-Lipshitz}\index{Cauchy-Lipshitz@\textsc{Cauchy-Lipshitz}} (\textit{admis})]
Avec les données précédentes, pour tout couple $\big( t_0, (x_0,y_0)\big) \in \left( I\times U \right)$, il existe une unique $I$-solution \uline{maximale} $\varphi : t \mapsto M(t) = \begin{pmatrix} x(t) \\ y(t) \end{pmatrix}$, telle que $\left| \begin{array}{l c r} x(t_0) &=& x_0 \\ y(t_0) &=& y_0 \end{array} \right. $
\end{theorem}

Une \emph{solution maximale}\index{Solution!maximale} est une solution qui n'est la restriction d'aucune autre. Son intervalle de définition est l'intervalle maximal.

\section{Équations non autonomes}
Dans cette section on appelle \emph{équation différentielle} : 
\begin{equation}\label{equaDiffNA}
x' = f(t,x)\tag{$\mathcal{E}$}
\end{equation}
où $f$ est une fonction de $\mathcal{C}^1(U,\Reel)$
\begin{theorem}[\textsc{Cauchy-Lipshitz}\index{Cauchy-Lipshitz@\textsc{Cauchy-Lipshitz}} (\textit{admis})]
$U$ un intervalle ouvert de $\Reel{}^2$, et en reprenant l'équation \eqref{equaDiffNA} : 
\[
    \forall (t_0,x_0) \in U, \exists ! \: \varphi \text{ telle que }
    \left|
    \begin{array}{l}
        \varphi \text{ soit \emph{solution maximale} de l'équation \eqref{equaDiffNA}}\\
        \varphi(t_0)=x_0
    \end{array}
    \right.
\]
\end{theorem}
%
%
\chapter{Calcul différentiel} % (fold)
\label{cha:calcul_differentiel}
% Je sais que le chapitre "Fonctions de plusieurs variables" est le même, 
% mais j'ai envie de tout refaire dans ce chapitre.
Dans ce chapitre, on se place dans $\left( E, N \right)$ et $\left( F,P \right)$, des espaces vectoriels normés de dimension finie. $U$ est un ouvert de $E$
\[
    \begin{array}{r c r c l}
    f&:& U &\to & F \\
     & & a &\mapsto & f(a)
    \end{array}
\]
\section{Dérivée selon un vecteur, dérivées partielles} % (fold)
\label{sec:derivee_selon_un_vecteur_derivees_partielles}
\begin{dfn}[Dérivée selon un vecteur]
     Soit $f:U\to F$ la fonction définie précédemment.\newline
    Soit $v$ un vecteur non nul de $E$, et $t$ un réel tel que $a+tv \in U$. \\
    On dit que $f$ \emph{admet une dérivée selon le vecteur $v$ au point $a$} (ou admet une \emph{dérivée directionnelle}) si la fonction réelle $t\mapsto f(a+tv)$ est dérivable en $0$. Cette dérivée en $a$ est notée $D_vf(a)$. \newline
    On a alors : 
    \begin{equation}
    D_vf(a)=\lim_{\substack{t\rightarrow 0\\ t\neq 0}}\dfrac{f(a+\textcolor{Red}{t}v) - f(a)}{t}
    \end{equation}
\end{dfn}
\begin{dfn}[Dérivées partielles dans une base]
     Soit $f:U\to F$ la fonction définie précédemment.\newline
     Soit $(e_1, \cdots ,e_n)$ une base de $E$. On appelle \emph{dérivée partielle}\index{Derivee@Dérivée!partielle} de $f$ dans $(e_n)_n$ ses dérivées par rapport aux vecteurs. La $i$-ième dérivée partielle en $a$ est notée $D_if(a)$ ou $\dfrac{\partial f}{\partial x_i}(a)$. Ainsi : 
     \begin{equation}
     \dfrac{\partial f}{\partial x_i}(a)=D_if(a)=\lim_{\substack{t\rightarrow 0\\ t\neq 0}}\dfrac{f(a+\textcolor{Red}{t}e_i) - f(a)}{t}
     \end{equation}
\end{dfn}
% section derivee_selon_un_vecteur_derivees_partielles (end)
\needspace{3cm}
\section{Différentielle} % (fold)
\label{sec:differentielle}
\subsection{Application différentiable} % (fold)
\label{sub:application_differentiable}
\begin{dfn}[Application différentiable au point $a$]
    Soit $f:U\to F$ la fonction définie précédemment.\newline
    On dit que $f$ est \emph{différentiable}\index{Differentiable@Différentiable} au point $a$ s'il existe une fonction linéaire $\d f(a) : E \to F$ telle que :
    \begin{equation}
    \forall h \in E, \ f(a + h) = f(a) + \d f(a)(h) + o(h)
    \end{equation}
\end{dfn}
\Attention{$\d f(a) \in \mathscr{L}(E,F)$ est une application linéaire de $E$ dans $F$. On l'applique sur un vecteur $h$ en notant $\d f(a)(h)$ ou $\d f(a)\cdot h$}
\begin{theorem}[$f$ dérivable en $a$]
     Soit $f$ la fonction définie précédemment. \newline
     Si $f$ est différentiable en $a$, alors $f$ est continue en $a$ et est dérivable en $a$ selon tout vecteur. Sa dérivée selon le vecteur $v$ est alors : 
     \begin{equation}
     D_v f(a) = \d f(a)(v)
     \end{equation}
\end{theorem}
% subsection application_differentiable (end)
\subsection{Jacobien, Jacobienne} % (fold)
\label{sub:jacobien_jacobienne}
% TODO: À terminer
\begin{dfn}[Jacobienne, Jacobien]
  Soient $(e_i)_i$ et $(e'_i)_i$ deux bases de respectivement $E$ et $F$.\newline
  Soit $f=(f_j)_j$ la fontion définie précédemment telle que $f(x_1,\cdots,x_n) = \sum_i f_i (x_i)\cdot e'_i$. 
On définit la \emph{Jacobienne}\index{Jacobien@Jacobien(ne)} $\mathcal{J}_a(f)$ comme la matrice de terme général $j_{i,j}=\dfrac{\partial f_i}{\partial x_j}=D_jf_i(a)$. \newline
Le \emph{Jacobien} est le déterminant de cette matrice.
\end{dfn}

\textbf{Exemple :} La Jacobienne de la fonction polaire (qui à $(r,\theta )$ associe $(r\cdot \cos{\theta} , r\cdot \sin{\theta} )$) est $\begin{pmatrix}
\cos{\theta} & -r\sin{\theta} \\
\sin{\theta} & r\cos{\theta}
\end{pmatrix}$. Son Jacobien est donc $r\left( \sin^2{\theta} + \cos^2{\theta} \right)=r$

% subsection jacobien_jacobienne (end)
% section differentielle (end)
\section{Opérations sur les applications différentiables} % (fold)
\label{sec:operations_sur_les_applications_differentiables}
\begin{itheorem}[Différentielle d'une composée d'applications]
Soient $E$,$F$ et $G$ trois espaces vectoriels \emphh{normés} \emphh{réels} de \emphhs{dimension finie}.\newline
Soient $f$ et $g$ deux fonctions telles que :
\[
    \begin{array}{r c l c l c l}
    f &:& E &\to& F \\
    g &:& & & F & \to & G 
    \end{array}
\]
Alors, la fonction $g \circ f$ est différentiable sur un ouvert de $E$ et :
     \begin{equation}
     \d (g\circ f)(a) = \d g\big( f(a)\big) \circ \d f(a)
     \end{equation}
     
\end{itheorem}
% section operations_sur_les_applications_differentiables (end)
\section{Cas des applications numériques} % (fold)
\label{sec:cas_des_applications_numeriques}
\subsection{Gradient} % (fold)
\label{sub:gradient}
\begin{dfn}[Gradient]
     Soit $E$ un espace euclidien, et $f$ une application allant de $U$ un ouvert de $E$ à $\Reel{}$. \\
     On appelle \emph{gradient}\index{Gradient} de $f$ en $a$ l'unique vecteur de $E$, noté $\Grad f(a)$, tel que :
     \begin{equation}
     \forall v\in E,\qquad \d f(a)\cdot v = \left( \Grad f(a) | u \right) 
     \end{equation}
\end{dfn}
\todo{Expression dans une base orthonormée}
\textbf{Remarque :} Parfois, le gradient de $f$ en $a$, $\Grad f(a)$, est aussi noté $\nabla f(a)$
% subsection gradient (end)
\subsection{Représentation des formes linéaires} % (fold)
\label{sub:representation_des_formes_lineaires}
\begin{dfn}[Dual d'un \gls{ev} réel] % D'après Vuibert
     Soit $E$ un espace euclidien. \\
     On appelle \emph{dual}\index{Dual} de $E$, et on note $\mathscr{L}(E,\Reel )$ ou $E^*$, l'ensemble des formes linéaires de $E$ dans $\Reel{}$.
\end{dfn}
\begin{theorem}[Représentation des formes linéaires]
     Soit $E$ un espace euclidien.\\
     Pour toute forme linéaire $f$ de $\mathscr{L}(E,\Reel )$ : 
     \begin{equation}
     \exists ! y,\, \forall x\in E,\qquad f(x) = (x|y)
     \end{equation}
\end{theorem}
\begin{proof}
     Pour tout $y \in E$, on peut associer l'application $\theta_y : x\mapsto (x|y)$. Cette application est linéaire. L'application \[ \left| \begin{array}{r c l c l} \theta &:& E &\to & \mathscr{L}(E,\mathbb{R})\\ && y &\mapsto& \theta_y \end{array}\right.\] est également linéaire. Son noyau $\mathrm{Ker}(\theta )$ étant nul, $\theta$ est injective. Mais $E$ et $\mathscr{L}(E,\mathbb{R})$ étant de même dimension, elle est donc \emphh{bijective}.\\
     Toute forme linéaire a donc un antécédant par $\theta$.\qed
\end{proof}
% subsection representation_des_formes_lineaires (end)
\subsection{Point critique} % (fold)
\label{sub:point_critique}
\begin{dfn}[Point critique]
     On appelle \emph{point critique}\index{Point critique} d'une application différentiable $f : E \to \Reel{}$ tout point de $E$ tel que 
     \begin{equation}
     \d f(a) = 0
     \end{equation}
\end{dfn}
\begin{dfn}[Extremum local]
     Soit $E$ un espace euclidien, et soit $f : E \to \Reel{}$. \\
     On dit que $f$ présente un \emph{extremum local}\index{Extremum!local} en un point $a$ si pour tout voisinage de $a$ : 
     \begin{subequations}
          \begin{align}
               \forall x\in V,\, & f(x) \le f(a) & \text{(Maximum local)}\\
               \forall x\in V,\, & f(x) \ge f(a) & \text{(Minimum local)}
          \end{align}
     \end{subequations}
\end{dfn}
\begin{itheorem}[Condition nécessaire d'éxistence d'extremum]
     Soit $E$ un espace euclidien, et soit $f : E \to \Reel{}$ une \emphh{fonction différentiable}. \\
     Si $f$ présente un extremum local en un point $a$, alors $\d f(a) = 0$.
\end{itheorem}
\Attention{La réciproque de ce théorème est fausse}
% subsection point_critique (end)
% section cas_des_applications_numeriques (end)
\section{Vecteurs tangents à une partie d'un espace normé de dimension finie} % (fold)
\label{sec:vecteurs_tangents_une_partie_d_un_espace_norme_de_dimension_finie}

% section vecteurs_tangents_une_partie_d_un_espace_norme_de_dimension_finie (end)
\section{Applications de classe $\mathcal{C}^1$} % (fold)
\label{sec:applications_de_classe_C1}

% section applications_de_classe_C1 (end)
\section{Applications de classe $\mathcal{C}^k$} % (fold)
\label{sec:applications_de_classe_C_k}

% section applications_de_classe_C_k (end)
% chapter calcul_differentiel (end)
\chapter{Fonctions de plusieurs variables}
\begin{methode}
\titre{Étude d'une fonction à deux variables} \newline
Passer en polaire
\end{methode}


Dans ce chapitre, on se place dans $\left( E, N \right)$ et $\left( F,P \right)$, des espaces vectoriels normés de dimension finie. $U$ est un ouvert de $E$
\[
    \begin{array}{r c r c l}
    f&:& U &\to & F \\
     & & a &\mapsto & f(a)
    \end{array}
\]

\section{Différentielle, dérivée}
\subsection{Différentielle}
\begin{dfn}[Différentielle]
Il existe \uline{au plus} un élément $\varphi$ de $\mathscr{L}(E,F)$ tel que
\[
    f(a+h) \underset{h\to 0}{=} f(a) + \varphi(h) + o(h)
\]
$\varphi$ est appelée la \emph{différentielle}\index{differentielle@Différentielle} de $f$. On la note $df(a)$
\end{dfn}

\textbf{Remarque :} $a$ et $h$ sont des \uline{vecteurs}. Donc sous la forme 
$a = \begin{pmatrix}
a_1 \\
a_2 \\
\vdots \\
a_n
\end{pmatrix}
$. De plus, $\varphi (h)$ est une application linéaire : $\varphi(h) \in \mathscr{L}\big( E,F\big)$

\subsection{Dérivée selon un vecteur}
\begin{dfn}[Dérivée en un point]
On note $\varphi_h : t \mapsto f(a+th)$. \\
$f$ admet une dérivée en $a$ selon $h$ si \uline{$\varphi_h$ est dérivable en $0$}. \\
Alors, on note cette dérivée $D_hf(a)=\varphi_h'$. Si elle existe : 
\begin{empheq}[box=\ibox]{equation}
D_hf(a)=\lim_{\substack{t\rightarrow 0\\ t\neq 0}}\dfrac{f(a+\textcolor{Red}{t}h) - f(a)}{t}
\end{empheq}
\end{dfn}
On a alors la dérivée pour tout $a$ définie par la fonction $D_hf : a \mapsto D_hf(a)$

\begin{dfn}[Application  de classe $\mathcal{C}^1$]
$f$ est de classe $\mathcal{C}^1$ sur $U$ si $\forall j \in [1,n], D_jf$ existe et est continue sur $U$
\end{dfn}


\begin{dfn}[$\mathcal{C}^k$-difféomorphisme]
$f$ (bijective) est un \emph{$\mathcal{C}^k$-difféomorphisme}\index{ck diffeomorphisme@$\mathcal{C}^k$-difféomorphisme} si elle et son inverse sont $\mathcal{C}^k$. C'est-à-dire : 
\[
\left\lbrace
\begin{array}{l}
f \in \mathcal{C}^1\left( U,V \right) \\
f^{-1} \in \mathcal{C}^1\left( V,U \right)
\end{array}
\right.
\]
\end{dfn}

\section{Inversion locale}
\begin{itheorem}[Théorème d'inversion locale \textit{(admis)}]
\[
\begin{array}{c}
f \in \mathcal{C}(U,F)\text{ injective est }\mathcal{C}^k\text{-difféomorphisme } \\ 
    \Leftrightarrow \\
 \forall a \in U, df(a) \text{ isomorphisme de }E\text{ dans }F
\end{array}
\]
\end{itheorem}

\section{Complément sur les courbes planes} % (fold)
\label{sec:complement_sur_les_courbes_planes}
\begin{theorem}[Formule de \textsc{Green}-\textsc{Riemann}]
	 Un compact $D$ délimitée par un une courbe plane $\Gamma$ positivemment orientée et $\CM^1$. Soient $P$ et $Q$ deux fonctions de classe $\mathcal{C}^1$ sur un ouvert dans lequel $\Gamma$ est tracé. On admet la formule de \textsc{Green}-\textsc{Riemann}\index{Green-Riemann@\textsc{Green}-\textsc{Riemann}} : 
	 \begin{equation}
	 \iint \limits_D \left[ \dfrac{\partial Q}{\partial x} \textcolor{couleurImp}{-} \dfrac{\partial P}{\partial y}\right](x,y) \, \d x\, \d y = \int \limits_\Gamma P\d x + Q \d y
	 \end{equation}
\end{theorem}
% section complement_sur_les_courbes_planes (end)

\part{Géométrie} 
\section{Arcs Paramétrés}
\begin{dfn}[Arc Paramétré]
On appelle \emph{arc paramétré} \index{arc parametre@Arc paramétré} de classe $\mathcal{C}^k$ un couple $\left( I,f \right)$ avec 
$
\left\lbrace
\begin{array}{c l}
I&\text{un intervalle de \Reel}\\
f&\text{une application de }\mathcal{C}^k\left( I,E \right)
\end{array}
\right.
$
\end{dfn}

\begin{dfn}
Quelques autres définitions : \\[0.3cm]
% Tableau de Définitions
\begin{tabular}{ >{\begin{bf}} l <{\end{bf}} >{\begin{math}} c <{\end{math}} @{ — } l}
Valeur Régulière\index{Valeur!reguliere@régulière} & t_0 &$f'(t_0) \neq 0$\\
Valeur Birégulière \index{Valeur!bireguliere@birégulière}&t_0&$\left( f'(t_0),f''(t_0) \right)$ est libre\\
Abscisse Curviligne \index{Abscisse curviligne}&s&$s'=N_2\left( f'(t) \right)$ sur un intervalle\\
Paramétrage normal &\big( J,g \big)&%TODO: Finir le paramétrage normal
\end{tabular}
\end{dfn}

Exemple d'abscisse curviligne : $s : t \mapsto \sinh (t)$ car $N_2\left( s(t) \right) = \sqrt{\int |\sinh (t)|^2} = 
 \sinh{}'(t) = \cosh (t)$. L'avantage d'une abscisse curvligne est de pouvoir simplifier l'étude d'une courbe. \\
%TODO: Exemple de paramétrage régulier
%Exemple de paramétrage régulier : $\bigg( ,ch(x) \bigg)$

\section{Courbes Planes}
\subsection{En polaire}
\begin{dfn}[Fonction $\arg$]
\ibox{$\theta \mapsto e^{i\theta}$ est une bijection de $]-\pi , \pi [$ sur $\mathbb{U}\backslash \lbrace -1 \rbrace$} où $\mathbb{U}=\lbrace z \in \Cmplx | |z| = 1 \rbrace $\\
Sa réciproque est l'application $u \mapsto \arg(u)$\\
Si on prend $u \in \mathbb{U}$, en notant $u=x+iy$, alors $\arg(u)=2\arctan \left( \dfrac{y}{x+1} \right)
$
\end{dfn}

\begin{itheorem}[Théorème du Relèvement]
Soit $f \in \mathcal{C}^n\left( I,\mathbb{U} \right)$ (avec $n \neq 0$). \\
$\exists \: \theta \in \mathcal{C}^n\left( I,\Reel \right)$ tel que $f(t) = e^{i\theta{}(t)}$. $\theta$ est appelé \emph{relèvement}\index{relevement@Relèvement} de $f$.
\end{itheorem}
%TODO: Finir la preuve (Prouver l'existence)
\begin{proof}
Si elle existe, $\theta$ n'est pas unique ($t \mapsto \theta(t) + 2\pi$ convient aussi).\\
Donc $f'(t) = i\theta{}' f(t) \Leftrightarrow \theta{}'(t)=-i\dfrac{f'(t)}{f(t)}$. \\
On peut alors intégrer : $\theta (t) = C - i \int \limits_{t_0}^t \dfrac{f'(u)}{f(u)}\; du$, et il ne reste plus qu'à prouver l'existence en ayant cette expression de $\theta{}(t)$
\end{proof}

\begin{dfn}[Tangente\index{Tangente}]
Si $\theta$ est une valeur régulière, on note $V$ l'angle $\left( \overrightarrow{u_{\theta}},\dfrac{\overrightarrow{\d M_\theta}}{\d\theta}  \right)$, et on définit \ibox{$\tan(V) = \dfrac{\rho (\theta)}{\rho '(\theta)}$}
\end{dfn}
\subsection{Étude d'une courbe paramétrée} % (fold)
\label{sub:etude_courbe_parametree}
\begin{methode}
\titre{Étude de courbes paramétrées}
Quelques conseils :
\begin{itemize}
	\item On essaye, si possible, de \emphh{passer en polaire}
	\item Ne pas oublier de vérifier les \emphh{ensembles de définition}
	\item Lors de l'étude \emph{au voisinage d'un point}, il suffit d'étudier les dérivées successives grâce à un \uline{développement limité}
\end{itemize}
\end{methode}
On a l'expression de $x$ et $y$ en fonction de $t$ : $x = f(t)$ et $y = g(t)$. Pour étudier la courbe : 
\begin{enumerate}
    \item Ensemble de \emph{définition} $\mathcal{D} = \mathcal{D}_f \bigcup \mathcal{D}_g$
    \item Étude des \emph{variations} : on étudie $x'$, $y'$ et $\dfrac{y'}{x'}$
    \item \emph{Branches infinies} 
    	\begin{itemize}
    		\item $x = \lim f$ ou $y = \lim g$ sont des asymptotes (avec $\lim f$ et $\lim g$ des limites finies)
    		\item Si, pour $t_0 \in \Reel{}$ les deux fonctions tendent \uline{simultanément} vers l'infini, on \emphh{étudie $\lim_{t \to t_0} \frac{y}{x}$}
    		\hfill \\
    		\begin{center}
    		\begin{tikzpicture}
    			 \draw[->, >=latex, color=couleurNoirClair] (-4,0) -- (4,0) node[below right]{$x$}; 
    			 \draw[->, >=latex, color=couleurNoirClair] (0,-2) -- (0,2) node[left]{$y$};
    			 \begin{scope}
    			 \clip (-4,-2) -- (-4,2) -- (4,2) -- (4,-2) ;
    			 \draw[thick] (0,1) -- ++(4,0) -- ++(-8,0) node[near end, below]{$\frac{y}{x} \xrightarrow[t \to t_0]{} 0$};
    			 \draw[thick] (3,0) -- ++(0,4) -- ++(0,-8) node[near end, above left]{$\frac{y}{x} \xrightarrow[t \to t_0]{} \infty$} ;
    			 \draw[color=couleurFonce!80!White, thick] (0,0.5) -- ++(20:8) -- ++(20:-16) (-2,-1.5) node{$\frac{y}{x} \xrightarrow[t \to t_0]{} a \in \Reel{}$};
    			 \end{scope}
    		\end{tikzpicture}
    		\captionof{figure}{Différents types d'asymptotes en fonction de $\frac{y}{x}$}
			\end{center}
    		\hfill \\
    		Dans le cas où $\frac{y}{x} \xrightarrow[t \to t_0]{} a \in \Reel{}$, on peut déterminer $b$ de l'équation $y = ax + b$ en examinant $y - ax$
    	\end{itemize}
%TODO Terminer étude des courbes polaires
\end{enumerate}
% subsection etude_courbe_parametree (end)
%\chapter{Les Quadriques}
%\chapter{Géométrie affine}
\part{Annexe}
\hfill \\
\rule{\columnwidth}{0.5pt}
\section{Équivalences}
Pour une définition de l'équivalence, \textit{cf.} définition \ref{equivalence} page \pageref{equivalence}
\begin{description}
\item[Formule de \textsc{Stirling}]\index{Stirling@\textsc{Stirling}} $\boxed{n!=\left(\dfrac{n}{e}\right)^{n}\sqrt{2\pi n}}$
\item[Équivalence de $\boldsymbol{\ln}$] $\dfrac{\ln(u)}{t^{\alpha}}\sim t^{\frac{\alpha+1}{2}}$
\item[Équivalents usuels en 0] \[
\arraycolsep=3pt\def\arraystretch{2.4}
\begin{array}{|r c l|r c r|r c l|}
\hline
\sin(u)& \underset{0}{\sim}& u & \cos(u)\textcolor{couleurFonce}{-1} &\underset{0}{\sim} & \textcolor{Red}{-} \dfrac{u^2}{2} & \ln(\emph{1+}u) &\underset{0}{\sim}& u \\
\hline
\sinh(u) &\underset{0}{\sim} &u & \cosh(u)\textcolor{couleurFonce}{-1} &\underset{0}{\sim} &\dfrac{u^2}{2} & e^u \textcolor{couleurFonce}{-1} &\underset{0}{\sim} & u \\
\hline
\end{array}
\]
\end{description}


\rule{\columnwidth}{0.5pt}
\section{Trigonométrie}
\subsection{Définition}
\begin{minipage}{0.45\textwidth}
\begin{equation}\label{def:cos}
\left\lbrace
\begin{array}{r c l}
\cos{x}&=&\dfrac{e^{ix}+e^{-ix}}{2}\\
\sin{x}&=&\dfrac{e^{ix}-e^{-ix}}{2i}\\
\end{array}
\right.
\end{equation}
\end{minipage}\hspace{0.1\textwidth}
\begin{minipage}{0.45\textwidth}
\begin{equation}\label{def:cosh}
\left\lbrace
\begin{array}{r c l}
\cosh{x}&=&\dfrac{e^{x}+e^{-x}}{2}\\
\sinh{x}&=&\dfrac{e^{x}-e^{-x}}{2}\\
\end{array}
\right.
\end{equation}
\end{minipage}
%
\subsection{Addition / Produit}
\begin{equation}\label{trigo}
\begin{array}{r@{\, = \,}l @{\qquad}|@{\qquad} r@{\, = \,}l}
\cos (a+b) & \cos a \cos b - \sin a \sin b 
    & \cos (a+b) + \cos (a-b) & 2 \cos a \cos b \\
\cos (a-b) & \cos a \cos b + \sin a \sin b 
    & \cos (a+b) - \cos (a-b) & 2 \sin a \sin b \\
\sin (a+b) & \sin a \cos b + \cos a \sin b 
    & \sin (a+b) + \sin (a-b) & 2 \sin a \cos b \\
\sin (a-b) & \sin a \cos b - \cos a \sin b 
    & \sin (a+b) - \sin (a-b) & 2 \cos a \sin b
\end{array}
\end{equation}
\subsection{Dérivation} % (fold)
\label{sub:derivation_trigo}
Dérivées des fonctions trigonométriques : 
\begin{align*}
	 \tan' x 	&= \dfrac{1}{1 + \cos^2 x} = 1 + \tan^2 x \\
	 \arcsin' x &= \dfrac{1}{\sqrt{1 - x^2}} \\
	 \arccos' x &= -\dfrac{1}{\sqrt{1-x^2}} \\
	 \arctan' x &= \dfrac{1}{1+x^2}
\end{align*}
% subsection derivation_trigo (end)
\subsection{Formule de \textsc{Moivre}} % (fold)
\label{sub:Moivre}
\begin{equation}
(\cos x + i \sin x)^n = \cos (nx) + i\sin (nx)
\end{equation}
% subsection Moivre (end)
\rule{\columnwidth}{0.5pt}
\section*{Généralités}
\begin{description}
\item[Conjugué] $z = -\overline{z} \Leftrightarrow z \in \Reel$
\item[Convexité] La fonction \textbf{exponentielle} est \textbf{convexe}, la fonction \textbf{logarithme} est \textbf{concave}
\end{description}
\needspace{5cm}
\rule{\columnwidth}{0.5pt}
\section*{Inégalités}
\begin{tabular}{ >{ \begin{bf} } r <{\end{bf}} c c}
    Modules & $\left|\left|\sum_i x_i \right| \right| \le \sum_i \| x_i \|$& \\[5mm]
    Module d'intégrales & $\left| \int_I f \right| \le \int_I \left| f \right| $ & \eqref{eq:SommeIntegrale} \\[3mm]
    Inégalité de la Moyenne & $m(b-a)\le \int_a^b f(x) \mathrm{d}x \le M(b-a)$ & \eqref{eq:inegalite_de_la_moyenne}
\end{tabular} \\[3mm]
\rule{\columnwidth}{0.5pt}
\section{Formules usuelles} % (fold)
\label{sec:formules_usuelles}
\begin{tabular}{c@{$\; = \;$}l@{$\qquad$} | @{$\qquad$}l@{$\; = \;$}l}
    $a^k - b^k$&$(a-b) \left( \sum_{p=0}^{k\textcolor{couleurImp}{-1}} a^p b^{k-1-p} \right) $ &
    $\sum_{k=1}^n k$    & $\dfrac{n(n+1)}{2}$ \\[6mm]
    $\sum_{k=1}^n k^2$  & $\dfrac{n(n+1)(2n+1)}{6}$ &
    $\sum_{k=1}^n k^3$  & $\dfrac{n^2(n+1)^2}{4}$ \\[3mm]
\end{tabular} \\[6mm]
% section formules_usuelles (end)
\rule{\columnwidth}{0.5pt}
\section{Espaces vectoriels} % (fold)
\label{sum:espaces_vectoriels}
   Espaces vectoriels normés usuels : \newline
   $\mathbb{R}^n, \mathbb{C}^n$, $\mathbb{R}_n[X], \mathbb{C}_n[X]$, $\mathcal{M}_n(\mathbb{R}), \mathcal{M}_n(\mathbb{C})$, $\mathcal{C}^n(I, \mathbb{R})$, 
%  sum:espaces_vectoriels (end)
\rule{\columnwidth}{0.5pt}
\section{Astuces}
\begin{description}
\item[Primitives de $1$] Dans une IPP, on peut primitiver $1$ par $1+x$ pour enlever un terme au dénominateur. \textit{Exemple : $\left( \ln(1+x)\right) ^n$}
\item[Fontion $\boldsymbol{k}$-lipschitzienne\index{Lipschitzienne@$k$-lipschitzienne}] Il suffit de montrer que $\exists k\in \Reel$ tq \ibox{$|f'(t)| \le k$}
\item[Inverse d'une Matrice $\boldsymbol{2\times 2}$] Si $M = \begin{pmatrix}
a & b \\ 
c & d
\end{pmatrix}$, alors $M^{-1} =  \dfrac{1}{da - bc}\begin{pmatrix}
d & -b \\ 
-c & a
\end{pmatrix}$
\item[Dérivée de $\boldsymbol{a^x}$] On a $a^x = e^{x \ln (a)} $, donc sa dérivée est \ibox{$\ln(a) \times a^x$}
%TODO: cf. Feuille 12
\end{description}
\rule{\columnwidth }{0.5pt}

\printnoidxglossary[type=\acronymtype,style=longragged,title={Liste des acronymes}]
\printindex
\end{document}
